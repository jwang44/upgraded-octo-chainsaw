{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ConvNet.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "CFEyrp_mYmIO",
        "ZX0XLJDjkqX_",
        "3yEtGFpwdYx1",
        "K7VpSLb22POJ"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jwang44/upgraded-octo-chainsaw/blob/main/resnet34%20parameter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dOxAvsuIul3",
        "outputId": "795797df-d128-4788-c34a-fa6eea7ebeea"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tj7sHHnv5fa",
        "outputId": "695e28c6-647d-4167-e498-0aa43810def3"
      },
      "source": [
        "%cd '/content/drive/MyDrive/imageunderstanding'\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/imageunderstanding'\n",
            "/content\n",
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqRELpG1DsZq",
        "outputId": "a8257fcb-fdce-4f51-848d-d69b5b8bf914"
      },
      "source": [
        "%cd '/content/drive/MyDrive/551 A3'\n",
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/551 A3\n",
            "mlruns\tTest.pkl  Train_labels.csv  Train.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1-Hfwf3lrw4"
      },
      "source": [
        "TRAIN_DATA_PATH = \"Train.pkl\"\n",
        "TRAIN_LABEL_PATH = \"Train_labels.csv\"\n",
        "TEST_DATA_PATH = \"Test.pkl\"\n",
        "CSV_OUTPUT_PATH = \"PRED_RESULT.csv\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XS2MS7nHzIN"
      },
      "source": [
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from PIL import Image\n",
        "import torch\n",
        "import pandas as pd"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBXdYCpunCp2",
        "outputId": "42f668b9-bf9c-442f-fcdc-f11e553bf5e3"
      },
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "DEVICE"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zppkOjQwVjy2"
      },
      "source": [
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2Vm3wY_K3DA"
      },
      "source": [
        "## Dataset Class / Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bydOoypc5SAM"
      },
      "source": [
        "IMG_SIZE = (224, 224)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMnXBwXMCqd5"
      },
      "source": [
        "# Transforms are common image transformations. They can be chained together using Compose.\n",
        "# Here we normalize images img=(img-0.5)/0.5\n",
        "img_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize([0.5], [0.5]),\n",
        "    transforms.Resize(IMG_SIZE)  \n",
        "    # transforms.RandomRotation(10, resample=PIL.Image.BILINEAR)\n",
        "    # transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)\n",
        "    # transforms.RandomAffine(degrees, translate=None, scale=None, shear=None, interpolation=<InterpolationMode.NEAREST: 'nearest'>, fill=0, fillcolor=None, resample=None)\n",
        "])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY0SndE2qcmb"
      },
      "source": [
        "# img_file: the pickle file containing the images\n",
        "# label_file: the .csv file containing the labels\n",
        "# transform: We use it for normalizing images (see above)\n",
        "# idx: This is a binary vector that is useful for creating training and validation set.\n",
        "# It return only samples where idx is True\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, img_file, label_file, transform=None, idx = None):\n",
        "        self.data = pickle.load( open( img_file, 'rb' ), encoding='bytes')\n",
        "        self.targets = np.genfromtxt(label_file, delimiter=',', skip_header=1, usecols=1) #[:,1:]\n",
        "        if idx is not None:\n",
        "          self.targets = self.targets[idx]\n",
        "          self.data = self.data[idx]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index].squeeze(), int(self.targets[index])\n",
        "        img = Image.fromarray((img*255).astype('uint8'), mode='L')\n",
        "        if self.transform is not None:\n",
        "           img = self.transform(img)\n",
        "        return img, target"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3b-QvEvqNE3"
      },
      "source": [
        "Get loader for all train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74kV_DwkqizL"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "dataset = MyDataset(TRAIN_DATA_PATH, TRAIN_LABEL_PATH,transform=img_transform, idx=None)\n",
        "# dataloader for all data\n",
        "data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZNiErb-pbPt"
      },
      "source": [
        "Get loaders for train/val data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05SXSXBRW4DX"
      },
      "source": [
        "VAL_SPLIT = 0.15\n",
        "shuffle = True\n",
        "\n",
        "# Creating indices for train and val split:\n",
        "dataset_size = len(dataset)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(VAL_SPLIT * dataset_size))\n",
        "if shuffle:\n",
        "  # set random seed so that we get the same split everytime\n",
        "  np.random.seed(0)\n",
        "  np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "train_dataset = MyDataset(TRAIN_DATA_PATH, TRAIN_LABEL_PATH,transform=img_transform, idx=train_indices)\n",
        "val_dataset = MyDataset(TRAIN_DATA_PATH, TRAIN_LABEL_PATH,transform=img_transform, idx=val_indices)\n",
        "\n",
        "# separate loaders for train and val data\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFEyrp_mYmIO"
      },
      "source": [
        "## Test Dataset / Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1d3LIGlYrDK"
      },
      "source": [
        "class MyTestSet(Dataset):\n",
        "  def __init__(self, img_file, transform=None):\n",
        "    self.data = pickle.load( open(img_file, 'rb' ), encoding='bytes')\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    # return self.data.shape[0]\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img = self.data[index].squeeze()\n",
        "    img = Image.fromarray((img*255).astype('uint8'), mode='L')\n",
        "    if self.transform is not None:\n",
        "      img = self.transform(img)\n",
        "    return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk5z7LRha2Uc"
      },
      "source": [
        "test_dataset = MyTestSet(TEST_DATA_PATH,transform=img_transform)\n",
        "# dataloader for test data\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdvC0-OfRnQV"
      },
      "source": [
        "## ResNet18"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFVSRS6gs8t7"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import torchvision.models as models"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAERROIVU3gP"
      },
      "source": [
        "import optuna\n",
        "import mlflow\n",
        "from pprint import pformat\n",
        "from torch.optim.lr_scheduler import StepLR"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbK8DR_bXB-7"
      },
      "source": [
        "! pip install optuna\n",
        "! pip install mlflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeVwuJHatdb7"
      },
      "source": [
        "Let's train these. But first, create the network, the optimizer and some lists for logging the training process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGj4HgAEMsxA"
      },
      "source": [
        "### alexnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx68ZfoSGgZy"
      },
      "source": [
        "# model = models.alexnet(pretrained=False)\n",
        "# model.features[0] = nn.Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
        "# model.classifier[6] = nn.Linear(4096, 10)\n",
        "# model = model.to(DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci11eU_mKnkX"
      },
      "source": [
        "### resnet18"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfN0avQP398k"
      },
      "source": [
        "model = models.resnet18(pretrained=False)\n",
        "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "model.fc = nn.Linear(in_features=512, out_features=10, bias=True)\n",
        "model = model.to(DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Kcb2ohYKrRk"
      },
      "source": [
        "### resnet34"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-Lb5h6MJmXv"
      },
      "source": [
        "model = models.resnet34(pretrained=False)\n",
        "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "model.fc = nn.Linear(in_features=512, out_features=10, bias=True)\n",
        "model = model.to(DEVICE)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFDcK-NeKvm4"
      },
      "source": [
        "### optimizer & initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJqfMWSFIztX"
      },
      "source": [
        "# optimizer = optim.SGD(tutor_model.parameters(), lr=0.01, momentum=0.5)\n",
        "# optimizer = optim.SGD(tutor_model.parameters(), lr=1, momentum=0.5)\n",
        "# optimizer = optim.Adam(model.parameters())\n",
        "optimizer = optim.RMSprop(model.parameters())\n",
        "\n",
        "train_losses = []\n",
        "train_counter = []\n",
        "val_losses = []\n",
        "val_counter = [i*len(train_loader.dataset) for i in range(3)]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck4yNKs46Q1Y"
      },
      "source": [
        "### Train and test function, used many times"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ps3KWsvtlSS"
      },
      "source": [
        "def train(epoch, model, loader):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(loader):\n",
        "    optimizer.zero_grad()\n",
        "    data = data.to(DEVICE)\n",
        "    # print(data.shape)\n",
        "    target = target.to(DEVICE)\n",
        "    output = model(data)\n",
        "    # print(output.shape)\n",
        "    # target = torch.argmax(target, dim=1) # convert from 1-hot to 1D\n",
        "    loss = F.cross_entropy(output, target) #negative log likelihood loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % 200 == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        epoch, batch_idx * len(data), len(loader.dataset),\n",
        "        100. * batch_idx / len(loader), loss.item()))\n",
        "      train_losses.append(loss.item())\n",
        "      train_counter.append(\n",
        "        (batch_idx*64) + ((epoch-1)*len(loader.dataset)))\n",
        "      torch.save(model.state_dict(), '/model.pth')\n",
        "      torch.save(optimizer.state_dict(), '/optimizer.pth')\n",
        "\n",
        "def val(model, loader):\n",
        "  model.eval()\n",
        "  val_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in loader:\n",
        "      data = data.to(DEVICE)\n",
        "      target = target.to(DEVICE)\n",
        "      output = model(data)\n",
        "      val_loss += F.cross_entropy(output, target, size_average=False).item()\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "  val_loss /= len(loader.dataset)\n",
        "  val_losses.append(val_loss)\n",
        "  print('Val set: Epoch: {}, Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "    epoch, val_loss, correct, len(loader.dataset),\n",
        "    100. * correct / len(loader.dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkgdjG8vV4BV"
      },
      "source": [
        "# for hyperparameter\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    train_set_size = len(train_loader.dataset)\n",
        "    num_batches = len(train_loader)\n",
        "    train_loss = 0.0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            batch_size = len(data)\n",
        "            print(f\"Train Epoch: {epoch} [{batch_idx * batch_size}/{train_set_size} \"\n",
        "                  f\"({100. * batch_idx / num_batches:.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
        "    avg_train_loss = train_loss / num_batches\n",
        "    return avg_train_loss\n",
        "\n",
        "# Testing loop\n",
        "def val(model, device, val_loader):\n",
        "    model.eval()\n",
        "    val_set_size = len(val_loader.dataset)\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            val_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    val_loss /= val_set_size\n",
        "\n",
        "    print(f\"Test set: Average loss: {val_loss:.4f}, Accuracy: {correct}/{val_set_size} \"\n",
        "          f\"({100. * correct / val_set_size:.0f}%)\\n\")\n",
        "    return val_loss"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxyNMVmPTG2L"
      },
      "source": [
        "### parameter selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1Co2AKZWCWX"
      },
      "source": [
        "def suggest_hyperparameters(trial):\n",
        "    # Obtain the learning rate on a logarithmic scale\n",
        "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-1, log=True)\n",
        "    # Obtain the dropout ratio in a range from 0.0 to 0.9 with step size 0.1\n",
        "    #dropout = trial.suggest_float(\"dropout\", 0.0, 0.9, step=0.1)\n",
        "    # Obtain the optimizer to use by name\n",
        "    optimizer_name = trial.suggest_categorical(\"optimizer_name\", [\"Adam\", \"RMSprop\"])\n",
        "    #momentum= trial.suggest_uniform('momentum', 0.4, 0.99)\n",
        "\n",
        "    print(f\"Suggested hyperparameters: \\n{pformat(trial.params)}\")\n",
        "    return lr,  optimizer_name#,momentum#dropout"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqPsRpU6XDl7"
      },
      "source": [
        "def objective(trial):\n",
        "    print(\"\\n********************************\\n\")\n",
        "    best_val_loss = float('Inf')\n",
        "    \n",
        "    # Start a new mlflow run\n",
        "    with mlflow.start_run():\n",
        "        # Get hyperparameter suggestions created by optuna and log them as params using mlflow\n",
        "        #lr,  optimizer_name, momentum= suggest_hyperparameters(trial)\n",
        "        \n",
        "        lr,optimizer_name= suggest_hyperparameters(trial)\n",
        "        mlflow.log_params(trial.params)\n",
        "\n",
        "        # Use CUDA if GPU is available and log device as param using mlflow\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        mlflow.log_param(\"device\", device)\n",
        "\n",
        "        # define model\n",
        "        #model = Tutor_model(dropout=dropout).to(device)\n",
        "        model = models.resnet18(pretrained=False)\n",
        "        model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "        model.fc = nn.Linear(in_features=512, out_features=10, bias=True)\n",
        "        model = model.to(DEVICE)\n",
        "\n",
        "        # Pick an optimizer based on optuna's parameter suggestion\n",
        "        if optimizer_name == \"Adam\":\n",
        "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "        if optimizer_name == \"RMSprop\":\n",
        "            optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
        "        scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
        "        \n",
        "        # Get DataLoaders for MNIST train and validation set\n",
        "        #train_loader, val_loader = get_mnist_dataloaders()\n",
        "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "        \n",
        "        # Network training & validation loop\n",
        "        for epoch in range(0, 5):\n",
        "            avg_train_loss = train(model, device, train_loader, optimizer, epoch)\n",
        "            #avg_train_loss = train( epoch,model,train_loader)\n",
        "            avg_val_loss = val(model, device, val_loader)\n",
        "            \n",
        "            if avg_val_loss <= best_val_loss:\n",
        "                best_val_loss = avg_val_loss\n",
        "\n",
        "            # Log average train and validation set loss metrics for the current epoch using mlflow\n",
        "            mlflow.log_metric(\"avg_train_losses\", avg_train_loss, step=epoch)\n",
        "            mlflow.log_metric(\"avg_val_loss\", avg_val_loss, step=epoch)\n",
        "            \n",
        "            scheduler.step()\n",
        "\n",
        "    # Return the best validation loss achieved by the network.\n",
        "    # This is needed as Optuna needs to know how the suggested hyperparameters are influencing the network loss.\n",
        "    return best_val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSFYDcBoXqTm"
      },
      "source": [
        "def objective(trial):\n",
        "    print(\"\\n********************************\\n\")\n",
        "    best_val_loss = float('Inf')\n",
        "    \n",
        "    # Start a new mlflow run\n",
        "    with mlflow.start_run():\n",
        "        # Get hyperparameter suggestions created by optuna and log them as params using mlflow\n",
        "        #lr,  optimizer_name, momentum= suggest_hyperparameters(trial)\n",
        "        \n",
        "        lr,optimizer_name= suggest_hyperparameters(trial)\n",
        "        mlflow.log_params(trial.params)\n",
        "\n",
        "        # Use CUDA if GPU is available and log device as param using mlflow\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        mlflow.log_param(\"device\", device)\n",
        "\n",
        "        # define model\n",
        "        #model = Tutor_model(dropout=dropout).to(device)\n",
        "        model = models.resnet34(pretrained=False)\n",
        "        model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "        model.fc = nn.Linear(in_features=512, out_features=10, bias=True)\n",
        "        model = model.to(DEVICE)\n",
        "\n",
        "        # Pick an optimizer based on optuna's parameter suggestion\n",
        "        if optimizer_name == \"Adam\":\n",
        "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "        if optimizer_name == \"RMSprop\":\n",
        "            optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
        "        scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
        "        \n",
        "        # Get DataLoaders for MNIST train and validation set\n",
        "        #train_loader, val_loader = get_mnist_dataloaders()\n",
        "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "        \n",
        "        # Network training & validation loop\n",
        "        for epoch in range(0, 5):\n",
        "            avg_train_loss = train(model, device, train_loader, optimizer, epoch)\n",
        "            #avg_train_loss = train( epoch,model,train_loader)\n",
        "            avg_val_loss = val(model, device, val_loader)\n",
        "            \n",
        "            if avg_val_loss <= best_val_loss:\n",
        "                best_val_loss = avg_val_loss\n",
        "\n",
        "            # Log average train and validation set loss metrics for the current epoch using mlflow\n",
        "            mlflow.log_metric(\"avg_train_losses\", avg_train_loss, step=epoch)\n",
        "            mlflow.log_metric(\"avg_val_loss\", avg_val_loss, step=epoch)\n",
        "            \n",
        "            scheduler.step()\n",
        "\n",
        "    # Return the best validation loss achieved by the network.\n",
        "    # This is needed as Optuna needs to know how the suggested hyperparameters are influencing the network loss.\n",
        "    return best_val_loss"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrPy8kMBX3l4",
        "outputId": "aeec33cf-fd97-4a84-a6f0-db71882f3465",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "    # Create the optuna study which shares the experiment name\n",
        "    study = optuna.create_study(study_name=\"pytorch-mlflow-optuna\", direction=\"minimize\")\n",
        "    study.optimize(objective, n_trials=10)\n",
        "\n",
        "    # Print optuna study statistics\n",
        "    print(\"\\n++++++++++++++++++++++++++++++++++\\n\")\n",
        "    print(\"Study statistics: \")\n",
        "    print(\"  Number of finished trials: \", len(study.trials))\n",
        "\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "\n",
        "    print(\"  Trial number: \", trial.number)\n",
        "    print(\"  Loss (trial value): \", trial.value)\n",
        "\n",
        "    print(\"  Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(\"    {}: {}\".format(key, value))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-04-11 05:20:21,648]\u001b[0m A new study created in memory with name: pytorch-mlflow-optuna\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "********************************\n",
            "\n",
            "Suggested hyperparameters: \n",
            "{'lr': 0.0006748416686076236, 'optimizer_name': 'Adam'}\n",
            "Train Epoch: 0 [0/51000 (0%)]\tLoss: 0.122030\n",
            "Train Epoch: 0 [1280/51000 (3%)]\tLoss: -6.516493\n",
            "Train Epoch: 0 [2560/51000 (5%)]\tLoss: -16.236490\n",
            "Train Epoch: 0 [3840/51000 (8%)]\tLoss: -23.507294\n",
            "Train Epoch: 0 [5120/51000 (10%)]\tLoss: -31.071487\n",
            "Train Epoch: 0 [6400/51000 (13%)]\tLoss: -41.717083\n",
            "Train Epoch: 0 [7680/51000 (15%)]\tLoss: -48.388134\n",
            "Train Epoch: 0 [8960/51000 (18%)]\tLoss: -60.316460\n",
            "Train Epoch: 0 [10240/51000 (20%)]\tLoss: -70.037361\n",
            "Train Epoch: 0 [11520/51000 (23%)]\tLoss: -79.677643\n",
            "Train Epoch: 0 [12800/51000 (25%)]\tLoss: -89.197746\n",
            "Train Epoch: 0 [14080/51000 (28%)]\tLoss: -101.279297\n",
            "Train Epoch: 0 [15360/51000 (30%)]\tLoss: -112.208862\n",
            "Train Epoch: 0 [16640/51000 (33%)]\tLoss: -124.741501\n",
            "Train Epoch: 0 [17920/51000 (35%)]\tLoss: -137.817001\n",
            "Train Epoch: 0 [19200/51000 (38%)]\tLoss: -149.337311\n",
            "Train Epoch: 0 [20480/51000 (40%)]\tLoss: -163.742935\n",
            "Train Epoch: 0 [21760/51000 (43%)]\tLoss: -179.239822\n",
            "Train Epoch: 0 [23040/51000 (45%)]\tLoss: -191.109436\n",
            "Train Epoch: 0 [24320/51000 (48%)]\tLoss: -206.334686\n",
            "Train Epoch: 0 [25600/51000 (50%)]\tLoss: -225.270828\n",
            "Train Epoch: 0 [26880/51000 (53%)]\tLoss: -240.534897\n",
            "Train Epoch: 0 [28160/51000 (55%)]\tLoss: -251.644333\n",
            "Train Epoch: 0 [29440/51000 (58%)]\tLoss: -269.573181\n",
            "Train Epoch: 0 [30720/51000 (60%)]\tLoss: -281.589966\n",
            "Train Epoch: 0 [32000/51000 (63%)]\tLoss: -300.659210\n",
            "Train Epoch: 0 [33280/51000 (65%)]\tLoss: -312.240814\n",
            "Train Epoch: 0 [34560/51000 (68%)]\tLoss: -326.578796\n",
            "Train Epoch: 0 [35840/51000 (70%)]\tLoss: -349.140808\n",
            "Train Epoch: 0 [37120/51000 (73%)]\tLoss: -375.646729\n",
            "Train Epoch: 0 [38400/51000 (75%)]\tLoss: -379.579041\n",
            "Train Epoch: 0 [39680/51000 (78%)]\tLoss: -404.535156\n",
            "Train Epoch: 0 [40960/51000 (80%)]\tLoss: -426.643463\n",
            "Train Epoch: 0 [42240/51000 (83%)]\tLoss: -443.789124\n",
            "Train Epoch: 0 [43520/51000 (85%)]\tLoss: -473.290192\n",
            "Train Epoch: 0 [44800/51000 (88%)]\tLoss: -493.351410\n",
            "Train Epoch: 0 [46080/51000 (90%)]\tLoss: -515.289062\n",
            "Train Epoch: 0 [47360/51000 (93%)]\tLoss: -533.748901\n",
            "Train Epoch: 0 [48640/51000 (95%)]\tLoss: -559.634094\n",
            "Train Epoch: 0 [49920/51000 (98%)]\tLoss: -583.703979\n",
            "Test set: Average loss: -614.5806, Accuracy: 1775/9000 (20%)\n",
            "\n",
            "Train Epoch: 1 [0/51000 (0%)]\tLoss: -602.859924\n",
            "Train Epoch: 1 [1280/51000 (3%)]\tLoss: -624.180115\n",
            "Train Epoch: 1 [2560/51000 (5%)]\tLoss: -633.201416\n",
            "Train Epoch: 1 [3840/51000 (8%)]\tLoss: -650.161499\n",
            "Train Epoch: 1 [5120/51000 (10%)]\tLoss: -675.882385\n",
            "Train Epoch: 1 [6400/51000 (13%)]\tLoss: -697.904968\n",
            "Train Epoch: 1 [7680/51000 (15%)]\tLoss: -712.909912\n",
            "Train Epoch: 1 [8960/51000 (18%)]\tLoss: -717.879456\n",
            "Train Epoch: 1 [10240/51000 (20%)]\tLoss: -738.369385\n",
            "Train Epoch: 1 [11520/51000 (23%)]\tLoss: -766.087891\n",
            "Train Epoch: 1 [12800/51000 (25%)]\tLoss: -778.470154\n",
            "Train Epoch: 1 [14080/51000 (28%)]\tLoss: -774.981750\n",
            "Train Epoch: 1 [15360/51000 (30%)]\tLoss: -802.783936\n",
            "Train Epoch: 1 [16640/51000 (33%)]\tLoss: -817.095093\n",
            "Train Epoch: 1 [17920/51000 (35%)]\tLoss: -858.456299\n",
            "Train Epoch: 1 [19200/51000 (38%)]\tLoss: -873.687317\n",
            "Train Epoch: 1 [20480/51000 (40%)]\tLoss: -867.311951\n",
            "Train Epoch: 1 [21760/51000 (43%)]\tLoss: -894.622009\n",
            "Train Epoch: 1 [23040/51000 (45%)]\tLoss: -920.557129\n",
            "Train Epoch: 1 [24320/51000 (48%)]\tLoss: -913.921631\n",
            "Train Epoch: 1 [25600/51000 (50%)]\tLoss: -969.065735\n",
            "Train Epoch: 1 [26880/51000 (53%)]\tLoss: -983.255859\n",
            "Train Epoch: 1 [28160/51000 (55%)]\tLoss: -983.786011\n",
            "Train Epoch: 1 [29440/51000 (58%)]\tLoss: -1004.669006\n",
            "Train Epoch: 1 [30720/51000 (60%)]\tLoss: -1017.224976\n",
            "Train Epoch: 1 [32000/51000 (63%)]\tLoss: -1062.933960\n",
            "Train Epoch: 1 [33280/51000 (65%)]\tLoss: -1083.308716\n",
            "Train Epoch: 1 [34560/51000 (68%)]\tLoss: -1096.416626\n",
            "Train Epoch: 1 [35840/51000 (70%)]\tLoss: -1103.569946\n",
            "Train Epoch: 1 [37120/51000 (73%)]\tLoss: -1131.303101\n",
            "Train Epoch: 1 [38400/51000 (75%)]\tLoss: -1125.656982\n",
            "Train Epoch: 1 [39680/51000 (78%)]\tLoss: -1181.650024\n",
            "Train Epoch: 1 [40960/51000 (80%)]\tLoss: -1212.869507\n",
            "Train Epoch: 1 [42240/51000 (83%)]\tLoss: -1211.950195\n",
            "Train Epoch: 1 [43520/51000 (85%)]\tLoss: -1269.163696\n",
            "Train Epoch: 1 [44800/51000 (88%)]\tLoss: -1273.420410\n",
            "Train Epoch: 1 [46080/51000 (90%)]\tLoss: -1288.363770\n",
            "Train Epoch: 1 [47360/51000 (93%)]\tLoss: -1299.372070\n",
            "Train Epoch: 1 [48640/51000 (95%)]\tLoss: -1313.940308\n",
            "Train Epoch: 1 [49920/51000 (98%)]\tLoss: -1359.312500\n",
            "Test set: Average loss: -1394.8632, Accuracy: 1823/9000 (20%)\n",
            "\n",
            "Train Epoch: 2 [0/51000 (0%)]\tLoss: -1360.399292\n",
            "Train Epoch: 2 [1280/51000 (3%)]\tLoss: -1394.542969\n",
            "Train Epoch: 2 [2560/51000 (5%)]\tLoss: -1406.602417\n",
            "Train Epoch: 2 [3840/51000 (8%)]\tLoss: -1396.964966\n",
            "Train Epoch: 2 [5120/51000 (10%)]\tLoss: -1442.209839\n",
            "Train Epoch: 2 [6400/51000 (13%)]\tLoss: -1462.910278\n",
            "Train Epoch: 2 [7680/51000 (15%)]\tLoss: -1477.331787\n",
            "Train Epoch: 2 [8960/51000 (18%)]\tLoss: -1481.771484\n",
            "Train Epoch: 2 [10240/51000 (20%)]\tLoss: -1490.611572\n",
            "Train Epoch: 2 [11520/51000 (23%)]\tLoss: -1529.333496\n",
            "Train Epoch: 2 [12800/51000 (25%)]\tLoss: -1544.685669\n",
            "Train Epoch: 2 [14080/51000 (28%)]\tLoss: -1536.995850\n",
            "Train Epoch: 2 [15360/51000 (30%)]\tLoss: -1562.517212\n",
            "Train Epoch: 2 [16640/51000 (33%)]\tLoss: -1576.931885\n",
            "Train Epoch: 2 [17920/51000 (35%)]\tLoss: -1617.637207\n",
            "Train Epoch: 2 [19200/51000 (38%)]\tLoss: -1628.623535\n",
            "Train Epoch: 2 [20480/51000 (40%)]\tLoss: -1622.702759\n",
            "Train Epoch: 2 [21760/51000 (43%)]\tLoss: -1639.316650\n",
            "Train Epoch: 2 [23040/51000 (45%)]\tLoss: -1664.217041\n",
            "Train Epoch: 2 [24320/51000 (48%)]\tLoss: -1645.308960\n",
            "Train Epoch: 2 [25600/51000 (50%)]\tLoss: -1712.886597\n",
            "Train Epoch: 2 [26880/51000 (53%)]\tLoss: -1732.091919\n",
            "Train Epoch: 2 [28160/51000 (55%)]\tLoss: -1753.571289\n",
            "Train Epoch: 2 [29440/51000 (58%)]\tLoss: -1762.254883\n",
            "Train Epoch: 2 [30720/51000 (60%)]\tLoss: -1747.064087\n",
            "Train Epoch: 2 [32000/51000 (63%)]\tLoss: -1807.880493\n",
            "Train Epoch: 2 [33280/51000 (65%)]\tLoss: -1812.906128\n",
            "Train Epoch: 2 [34560/51000 (68%)]\tLoss: -1839.133667\n",
            "Train Epoch: 2 [35840/51000 (70%)]\tLoss: -1816.746460\n",
            "Train Epoch: 2 [37120/51000 (73%)]\tLoss: -1855.550415\n",
            "Train Epoch: 2 [38400/51000 (75%)]\tLoss: -1841.785767\n",
            "Train Epoch: 2 [39680/51000 (78%)]\tLoss: -1896.039673\n",
            "Train Epoch: 2 [40960/51000 (80%)]\tLoss: -1935.926392\n",
            "Train Epoch: 2 [42240/51000 (83%)]\tLoss: -1935.238892\n",
            "Train Epoch: 2 [43520/51000 (85%)]\tLoss: -1993.968872\n",
            "Train Epoch: 2 [44800/51000 (88%)]\tLoss: -1986.015869\n",
            "Train Epoch: 2 [46080/51000 (90%)]\tLoss: -1992.105347\n",
            "Train Epoch: 2 [47360/51000 (93%)]\tLoss: -2039.090332\n",
            "Train Epoch: 2 [48640/51000 (95%)]\tLoss: -2052.421631\n",
            "Train Epoch: 2 [49920/51000 (98%)]\tLoss: -2073.011230\n",
            "Test set: Average loss: -2078.9687, Accuracy: 1682/9000 (19%)\n",
            "\n",
            "Train Epoch: 3 [0/51000 (0%)]\tLoss: -2052.488281\n",
            "Train Epoch: 3 [1280/51000 (3%)]\tLoss: -2098.817871\n",
            "Train Epoch: 3 [2560/51000 (5%)]\tLoss: -2102.251953\n",
            "Train Epoch: 3 [3840/51000 (8%)]\tLoss: -2092.816650\n",
            "Train Epoch: 3 [5120/51000 (10%)]\tLoss: -2122.306396\n",
            "Train Epoch: 3 [6400/51000 (13%)]\tLoss: -2153.674072\n",
            "Train Epoch: 3 [7680/51000 (15%)]\tLoss: -2174.860840\n",
            "Train Epoch: 3 [8960/51000 (18%)]\tLoss: -2161.887451\n",
            "Train Epoch: 3 [10240/51000 (20%)]\tLoss: -2182.020752\n",
            "Train Epoch: 3 [11520/51000 (23%)]\tLoss: -2207.392578\n",
            "Train Epoch: 3 [12800/51000 (25%)]\tLoss: -2222.588379\n",
            "Train Epoch: 3 [14080/51000 (28%)]\tLoss: -2239.601318\n",
            "Train Epoch: 3 [15360/51000 (30%)]\tLoss: -2235.209961\n",
            "Train Epoch: 3 [16640/51000 (33%)]\tLoss: -2233.246582\n",
            "Train Epoch: 3 [17920/51000 (35%)]\tLoss: -2288.100342\n",
            "Train Epoch: 3 [19200/51000 (38%)]\tLoss: -2310.284668\n",
            "Train Epoch: 3 [20480/51000 (40%)]\tLoss: -2297.372803\n",
            "Train Epoch: 3 [21760/51000 (43%)]\tLoss: -2313.695801\n",
            "Train Epoch: 3 [23040/51000 (45%)]\tLoss: -2322.225342\n",
            "Train Epoch: 3 [24320/51000 (48%)]\tLoss: -2278.987061\n",
            "Train Epoch: 3 [25600/51000 (50%)]\tLoss: -2385.078857\n",
            "Train Epoch: 3 [26880/51000 (53%)]\tLoss: -2381.570801\n",
            "Train Epoch: 3 [28160/51000 (55%)]\tLoss: -2386.573486\n",
            "Train Epoch: 3 [29440/51000 (58%)]\tLoss: -2412.294922\n",
            "Train Epoch: 3 [30720/51000 (60%)]\tLoss: -2392.358643\n",
            "Train Epoch: 3 [32000/51000 (63%)]\tLoss: -2448.350098\n",
            "Train Epoch: 3 [33280/51000 (65%)]\tLoss: -2453.026367\n",
            "Train Epoch: 3 [34560/51000 (68%)]\tLoss: -2453.821045\n",
            "Train Epoch: 3 [35840/51000 (70%)]\tLoss: -2455.171631\n",
            "Train Epoch: 3 [37120/51000 (73%)]\tLoss: -2481.171387\n",
            "Train Epoch: 3 [38400/51000 (75%)]\tLoss: -2430.202148\n",
            "Train Epoch: 3 [39680/51000 (78%)]\tLoss: -2524.189941\n",
            "Train Epoch: 3 [40960/51000 (80%)]\tLoss: -2532.712402\n",
            "Train Epoch: 3 [42240/51000 (83%)]\tLoss: -2525.092529\n",
            "Train Epoch: 3 [43520/51000 (85%)]\tLoss: -2567.855225\n",
            "Train Epoch: 3 [44800/51000 (88%)]\tLoss: -2558.620361\n",
            "Train Epoch: 3 [46080/51000 (90%)]\tLoss: -2561.091309\n",
            "Train Epoch: 3 [47360/51000 (93%)]\tLoss: -2607.654785\n",
            "Train Epoch: 3 [48640/51000 (95%)]\tLoss: -2594.270264\n",
            "Train Epoch: 3 [49920/51000 (98%)]\tLoss: -2631.368896\n",
            "Test set: Average loss: -2655.0426, Accuracy: 1710/9000 (19%)\n",
            "\n",
            "Train Epoch: 4 [0/51000 (0%)]\tLoss: -2570.375488\n",
            "Train Epoch: 4 [1280/51000 (3%)]\tLoss: -2650.952393\n",
            "Train Epoch: 4 [2560/51000 (5%)]\tLoss: -2643.256592\n",
            "Train Epoch: 4 [3840/51000 (8%)]\tLoss: -2645.197021\n",
            "Train Epoch: 4 [5120/51000 (10%)]\tLoss: -2648.107910\n",
            "Train Epoch: 4 [6400/51000 (13%)]\tLoss: -2672.735107\n",
            "Train Epoch: 4 [7680/51000 (15%)]\tLoss: -2699.306152\n",
            "Train Epoch: 4 [8960/51000 (18%)]\tLoss: -2679.053955\n",
            "Train Epoch: 4 [10240/51000 (20%)]\tLoss: -2721.005615\n",
            "Train Epoch: 4 [11520/51000 (23%)]\tLoss: -2702.384521\n",
            "Train Epoch: 4 [12800/51000 (25%)]\tLoss: -2742.398682\n",
            "Train Epoch: 4 [14080/51000 (28%)]\tLoss: -2763.483887\n",
            "Train Epoch: 4 [15360/51000 (30%)]\tLoss: -2738.831543\n",
            "Train Epoch: 4 [16640/51000 (33%)]\tLoss: -2754.530518\n",
            "Train Epoch: 4 [17920/51000 (35%)]\tLoss: -2776.354980\n",
            "Train Epoch: 4 [19200/51000 (38%)]\tLoss: -2795.281006\n",
            "Train Epoch: 4 [20480/51000 (40%)]\tLoss: -2798.226807\n",
            "Train Epoch: 4 [21760/51000 (43%)]\tLoss: -2807.418945\n",
            "Train Epoch: 4 [23040/51000 (45%)]\tLoss: -2805.119629\n",
            "Train Epoch: 4 [24320/51000 (48%)]\tLoss: -2766.541992\n",
            "Train Epoch: 4 [25600/51000 (50%)]\tLoss: -2860.014893\n",
            "Train Epoch: 4 [26880/51000 (53%)]\tLoss: -2848.572021\n",
            "Train Epoch: 4 [28160/51000 (55%)]\tLoss: -2858.044678\n",
            "Train Epoch: 4 [29440/51000 (58%)]\tLoss: -2872.680908\n",
            "Train Epoch: 4 [30720/51000 (60%)]\tLoss: -2861.660889\n",
            "Train Epoch: 4 [32000/51000 (63%)]\tLoss: -2911.890625\n",
            "Train Epoch: 4 [33280/51000 (65%)]\tLoss: -2911.171875\n",
            "Train Epoch: 4 [34560/51000 (68%)]\tLoss: -2884.985840\n",
            "Train Epoch: 4 [35840/51000 (70%)]\tLoss: -2902.284668\n",
            "Train Epoch: 4 [37120/51000 (73%)]\tLoss: -2904.269287\n",
            "Train Epoch: 4 [38400/51000 (75%)]\tLoss: -2862.987549\n",
            "Train Epoch: 4 [39680/51000 (78%)]\tLoss: -2958.178711\n",
            "Train Epoch: 4 [40960/51000 (80%)]\tLoss: -2969.772461\n",
            "Train Epoch: 4 [42240/51000 (83%)]\tLoss: -2962.841309\n",
            "Train Epoch: 4 [43520/51000 (85%)]\tLoss: -2980.426270\n",
            "Train Epoch: 4 [44800/51000 (88%)]\tLoss: -2975.562012\n",
            "Train Epoch: 4 [46080/51000 (90%)]\tLoss: -2991.359375\n",
            "Train Epoch: 4 [47360/51000 (93%)]\tLoss: -3023.647217\n",
            "Train Epoch: 4 [48640/51000 (95%)]\tLoss: -2998.586182\n",
            "Train Epoch: 4 [49920/51000 (98%)]\tLoss: -3040.812012\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-04-11 05:48:36,375]\u001b[0m Trial 0 finished with value: -3067.4624713541666 and parameters: {'lr': 0.0006748416686076236, 'optimizer_name': 'Adam'}. Best is trial 0 with value: -3067.4624713541666.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test set: Average loss: -3067.4625, Accuracy: 1723/9000 (19%)\n",
            "\n",
            "\n",
            "********************************\n",
            "\n",
            "Suggested hyperparameters: \n",
            "{'lr': 0.021983150250598, 'optimizer_name': 'Adam'}\n",
            "Train Epoch: 0 [0/51000 (0%)]\tLoss: -0.047419\n",
            "Train Epoch: 0 [1280/51000 (3%)]\tLoss: -279.745239\n",
            "Train Epoch: 0 [2560/51000 (5%)]\tLoss: -817.955688\n",
            "Train Epoch: 0 [3840/51000 (8%)]\tLoss: -1672.500000\n",
            "Train Epoch: 0 [5120/51000 (10%)]\tLoss: -2935.091309\n",
            "Train Epoch: 0 [6400/51000 (13%)]\tLoss: -4798.421387\n",
            "Train Epoch: 0 [7680/51000 (15%)]\tLoss: -7178.535156\n",
            "Train Epoch: 0 [8960/51000 (18%)]\tLoss: -10136.256836\n",
            "Train Epoch: 0 [10240/51000 (20%)]\tLoss: -13705.512695\n",
            "Train Epoch: 0 [11520/51000 (23%)]\tLoss: -17933.255859\n",
            "Train Epoch: 0 [12800/51000 (25%)]\tLoss: -22768.416016\n",
            "Train Epoch: 0 [14080/51000 (28%)]\tLoss: -28221.017578\n",
            "Train Epoch: 0 [15360/51000 (30%)]\tLoss: -34305.453125\n",
            "Train Epoch: 0 [16640/51000 (33%)]\tLoss: -41157.312500\n",
            "Train Epoch: 0 [17920/51000 (35%)]\tLoss: -48630.328125\n",
            "Train Epoch: 0 [19200/51000 (38%)]\tLoss: -56628.878906\n",
            "Train Epoch: 0 [20480/51000 (40%)]\tLoss: -65514.933594\n",
            "Train Epoch: 0 [21760/51000 (43%)]\tLoss: -74846.078125\n",
            "Train Epoch: 0 [23040/51000 (45%)]\tLoss: -84947.265625\n",
            "Train Epoch: 0 [24320/51000 (48%)]\tLoss: -95616.601562\n",
            "Train Epoch: 0 [25600/51000 (50%)]\tLoss: -107040.867188\n",
            "Train Epoch: 0 [26880/51000 (53%)]\tLoss: -118985.945312\n",
            "Train Epoch: 0 [28160/51000 (55%)]\tLoss: -131623.828125\n",
            "Train Epoch: 0 [29440/51000 (58%)]\tLoss: -144926.390625\n",
            "Train Epoch: 0 [30720/51000 (60%)]\tLoss: -158706.546875\n",
            "Train Epoch: 0 [32000/51000 (63%)]\tLoss: -173335.453125\n",
            "Train Epoch: 0 [33280/51000 (65%)]\tLoss: -188377.890625\n",
            "Train Epoch: 0 [34560/51000 (68%)]\tLoss: -203801.750000\n",
            "Train Epoch: 0 [35840/51000 (70%)]\tLoss: -220626.625000\n",
            "Train Epoch: 0 [37120/51000 (73%)]\tLoss: -237455.218750\n",
            "Train Epoch: 0 [38400/51000 (75%)]\tLoss: -255007.718750\n",
            "Train Epoch: 0 [39680/51000 (78%)]\tLoss: -273491.437500\n",
            "Train Epoch: 0 [40960/51000 (80%)]\tLoss: -292232.343750\n",
            "Train Epoch: 0 [42240/51000 (83%)]\tLoss: -311711.968750\n",
            "Train Epoch: 0 [43520/51000 (85%)]\tLoss: -331690.531250\n",
            "Train Epoch: 0 [44800/51000 (88%)]\tLoss: -352517.843750\n",
            "Train Epoch: 0 [46080/51000 (90%)]\tLoss: -373747.000000\n",
            "Train Epoch: 0 [47360/51000 (93%)]\tLoss: -395436.187500\n",
            "Train Epoch: 0 [48640/51000 (95%)]\tLoss: -417484.218750\n",
            "Train Epoch: 0 [49920/51000 (98%)]\tLoss: -441145.156250\n",
            "Test set: Average loss: -457853.2096, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 1 [0/51000 (0%)]\tLoss: -462109.500000\n",
            "Train Epoch: 1 [1280/51000 (3%)]\tLoss: -478963.687500\n",
            "Train Epoch: 1 [2560/51000 (5%)]\tLoss: -496222.781250\n",
            "Train Epoch: 1 [3840/51000 (8%)]\tLoss: -513654.500000\n",
            "Train Epoch: 1 [5120/51000 (10%)]\tLoss: -530880.687500\n",
            "Train Epoch: 1 [6400/51000 (13%)]\tLoss: -548517.875000\n",
            "Train Epoch: 1 [7680/51000 (15%)]\tLoss: -566205.437500\n",
            "Train Epoch: 1 [8960/51000 (18%)]\tLoss: -584024.000000\n",
            "Train Epoch: 1 [10240/51000 (20%)]\tLoss: -602759.437500\n",
            "Train Epoch: 1 [11520/51000 (23%)]\tLoss: -619569.062500\n",
            "Train Epoch: 1 [12800/51000 (25%)]\tLoss: -639302.000000\n",
            "Train Epoch: 1 [14080/51000 (28%)]\tLoss: -657005.000000\n",
            "Train Epoch: 1 [15360/51000 (30%)]\tLoss: -675919.437500\n",
            "Train Epoch: 1 [16640/51000 (33%)]\tLoss: -695170.875000\n",
            "Train Epoch: 1 [17920/51000 (35%)]\tLoss: -714374.625000\n",
            "Train Epoch: 1 [19200/51000 (38%)]\tLoss: -734329.750000\n",
            "Train Epoch: 1 [20480/51000 (40%)]\tLoss: -753774.437500\n",
            "Train Epoch: 1 [21760/51000 (43%)]\tLoss: -773829.125000\n",
            "Train Epoch: 1 [23040/51000 (45%)]\tLoss: -793807.562500\n",
            "Train Epoch: 1 [24320/51000 (48%)]\tLoss: -814071.812500\n",
            "Train Epoch: 1 [25600/51000 (50%)]\tLoss: -835007.687500\n",
            "Train Epoch: 1 [26880/51000 (53%)]\tLoss: -855187.750000\n",
            "Train Epoch: 1 [28160/51000 (55%)]\tLoss: -876806.375000\n",
            "Train Epoch: 1 [29440/51000 (58%)]\tLoss: -897876.250000\n",
            "Train Epoch: 1 [30720/51000 (60%)]\tLoss: -919013.562500\n",
            "Train Epoch: 1 [32000/51000 (63%)]\tLoss: -941071.375000\n",
            "Train Epoch: 1 [33280/51000 (65%)]\tLoss: -962609.562500\n",
            "Train Epoch: 1 [34560/51000 (68%)]\tLoss: -984371.062500\n",
            "Train Epoch: 1 [35840/51000 (70%)]\tLoss: -1007228.375000\n",
            "Train Epoch: 1 [37120/51000 (73%)]\tLoss: -1029448.687500\n",
            "Train Epoch: 1 [38400/51000 (75%)]\tLoss: -1051752.625000\n",
            "Train Epoch: 1 [39680/51000 (78%)]\tLoss: -1075661.375000\n",
            "Train Epoch: 1 [40960/51000 (80%)]\tLoss: -1098255.750000\n",
            "Train Epoch: 1 [42240/51000 (83%)]\tLoss: -1121492.375000\n",
            "Train Epoch: 1 [43520/51000 (85%)]\tLoss: -1145075.625000\n",
            "Train Epoch: 1 [44800/51000 (88%)]\tLoss: -1168985.000000\n",
            "Train Epoch: 1 [46080/51000 (90%)]\tLoss: -1192751.000000\n",
            "Train Epoch: 1 [47360/51000 (93%)]\tLoss: -1217035.875000\n",
            "Train Epoch: 1 [48640/51000 (95%)]\tLoss: -1241115.250000\n",
            "Train Epoch: 1 [49920/51000 (98%)]\tLoss: -1266863.750000\n",
            "Test set: Average loss: -1288807.4662, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 2 [0/51000 (0%)]\tLoss: -1288060.625000\n",
            "Train Epoch: 2 [1280/51000 (3%)]\tLoss: -1305933.875000\n",
            "Train Epoch: 2 [2560/51000 (5%)]\tLoss: -1323655.750000\n",
            "Train Epoch: 2 [3840/51000 (8%)]\tLoss: -1341745.000000\n",
            "Train Epoch: 2 [5120/51000 (10%)]\tLoss: -1358724.375000\n",
            "Train Epoch: 2 [6400/51000 (13%)]\tLoss: -1376691.500000\n",
            "Train Epoch: 2 [7680/51000 (15%)]\tLoss: -1394451.125000\n",
            "Train Epoch: 2 [8960/51000 (18%)]\tLoss: -1411910.500000\n",
            "Train Epoch: 2 [10240/51000 (20%)]\tLoss: -1431363.125000\n",
            "Train Epoch: 2 [11520/51000 (23%)]\tLoss: -1447595.500000\n",
            "Train Epoch: 2 [12800/51000 (25%)]\tLoss: -1466821.750000\n",
            "Train Epoch: 2 [14080/51000 (28%)]\tLoss: -1484683.625000\n",
            "Train Epoch: 2 [15360/51000 (30%)]\tLoss: -1502379.500000\n",
            "Train Epoch: 2 [16640/51000 (33%)]\tLoss: -1521025.875000\n",
            "Train Epoch: 2 [17920/51000 (35%)]\tLoss: -1539136.625000\n",
            "Train Epoch: 2 [19200/51000 (38%)]\tLoss: -1558633.625000\n",
            "Train Epoch: 2 [20480/51000 (40%)]\tLoss: -1576158.000000\n",
            "Train Epoch: 2 [21760/51000 (43%)]\tLoss: -1595025.125000\n",
            "Train Epoch: 2 [23040/51000 (45%)]\tLoss: -1613181.875000\n",
            "Train Epoch: 2 [24320/51000 (48%)]\tLoss: -1631648.250000\n",
            "Train Epoch: 2 [25600/51000 (50%)]\tLoss: -1651172.625000\n",
            "Train Epoch: 2 [26880/51000 (53%)]\tLoss: -1668629.375000\n",
            "Train Epoch: 2 [28160/51000 (55%)]\tLoss: -1688857.250000\n",
            "Train Epoch: 2 [29440/51000 (58%)]\tLoss: -1707539.000000\n",
            "Train Epoch: 2 [30720/51000 (60%)]\tLoss: -1726315.875000\n",
            "Train Epoch: 2 [32000/51000 (63%)]\tLoss: -1745999.625000\n",
            "Train Epoch: 2 [33280/51000 (65%)]\tLoss: -1764699.000000\n",
            "Train Epoch: 2 [34560/51000 (68%)]\tLoss: -1783399.750000\n",
            "Train Epoch: 2 [35840/51000 (70%)]\tLoss: -1803741.000000\n",
            "Train Epoch: 2 [37120/51000 (73%)]\tLoss: -1822789.250000\n",
            "Train Epoch: 2 [38400/51000 (75%)]\tLoss: -1841587.500000\n",
            "Train Epoch: 2 [39680/51000 (78%)]\tLoss: -1862739.125000\n",
            "Train Epoch: 2 [40960/51000 (80%)]\tLoss: -1881601.750000\n",
            "Train Epoch: 2 [42240/51000 (83%)]\tLoss: -1901201.750000\n",
            "Train Epoch: 2 [43520/51000 (85%)]\tLoss: -1920936.000000\n",
            "Train Epoch: 2 [44800/51000 (88%)]\tLoss: -1940945.250000\n",
            "Train Epoch: 2 [46080/51000 (90%)]\tLoss: -1960493.250000\n",
            "Train Epoch: 2 [47360/51000 (93%)]\tLoss: -1980908.500000\n",
            "Train Epoch: 2 [48640/51000 (95%)]\tLoss: -2000334.625000\n",
            "Train Epoch: 2 [49920/51000 (98%)]\tLoss: -2022040.875000\n",
            "Test set: Average loss: -2039685.9716, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 3 [0/51000 (0%)]\tLoss: -2038468.125000\n",
            "Train Epoch: 3 [1280/51000 (3%)]\tLoss: -2053396.875000\n",
            "Train Epoch: 3 [2560/51000 (5%)]\tLoss: -2067826.375000\n",
            "Train Epoch: 3 [3840/51000 (8%)]\tLoss: -2082740.750000\n",
            "Train Epoch: 3 [5120/51000 (10%)]\tLoss: -2095857.500000\n",
            "Train Epoch: 3 [6400/51000 (13%)]\tLoss: -2110448.750000\n",
            "Train Epoch: 3 [7680/51000 (15%)]\tLoss: -2124793.250000\n",
            "Train Epoch: 3 [8960/51000 (18%)]\tLoss: -2138473.000000\n",
            "Train Epoch: 3 [10240/51000 (20%)]\tLoss: -2154910.500000\n",
            "Train Epoch: 3 [11520/51000 (23%)]\tLoss: -2166817.500000\n",
            "Train Epoch: 3 [12800/51000 (25%)]\tLoss: -2182700.750000\n",
            "Train Epoch: 3 [14080/51000 (28%)]\tLoss: -2197145.000000\n",
            "Train Epoch: 3 [15360/51000 (30%)]\tLoss: -2210606.750000\n",
            "Train Epoch: 3 [16640/51000 (33%)]\tLoss: -2225663.750000\n",
            "Train Epoch: 3 [17920/51000 (35%)]\tLoss: -2239877.000000\n",
            "Train Epoch: 3 [19200/51000 (38%)]\tLoss: -2256022.500000\n",
            "Train Epoch: 3 [20480/51000 (40%)]\tLoss: -2269038.750000\n",
            "Train Epoch: 3 [21760/51000 (43%)]\tLoss: -2284109.250000\n",
            "Train Epoch: 3 [23040/51000 (45%)]\tLoss: -2297966.000000\n",
            "Train Epoch: 3 [24320/51000 (48%)]\tLoss: -2312227.000000\n",
            "Train Epoch: 3 [25600/51000 (50%)]\tLoss: -2327943.500000\n",
            "Train Epoch: 3 [26880/51000 (53%)]\tLoss: -2340567.000000\n",
            "Train Epoch: 3 [28160/51000 (55%)]\tLoss: -2357130.750000\n",
            "Train Epoch: 3 [29440/51000 (58%)]\tLoss: -2371390.500000\n",
            "Train Epoch: 3 [30720/51000 (60%)]\tLoss: -2385883.250000\n",
            "Train Epoch: 3 [32000/51000 (63%)]\tLoss: -2401240.500000\n",
            "Train Epoch: 3 [33280/51000 (65%)]\tLoss: -2415370.000000\n",
            "Train Epoch: 3 [34560/51000 (68%)]\tLoss: -2429352.500000\n",
            "Train Epoch: 3 [35840/51000 (70%)]\tLoss: -2445509.750000\n",
            "Train Epoch: 3 [37120/51000 (73%)]\tLoss: -2459888.750000\n",
            "Train Epoch: 3 [38400/51000 (75%)]\tLoss: -2473799.500000\n",
            "Train Epoch: 3 [39680/51000 (78%)]\tLoss: -2490706.250000\n",
            "Train Epoch: 3 [40960/51000 (80%)]\tLoss: -2504637.250000\n",
            "Train Epoch: 3 [42240/51000 (83%)]\tLoss: -2519450.000000\n",
            "Train Epoch: 3 [43520/51000 (85%)]\tLoss: -2534272.500000\n",
            "Train Epoch: 3 [44800/51000 (88%)]\tLoss: -2549383.500000\n",
            "Train Epoch: 3 [46080/51000 (90%)]\tLoss: -2563843.750000\n",
            "Train Epoch: 3 [47360/51000 (93%)]\tLoss: -2579535.250000\n",
            "Train Epoch: 3 [48640/51000 (95%)]\tLoss: -2593640.750000\n",
            "Train Epoch: 3 [49920/51000 (98%)]\tLoss: -2610554.250000\n",
            "Test set: Average loss: -2623223.8969, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 4 [0/51000 (0%)]\tLoss: -2621865.750000\n",
            "Train Epoch: 4 [1280/51000 (3%)]\tLoss: -2633438.000000\n",
            "Train Epoch: 4 [2560/51000 (5%)]\tLoss: -2644261.000000\n",
            "Train Epoch: 4 [3840/51000 (8%)]\tLoss: -2655667.750000\n",
            "Train Epoch: 4 [5120/51000 (10%)]\tLoss: -2664768.000000\n",
            "Train Epoch: 4 [6400/51000 (13%)]\tLoss: -2675736.750000\n",
            "Train Epoch: 4 [7680/51000 (15%)]\tLoss: -2686454.000000\n",
            "Train Epoch: 4 [8960/51000 (18%)]\tLoss: -2696248.000000\n",
            "Train Epoch: 4 [10240/51000 (20%)]\tLoss: -2709379.000000\n",
            "Train Epoch: 4 [11520/51000 (23%)]\tLoss: -2717036.750000\n",
            "Train Epoch: 4 [12800/51000 (25%)]\tLoss: -2729398.250000\n",
            "Train Epoch: 4 [14080/51000 (28%)]\tLoss: -2740338.500000\n",
            "Train Epoch: 4 [15360/51000 (30%)]\tLoss: -2749643.750000\n",
            "Train Epoch: 4 [16640/51000 (33%)]\tLoss: -2761072.750000\n",
            "Train Epoch: 4 [17920/51000 (35%)]\tLoss: -2771453.000000\n",
            "Train Epoch: 4 [19200/51000 (38%)]\tLoss: -2784183.000000\n",
            "Train Epoch: 4 [20480/51000 (40%)]\tLoss: -2792936.000000\n",
            "Train Epoch: 4 [21760/51000 (43%)]\tLoss: -2804303.000000\n",
            "Train Epoch: 4 [23040/51000 (45%)]\tLoss: -2814086.750000\n",
            "Train Epoch: 4 [24320/51000 (48%)]\tLoss: -2824370.250000\n",
            "Train Epoch: 4 [25600/51000 (50%)]\tLoss: -2836431.000000\n",
            "Train Epoch: 4 [26880/51000 (53%)]\tLoss: -2844644.750000\n",
            "Train Epoch: 4 [28160/51000 (55%)]\tLoss: -2857694.500000\n",
            "Train Epoch: 4 [29440/51000 (58%)]\tLoss: -2867898.500000\n",
            "Train Epoch: 4 [30720/51000 (60%)]\tLoss: -2878467.500000\n",
            "Train Epoch: 4 [32000/51000 (63%)]\tLoss: -2889862.000000\n",
            "Train Epoch: 4 [33280/51000 (65%)]\tLoss: -2899888.500000\n",
            "Train Epoch: 4 [34560/51000 (68%)]\tLoss: -2909662.250000\n",
            "Train Epoch: 4 [35840/51000 (70%)]\tLoss: -2922047.000000\n",
            "Train Epoch: 4 [37120/51000 (73%)]\tLoss: -2932300.250000\n",
            "Train Epoch: 4 [38400/51000 (75%)]\tLoss: -2941940.250000\n",
            "Train Epoch: 4 [39680/51000 (78%)]\tLoss: -2955068.500000\n",
            "Train Epoch: 4 [40960/51000 (80%)]\tLoss: -2964741.750000\n",
            "Train Epoch: 4 [42240/51000 (83%)]\tLoss: -2975418.250000\n",
            "Train Epoch: 4 [43520/51000 (85%)]\tLoss: -2986032.500000\n",
            "Train Epoch: 4 [44800/51000 (88%)]\tLoss: -2996963.000000\n",
            "Train Epoch: 4 [46080/51000 (90%)]\tLoss: -3007123.750000\n",
            "Train Epoch: 4 [47360/51000 (93%)]\tLoss: -3018810.500000\n",
            "Train Epoch: 4 [48640/51000 (95%)]\tLoss: -3028478.750000\n",
            "Train Epoch: 4 [49920/51000 (98%)]\tLoss: -3041358.000000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-04-11 06:16:35,224]\u001b[0m Trial 1 finished with value: -3050000.351111111 and parameters: {'lr': 0.021983150250598, 'optimizer_name': 'Adam'}. Best is trial 1 with value: -3050000.351111111.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test set: Average loss: -3050000.3511, Accuracy: 895/9000 (10%)\n",
            "\n",
            "\n",
            "********************************\n",
            "\n",
            "Suggested hyperparameters: \n",
            "{'lr': 0.0007200561133814223, 'optimizer_name': 'RMSprop'}\n",
            "Train Epoch: 0 [0/51000 (0%)]\tLoss: 0.285095\n",
            "Train Epoch: 0 [1280/51000 (3%)]\tLoss: -41.204960\n",
            "Train Epoch: 0 [2560/51000 (5%)]\tLoss: -66.416664\n",
            "Train Epoch: 0 [3840/51000 (8%)]\tLoss: -92.013123\n",
            "Train Epoch: 0 [5120/51000 (10%)]\tLoss: -114.488914\n",
            "Train Epoch: 0 [6400/51000 (13%)]\tLoss: -135.042877\n",
            "Train Epoch: 0 [7680/51000 (15%)]\tLoss: -154.262436\n",
            "Train Epoch: 0 [8960/51000 (18%)]\tLoss: -172.887863\n",
            "Train Epoch: 0 [10240/51000 (20%)]\tLoss: -191.582321\n",
            "Train Epoch: 0 [11520/51000 (23%)]\tLoss: -209.616409\n",
            "Train Epoch: 0 [12800/51000 (25%)]\tLoss: -227.166626\n",
            "Train Epoch: 0 [14080/51000 (28%)]\tLoss: -245.435989\n",
            "Train Epoch: 0 [15360/51000 (30%)]\tLoss: -262.794983\n",
            "Train Epoch: 0 [16640/51000 (33%)]\tLoss: -280.156982\n",
            "Train Epoch: 0 [17920/51000 (35%)]\tLoss: -298.691833\n",
            "Train Epoch: 0 [19200/51000 (38%)]\tLoss: -316.523285\n",
            "Train Epoch: 0 [20480/51000 (40%)]\tLoss: -335.798859\n",
            "Train Epoch: 0 [21760/51000 (43%)]\tLoss: -353.938660\n",
            "Train Epoch: 0 [23040/51000 (45%)]\tLoss: -372.141388\n",
            "Train Epoch: 0 [24320/51000 (48%)]\tLoss: -390.031250\n",
            "Train Epoch: 0 [25600/51000 (50%)]\tLoss: -410.031738\n",
            "Train Epoch: 0 [26880/51000 (53%)]\tLoss: -427.841156\n",
            "Train Epoch: 0 [28160/51000 (55%)]\tLoss: -446.686096\n",
            "Train Epoch: 0 [29440/51000 (58%)]\tLoss: -467.078766\n",
            "Train Epoch: 0 [30720/51000 (60%)]\tLoss: -485.462006\n",
            "Train Epoch: 0 [32000/51000 (63%)]\tLoss: -506.330353\n",
            "Train Epoch: 0 [33280/51000 (65%)]\tLoss: -526.301575\n",
            "Train Epoch: 0 [34560/51000 (68%)]\tLoss: -544.271484\n",
            "Train Epoch: 0 [35840/51000 (70%)]\tLoss: -565.952881\n",
            "Train Epoch: 0 [37120/51000 (73%)]\tLoss: -585.313354\n",
            "Train Epoch: 0 [38400/51000 (75%)]\tLoss: -605.454956\n",
            "Train Epoch: 0 [39680/51000 (78%)]\tLoss: -628.010803\n",
            "Train Epoch: 0 [40960/51000 (80%)]\tLoss: -648.079285\n",
            "Train Epoch: 0 [42240/51000 (83%)]\tLoss: -670.216797\n",
            "Train Epoch: 0 [43520/51000 (85%)]\tLoss: -690.106506\n",
            "Train Epoch: 0 [44800/51000 (88%)]\tLoss: -712.753662\n",
            "Train Epoch: 0 [46080/51000 (90%)]\tLoss: -734.005981\n",
            "Train Epoch: 0 [47360/51000 (93%)]\tLoss: -756.414734\n",
            "Train Epoch: 0 [48640/51000 (95%)]\tLoss: -778.896118\n",
            "Train Epoch: 0 [49920/51000 (98%)]\tLoss: -802.154297\n",
            "Test set: Average loss: -828.8310, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 1 [0/51000 (0%)]\tLoss: -819.831848\n",
            "Train Epoch: 1 [1280/51000 (3%)]\tLoss: -838.145813\n",
            "Train Epoch: 1 [2560/51000 (5%)]\tLoss: -853.902344\n",
            "Train Epoch: 1 [3840/51000 (8%)]\tLoss: -870.809509\n",
            "Train Epoch: 1 [5120/51000 (10%)]\tLoss: -885.799927\n",
            "Train Epoch: 1 [6400/51000 (13%)]\tLoss: -902.539734\n",
            "Train Epoch: 1 [7680/51000 (15%)]\tLoss: -918.814697\n",
            "Train Epoch: 1 [8960/51000 (18%)]\tLoss: -935.574402\n",
            "Train Epoch: 1 [10240/51000 (20%)]\tLoss: -950.933899\n",
            "Train Epoch: 1 [11520/51000 (23%)]\tLoss: -963.777954\n",
            "Train Epoch: 1 [12800/51000 (25%)]\tLoss: -981.755249\n",
            "Train Epoch: 1 [14080/51000 (28%)]\tLoss: -1001.107300\n",
            "Train Epoch: 1 [15360/51000 (30%)]\tLoss: -1018.036560\n",
            "Train Epoch: 1 [16640/51000 (33%)]\tLoss: -1040.717896\n",
            "Train Epoch: 1 [17920/51000 (35%)]\tLoss: -1064.638794\n",
            "Train Epoch: 1 [19200/51000 (38%)]\tLoss: -1101.024536\n",
            "Train Epoch: 1 [20480/51000 (40%)]\tLoss: -1126.712036\n",
            "Train Epoch: 1 [21760/51000 (43%)]\tLoss: -1147.770874\n",
            "Train Epoch: 1 [23040/51000 (45%)]\tLoss: -1168.137573\n",
            "Train Epoch: 1 [24320/51000 (48%)]\tLoss: -1186.858643\n",
            "Train Epoch: 1 [25600/51000 (50%)]\tLoss: -1209.345581\n",
            "Train Epoch: 1 [26880/51000 (53%)]\tLoss: -1225.958618\n",
            "Train Epoch: 1 [28160/51000 (55%)]\tLoss: -1247.283569\n",
            "Train Epoch: 1 [29440/51000 (58%)]\tLoss: -1267.359619\n",
            "Train Epoch: 1 [30720/51000 (60%)]\tLoss: -1284.869751\n",
            "Train Epoch: 1 [32000/51000 (63%)]\tLoss: -1307.794678\n",
            "Train Epoch: 1 [33280/51000 (65%)]\tLoss: -1326.573120\n",
            "Train Epoch: 1 [34560/51000 (68%)]\tLoss: -1344.357056\n",
            "Train Epoch: 1 [35840/51000 (70%)]\tLoss: -1367.376221\n",
            "Train Epoch: 1 [37120/51000 (73%)]\tLoss: -1386.671021\n",
            "Train Epoch: 1 [38400/51000 (75%)]\tLoss: -1404.197021\n",
            "Train Epoch: 1 [39680/51000 (78%)]\tLoss: -1428.803833\n",
            "Train Epoch: 1 [40960/51000 (80%)]\tLoss: -1447.484253\n",
            "Train Epoch: 1 [42240/51000 (83%)]\tLoss: -1469.355225\n",
            "Train Epoch: 1 [43520/51000 (85%)]\tLoss: -1487.535278\n",
            "Train Epoch: 1 [44800/51000 (88%)]\tLoss: -1511.685669\n",
            "Train Epoch: 1 [46080/51000 (90%)]\tLoss: -1531.425415\n",
            "Train Epoch: 1 [47360/51000 (93%)]\tLoss: -1553.142944\n",
            "Train Epoch: 1 [48640/51000 (95%)]\tLoss: -1572.906250\n",
            "Train Epoch: 1 [49920/51000 (98%)]\tLoss: -1596.894531\n",
            "Test set: Average loss: -1625.6120, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 2 [0/51000 (0%)]\tLoss: -1611.207642\n",
            "Train Epoch: 2 [1280/51000 (3%)]\tLoss: -1630.886963\n",
            "Train Epoch: 2 [2560/51000 (5%)]\tLoss: -1645.788696\n",
            "Train Epoch: 2 [3840/51000 (8%)]\tLoss: -1662.150391\n",
            "Train Epoch: 2 [5120/51000 (10%)]\tLoss: -1675.876099\n",
            "Train Epoch: 2 [6400/51000 (13%)]\tLoss: -1690.826660\n",
            "Train Epoch: 2 [7680/51000 (15%)]\tLoss: -1706.013916\n",
            "Train Epoch: 2 [8960/51000 (18%)]\tLoss: -1723.266968\n",
            "Train Epoch: 2 [10240/51000 (20%)]\tLoss: -1737.011353\n",
            "Train Epoch: 2 [11520/51000 (23%)]\tLoss: -1751.093750\n",
            "Train Epoch: 2 [12800/51000 (25%)]\tLoss: -1768.726929\n",
            "Train Epoch: 2 [14080/51000 (28%)]\tLoss: -1784.526733\n",
            "Train Epoch: 2 [15360/51000 (30%)]\tLoss: -1798.055420\n",
            "Train Epoch: 2 [16640/51000 (33%)]\tLoss: -1814.564697\n",
            "Train Epoch: 2 [17920/51000 (35%)]\tLoss: -1825.454834\n",
            "Train Epoch: 2 [19200/51000 (38%)]\tLoss: -1846.136841\n",
            "Train Epoch: 2 [20480/51000 (40%)]\tLoss: -1862.364258\n",
            "Train Epoch: 2 [21760/51000 (43%)]\tLoss: -1879.574097\n",
            "Train Epoch: 2 [23040/51000 (45%)]\tLoss: -1894.902344\n",
            "Train Epoch: 2 [24320/51000 (48%)]\tLoss: -1909.239014\n",
            "Train Epoch: 2 [25600/51000 (50%)]\tLoss: -1927.842773\n",
            "Train Epoch: 2 [26880/51000 (53%)]\tLoss: -1941.802246\n",
            "Train Epoch: 2 [28160/51000 (55%)]\tLoss: -1954.023438\n",
            "Train Epoch: 2 [29440/51000 (58%)]\tLoss: -1973.504639\n",
            "Train Epoch: 2 [30720/51000 (60%)]\tLoss: -1990.312012\n",
            "Train Epoch: 2 [32000/51000 (63%)]\tLoss: -2008.214600\n",
            "Train Epoch: 2 [33280/51000 (65%)]\tLoss: -2025.251343\n",
            "Train Epoch: 2 [34560/51000 (68%)]\tLoss: -2037.546143\n",
            "Train Epoch: 2 [35840/51000 (70%)]\tLoss: -2054.692383\n",
            "Train Epoch: 2 [37120/51000 (73%)]\tLoss: -2070.000000\n",
            "Train Epoch: 2 [38400/51000 (75%)]\tLoss: -2085.575195\n",
            "Train Epoch: 2 [39680/51000 (78%)]\tLoss: -2106.261963\n",
            "Train Epoch: 2 [40960/51000 (80%)]\tLoss: -2120.097656\n",
            "Train Epoch: 2 [42240/51000 (83%)]\tLoss: -2138.524414\n",
            "Train Epoch: 2 [43520/51000 (85%)]\tLoss: -2153.122070\n",
            "Train Epoch: 2 [44800/51000 (88%)]\tLoss: -2172.299316\n",
            "Train Epoch: 2 [46080/51000 (90%)]\tLoss: -2187.139404\n",
            "Train Epoch: 2 [47360/51000 (93%)]\tLoss: -2205.820801\n",
            "Train Epoch: 2 [48640/51000 (95%)]\tLoss: -2223.353760\n",
            "Train Epoch: 2 [49920/51000 (98%)]\tLoss: -2242.432129\n",
            "Test set: Average loss: -2304.6815, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 3 [0/51000 (0%)]\tLoss: -2252.907471\n",
            "Train Epoch: 3 [1280/51000 (3%)]\tLoss: -2268.916504\n",
            "Train Epoch: 3 [2560/51000 (5%)]\tLoss: -2278.724854\n",
            "Train Epoch: 3 [3840/51000 (8%)]\tLoss: -2293.170166\n",
            "Train Epoch: 3 [5120/51000 (10%)]\tLoss: -2303.868896\n",
            "Train Epoch: 3 [6400/51000 (13%)]\tLoss: -2314.037842\n",
            "Train Epoch: 3 [7680/51000 (15%)]\tLoss: -2324.956055\n",
            "Train Epoch: 3 [8960/51000 (18%)]\tLoss: -2341.211914\n",
            "Train Epoch: 3 [10240/51000 (20%)]\tLoss: -2354.856201\n",
            "Train Epoch: 3 [11520/51000 (23%)]\tLoss: -2365.138428\n",
            "Train Epoch: 3 [12800/51000 (25%)]\tLoss: -2378.153320\n",
            "Train Epoch: 3 [14080/51000 (28%)]\tLoss: -2390.903320\n",
            "Train Epoch: 3 [15360/51000 (30%)]\tLoss: -2400.471680\n",
            "Train Epoch: 3 [16640/51000 (33%)]\tLoss: -2413.017334\n",
            "Train Epoch: 3 [17920/51000 (35%)]\tLoss: -2425.064209\n",
            "Train Epoch: 3 [19200/51000 (38%)]\tLoss: -2440.637695\n",
            "Train Epoch: 3 [20480/51000 (40%)]\tLoss: -2451.642578\n",
            "Train Epoch: 3 [21760/51000 (43%)]\tLoss: -2465.044678\n",
            "Train Epoch: 3 [23040/51000 (45%)]\tLoss: -2476.732666\n",
            "Train Epoch: 3 [24320/51000 (48%)]\tLoss: -2487.328125\n",
            "Train Epoch: 3 [25600/51000 (50%)]\tLoss: -2502.857422\n",
            "Train Epoch: 3 [26880/51000 (53%)]\tLoss: -2511.691650\n",
            "Train Epoch: 3 [28160/51000 (55%)]\tLoss: -2526.125488\n",
            "Train Epoch: 3 [29440/51000 (58%)]\tLoss: -2539.131592\n",
            "Train Epoch: 3 [30720/51000 (60%)]\tLoss: -2550.453369\n",
            "Train Epoch: 3 [32000/51000 (63%)]\tLoss: -2564.873535\n",
            "Train Epoch: 3 [33280/51000 (65%)]\tLoss: -2578.456543\n",
            "Train Epoch: 3 [34560/51000 (68%)]\tLoss: -2584.898926\n",
            "Train Epoch: 3 [35840/51000 (70%)]\tLoss: -2602.107178\n",
            "Train Epoch: 3 [37120/51000 (73%)]\tLoss: -2613.600098\n",
            "Train Epoch: 3 [38400/51000 (75%)]\tLoss: -2625.237549\n",
            "Train Epoch: 3 [39680/51000 (78%)]\tLoss: -2642.057861\n",
            "Train Epoch: 3 [40960/51000 (80%)]\tLoss: -2650.405518\n",
            "Train Epoch: 3 [42240/51000 (83%)]\tLoss: -2663.279053\n",
            "Train Epoch: 3 [43520/51000 (85%)]\tLoss: -2677.047607\n",
            "Train Epoch: 3 [44800/51000 (88%)]\tLoss: -2690.899414\n",
            "Train Epoch: 3 [46080/51000 (90%)]\tLoss: -2701.444336\n",
            "Train Epoch: 3 [47360/51000 (93%)]\tLoss: -2717.101318\n",
            "Train Epoch: 3 [48640/51000 (95%)]\tLoss: -2730.106201\n",
            "Train Epoch: 3 [49920/51000 (98%)]\tLoss: -2744.423828\n",
            "Test set: Average loss: -2638.4866, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 4 [0/51000 (0%)]\tLoss: -2750.388672\n",
            "Train Epoch: 4 [1280/51000 (3%)]\tLoss: -2764.181152\n",
            "Train Epoch: 4 [2560/51000 (5%)]\tLoss: -2772.712646\n",
            "Train Epoch: 4 [3840/51000 (8%)]\tLoss: -2783.159180\n",
            "Train Epoch: 4 [5120/51000 (10%)]\tLoss: -2789.805176\n",
            "Train Epoch: 4 [6400/51000 (13%)]\tLoss: -2800.089600\n",
            "Train Epoch: 4 [7680/51000 (15%)]\tLoss: -2807.836426\n",
            "Train Epoch: 4 [8960/51000 (18%)]\tLoss: -2818.250244\n",
            "Train Epoch: 4 [10240/51000 (20%)]\tLoss: -2828.534668\n",
            "Train Epoch: 4 [11520/51000 (23%)]\tLoss: -2836.675293\n",
            "Train Epoch: 4 [12800/51000 (25%)]\tLoss: -2846.632812\n",
            "Train Epoch: 4 [14080/51000 (28%)]\tLoss: -2856.431885\n",
            "Train Epoch: 4 [15360/51000 (30%)]\tLoss: -2864.037842\n",
            "Train Epoch: 4 [16640/51000 (33%)]\tLoss: -2872.561768\n",
            "Train Epoch: 4 [17920/51000 (35%)]\tLoss: -2882.187744\n",
            "Train Epoch: 4 [19200/51000 (38%)]\tLoss: -2894.708008\n",
            "Train Epoch: 4 [20480/51000 (40%)]\tLoss: -2902.154297\n",
            "Train Epoch: 4 [21760/51000 (43%)]\tLoss: -2912.686523\n",
            "Train Epoch: 4 [23040/51000 (45%)]\tLoss: -2919.702881\n",
            "Train Epoch: 4 [24320/51000 (48%)]\tLoss: -2926.994385\n",
            "Train Epoch: 4 [25600/51000 (50%)]\tLoss: -2940.060791\n",
            "Train Epoch: 4 [26880/51000 (53%)]\tLoss: -2946.584229\n",
            "Train Epoch: 4 [28160/51000 (55%)]\tLoss: -2957.595459\n",
            "Train Epoch: 4 [29440/51000 (58%)]\tLoss: -2967.767822\n",
            "Train Epoch: 4 [30720/51000 (60%)]\tLoss: -2974.818604\n",
            "Train Epoch: 4 [32000/51000 (63%)]\tLoss: -2987.236816\n",
            "Train Epoch: 4 [33280/51000 (65%)]\tLoss: -2995.275879\n",
            "Train Epoch: 4 [34560/51000 (68%)]\tLoss: -3000.696533\n",
            "Train Epoch: 4 [35840/51000 (70%)]\tLoss: -3014.344727\n",
            "Train Epoch: 4 [37120/51000 (73%)]\tLoss: -3021.875000\n",
            "Train Epoch: 4 [38400/51000 (75%)]\tLoss: -3030.716064\n",
            "Train Epoch: 4 [39680/51000 (78%)]\tLoss: -3044.060059\n",
            "Train Epoch: 4 [40960/51000 (80%)]\tLoss: -3051.571533\n",
            "Train Epoch: 4 [42240/51000 (83%)]\tLoss: -3062.226562\n",
            "Train Epoch: 4 [43520/51000 (85%)]\tLoss: -3067.656006\n",
            "Train Epoch: 4 [44800/51000 (88%)]\tLoss: -3078.381592\n",
            "Train Epoch: 4 [46080/51000 (90%)]\tLoss: -3087.232910\n",
            "Train Epoch: 4 [47360/51000 (93%)]\tLoss: -3100.157959\n",
            "Train Epoch: 4 [48640/51000 (95%)]\tLoss: -3107.880859\n",
            "Train Epoch: 4 [49920/51000 (98%)]\tLoss: -3119.237061\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-04-11 06:44:40,064]\u001b[0m Trial 2 finished with value: -3105.170434027778 and parameters: {'lr': 0.0007200561133814223, 'optimizer_name': 'RMSprop'}. Best is trial 1 with value: -3050000.351111111.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test set: Average loss: -3105.1704, Accuracy: 895/9000 (10%)\n",
            "\n",
            "\n",
            "********************************\n",
            "\n",
            "Suggested hyperparameters: \n",
            "{'lr': 0.0043463293390448, 'optimizer_name': 'Adam'}\n",
            "Train Epoch: 0 [0/51000 (0%)]\tLoss: -0.121064\n",
            "Train Epoch: 0 [1280/51000 (3%)]\tLoss: -41.785236\n",
            "Train Epoch: 0 [2560/51000 (5%)]\tLoss: -98.294136\n",
            "Train Epoch: 0 [3840/51000 (8%)]\tLoss: -166.291428\n",
            "Train Epoch: 0 [5120/51000 (10%)]\tLoss: -247.373489\n",
            "Train Epoch: 0 [6400/51000 (13%)]\tLoss: -343.872955\n",
            "Train Epoch: 0 [7680/51000 (15%)]\tLoss: -454.628937\n",
            "Train Epoch: 0 [8960/51000 (18%)]\tLoss: -580.339355\n",
            "Train Epoch: 0 [10240/51000 (20%)]\tLoss: -730.499023\n",
            "Train Epoch: 0 [11520/51000 (23%)]\tLoss: -892.783264\n",
            "Train Epoch: 0 [12800/51000 (25%)]\tLoss: -1074.003296\n",
            "Train Epoch: 0 [14080/51000 (28%)]\tLoss: -1276.648071\n",
            "Train Epoch: 0 [15360/51000 (30%)]\tLoss: -1494.029907\n",
            "Train Epoch: 0 [16640/51000 (33%)]\tLoss: -1730.904785\n",
            "Train Epoch: 0 [17920/51000 (35%)]\tLoss: -1997.436523\n",
            "Train Epoch: 0 [19200/51000 (38%)]\tLoss: -2278.290771\n",
            "Train Epoch: 0 [20480/51000 (40%)]\tLoss: -2623.520752\n",
            "Train Epoch: 0 [21760/51000 (43%)]\tLoss: -3057.710449\n",
            "Train Epoch: 0 [23040/51000 (45%)]\tLoss: -3479.892090\n",
            "Train Epoch: 0 [24320/51000 (48%)]\tLoss: -3914.879395\n",
            "Train Epoch: 0 [25600/51000 (50%)]\tLoss: -4363.326660\n",
            "Train Epoch: 0 [26880/51000 (53%)]\tLoss: -4839.757324\n",
            "Train Epoch: 0 [28160/51000 (55%)]\tLoss: -5339.827637\n",
            "Train Epoch: 0 [29440/51000 (58%)]\tLoss: -5875.966309\n",
            "Train Epoch: 0 [30720/51000 (60%)]\tLoss: -6423.622070\n",
            "Train Epoch: 0 [32000/51000 (63%)]\tLoss: -6992.373047\n",
            "Train Epoch: 0 [33280/51000 (65%)]\tLoss: -7593.434570\n",
            "Train Epoch: 0 [34560/51000 (68%)]\tLoss: -8227.244141\n",
            "Train Epoch: 0 [35840/51000 (70%)]\tLoss: -8895.543945\n",
            "Train Epoch: 0 [37120/51000 (73%)]\tLoss: -9558.047852\n",
            "Train Epoch: 0 [38400/51000 (75%)]\tLoss: -10258.349609\n",
            "Train Epoch: 0 [39680/51000 (78%)]\tLoss: -10978.989258\n",
            "Train Epoch: 0 [40960/51000 (80%)]\tLoss: -11724.922852\n",
            "Train Epoch: 0 [42240/51000 (83%)]\tLoss: -12494.078125\n",
            "Train Epoch: 0 [43520/51000 (85%)]\tLoss: -13295.183594\n",
            "Train Epoch: 0 [44800/51000 (88%)]\tLoss: -14110.208008\n",
            "Train Epoch: 0 [46080/51000 (90%)]\tLoss: -14954.124023\n",
            "Train Epoch: 0 [47360/51000 (93%)]\tLoss: -15800.656250\n",
            "Train Epoch: 0 [48640/51000 (95%)]\tLoss: -16677.310547\n",
            "Train Epoch: 0 [49920/51000 (98%)]\tLoss: -17614.929688\n",
            "Test set: Average loss: -19076.3135, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 1 [0/51000 (0%)]\tLoss: -18436.623047\n",
            "Train Epoch: 1 [1280/51000 (3%)]\tLoss: -19102.919922\n",
            "Train Epoch: 1 [2560/51000 (5%)]\tLoss: -19756.751953\n",
            "Train Epoch: 1 [3840/51000 (8%)]\tLoss: -20472.162109\n",
            "Train Epoch: 1 [5120/51000 (10%)]\tLoss: -21146.546875\n",
            "Train Epoch: 1 [6400/51000 (13%)]\tLoss: -21840.660156\n",
            "Train Epoch: 1 [7680/51000 (15%)]\tLoss: -22529.023438\n",
            "Train Epoch: 1 [8960/51000 (18%)]\tLoss: -23245.244141\n",
            "Train Epoch: 1 [10240/51000 (20%)]\tLoss: -23988.998047\n",
            "Train Epoch: 1 [11520/51000 (23%)]\tLoss: -24679.042969\n",
            "Train Epoch: 1 [12800/51000 (25%)]\tLoss: -25433.013672\n",
            "Train Epoch: 1 [14080/51000 (28%)]\tLoss: -26167.888672\n",
            "Train Epoch: 1 [15360/51000 (30%)]\tLoss: -26896.171875\n",
            "Train Epoch: 1 [16640/51000 (33%)]\tLoss: -27643.058594\n",
            "Train Epoch: 1 [17920/51000 (35%)]\tLoss: -28377.644531\n",
            "Train Epoch: 1 [19200/51000 (38%)]\tLoss: -29174.068359\n",
            "Train Epoch: 1 [20480/51000 (40%)]\tLoss: -29957.902344\n",
            "Train Epoch: 1 [21760/51000 (43%)]\tLoss: -30745.398438\n",
            "Train Epoch: 1 [23040/51000 (45%)]\tLoss: -31541.414062\n",
            "Train Epoch: 1 [24320/51000 (48%)]\tLoss: -32325.980469\n",
            "Train Epoch: 1 [25600/51000 (50%)]\tLoss: -33154.718750\n",
            "Train Epoch: 1 [26880/51000 (53%)]\tLoss: -33969.937500\n",
            "Train Epoch: 1 [28160/51000 (55%)]\tLoss: -34828.277344\n",
            "Train Epoch: 1 [29440/51000 (58%)]\tLoss: -35656.226562\n",
            "Train Epoch: 1 [30720/51000 (60%)]\tLoss: -36502.511719\n",
            "Train Epoch: 1 [32000/51000 (63%)]\tLoss: -37368.175781\n",
            "Train Epoch: 1 [33280/51000 (65%)]\tLoss: -38220.617188\n",
            "Train Epoch: 1 [34560/51000 (68%)]\tLoss: -39074.664062\n",
            "Train Epoch: 1 [35840/51000 (70%)]\tLoss: -39985.914062\n",
            "Train Epoch: 1 [37120/51000 (73%)]\tLoss: -40805.277344\n",
            "Train Epoch: 1 [38400/51000 (75%)]\tLoss: -41721.460938\n",
            "Train Epoch: 1 [39680/51000 (78%)]\tLoss: -42659.089844\n",
            "Train Epoch: 1 [40960/51000 (80%)]\tLoss: -43545.011719\n",
            "Train Epoch: 1 [42240/51000 (83%)]\tLoss: -44477.273438\n",
            "Train Epoch: 1 [43520/51000 (85%)]\tLoss: -45372.761719\n",
            "Train Epoch: 1 [44800/51000 (88%)]\tLoss: -46341.675781\n",
            "Train Epoch: 1 [46080/51000 (90%)]\tLoss: -47290.070312\n",
            "Train Epoch: 1 [47360/51000 (93%)]\tLoss: -48245.351562\n",
            "Train Epoch: 1 [48640/51000 (95%)]\tLoss: -49190.289062\n",
            "Train Epoch: 1 [49920/51000 (98%)]\tLoss: -50204.328125\n",
            "Test set: Average loss: -51750.7181, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 2 [0/51000 (0%)]\tLoss: -51037.222656\n",
            "Train Epoch: 2 [1280/51000 (3%)]\tLoss: -51745.339844\n",
            "Train Epoch: 2 [2560/51000 (5%)]\tLoss: -52449.804688\n",
            "Train Epoch: 2 [3840/51000 (8%)]\tLoss: -53160.085938\n",
            "Train Epoch: 2 [5120/51000 (10%)]\tLoss: -53821.750000\n",
            "Train Epoch: 2 [6400/51000 (13%)]\tLoss: -54528.000000\n",
            "Train Epoch: 2 [7680/51000 (15%)]\tLoss: -55227.816406\n",
            "Train Epoch: 2 [8960/51000 (18%)]\tLoss: -55931.390625\n",
            "Train Epoch: 2 [10240/51000 (20%)]\tLoss: -56680.871094\n",
            "Train Epoch: 2 [11520/51000 (23%)]\tLoss: -57336.421875\n",
            "Train Epoch: 2 [12800/51000 (25%)]\tLoss: -58068.078125\n",
            "Train Epoch: 2 [14080/51000 (28%)]\tLoss: -58802.660156\n",
            "Train Epoch: 2 [15360/51000 (30%)]\tLoss: -59488.589844\n",
            "Train Epoch: 2 [16640/51000 (33%)]\tLoss: -60220.734375\n",
            "Train Epoch: 2 [17920/51000 (35%)]\tLoss: -60933.351562\n",
            "Train Epoch: 2 [19200/51000 (38%)]\tLoss: -61694.960938\n",
            "Train Epoch: 2 [20480/51000 (40%)]\tLoss: -62405.960938\n",
            "Train Epoch: 2 [21760/51000 (43%)]\tLoss: -63139.902344\n",
            "Train Epoch: 2 [23040/51000 (45%)]\tLoss: -63829.625000\n",
            "Train Epoch: 2 [24320/51000 (48%)]\tLoss: -64584.675781\n",
            "Train Epoch: 2 [25600/51000 (50%)]\tLoss: -65357.957031\n",
            "Train Epoch: 2 [26880/51000 (53%)]\tLoss: -66047.156250\n",
            "Train Epoch: 2 [28160/51000 (55%)]\tLoss: -66802.015625\n",
            "Train Epoch: 2 [29440/51000 (58%)]\tLoss: -67561.320312\n",
            "Train Epoch: 2 [30720/51000 (60%)]\tLoss: -68315.726562\n",
            "Train Epoch: 2 [32000/51000 (63%)]\tLoss: -69082.679688\n",
            "Train Epoch: 2 [33280/51000 (65%)]\tLoss: -69840.445312\n",
            "Train Epoch: 2 [34560/51000 (68%)]\tLoss: -70560.070312\n",
            "Train Epoch: 2 [35840/51000 (70%)]\tLoss: -71368.398438\n",
            "Train Epoch: 2 [37120/51000 (73%)]\tLoss: -72054.968750\n",
            "Train Epoch: 2 [38400/51000 (75%)]\tLoss: -72843.367188\n",
            "Train Epoch: 2 [39680/51000 (78%)]\tLoss: -73675.843750\n",
            "Train Epoch: 2 [40960/51000 (80%)]\tLoss: -74388.875000\n",
            "Train Epoch: 2 [42240/51000 (83%)]\tLoss: -75192.046875\n",
            "Train Epoch: 2 [43520/51000 (85%)]\tLoss: -75949.710938\n",
            "Train Epoch: 2 [44800/51000 (88%)]\tLoss: -76733.632812\n",
            "Train Epoch: 2 [46080/51000 (90%)]\tLoss: -77524.000000\n",
            "Train Epoch: 2 [47360/51000 (93%)]\tLoss: -78341.796875\n",
            "Train Epoch: 2 [48640/51000 (95%)]\tLoss: -79106.281250\n",
            "Train Epoch: 2 [49920/51000 (98%)]\tLoss: -79953.445312\n",
            "Test set: Average loss: -80883.5367, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 3 [0/51000 (0%)]\tLoss: -80586.750000\n",
            "Train Epoch: 3 [1280/51000 (3%)]\tLoss: -81192.812500\n",
            "Train Epoch: 3 [2560/51000 (5%)]\tLoss: -81744.875000\n",
            "Train Epoch: 3 [3840/51000 (8%)]\tLoss: -82336.296875\n",
            "Train Epoch: 3 [5120/51000 (10%)]\tLoss: -82848.796875\n",
            "Train Epoch: 3 [6400/51000 (13%)]\tLoss: -83392.710938\n",
            "Train Epoch: 3 [7680/51000 (15%)]\tLoss: -83958.757812\n",
            "Train Epoch: 3 [8960/51000 (18%)]\tLoss: -84514.960938\n",
            "Train Epoch: 3 [10240/51000 (20%)]\tLoss: -85132.203125\n",
            "Train Epoch: 3 [11520/51000 (23%)]\tLoss: -85600.265625\n",
            "Train Epoch: 3 [12800/51000 (25%)]\tLoss: -86197.632812\n",
            "Train Epoch: 3 [14080/51000 (28%)]\tLoss: -86814.960938\n",
            "Train Epoch: 3 [15360/51000 (30%)]\tLoss: -87276.320312\n",
            "Train Epoch: 3 [16640/51000 (33%)]\tLoss: -87872.593750\n",
            "Train Epoch: 3 [17920/51000 (35%)]\tLoss: -88475.656250\n",
            "Train Epoch: 3 [19200/51000 (38%)]\tLoss: -89102.656250\n",
            "Train Epoch: 3 [20480/51000 (40%)]\tLoss: -89640.804688\n",
            "Train Epoch: 3 [21760/51000 (43%)]\tLoss: -90227.992188\n",
            "Train Epoch: 3 [23040/51000 (45%)]\tLoss: -90777.187500\n",
            "Train Epoch: 3 [24320/51000 (48%)]\tLoss: -91351.296875\n",
            "Train Epoch: 3 [25600/51000 (50%)]\tLoss: -91986.234375\n",
            "Train Epoch: 3 [26880/51000 (53%)]\tLoss: -92447.523438\n",
            "Train Epoch: 3 [28160/51000 (55%)]\tLoss: -93123.750000\n",
            "Train Epoch: 3 [29440/51000 (58%)]\tLoss: -93673.242188\n",
            "Train Epoch: 3 [30720/51000 (60%)]\tLoss: -94273.203125\n",
            "Train Epoch: 3 [32000/51000 (63%)]\tLoss: -94849.164062\n",
            "Train Epoch: 3 [33280/51000 (65%)]\tLoss: -95425.078125\n",
            "Train Epoch: 3 [34560/51000 (68%)]\tLoss: -95979.578125\n",
            "Train Epoch: 3 [35840/51000 (70%)]\tLoss: -96642.164062\n",
            "Train Epoch: 3 [37120/51000 (73%)]\tLoss: -97175.757812\n",
            "Train Epoch: 3 [38400/51000 (75%)]\tLoss: -97727.976562\n",
            "Train Epoch: 3 [39680/51000 (78%)]\tLoss: -98369.335938\n",
            "Train Epoch: 3 [40960/51000 (80%)]\tLoss: -98913.835938\n",
            "Train Epoch: 3 [42240/51000 (83%)]\tLoss: -99515.234375\n",
            "Train Epoch: 3 [43520/51000 (85%)]\tLoss: -100101.554688\n",
            "Train Epoch: 3 [44800/51000 (88%)]\tLoss: -100654.210938\n",
            "Train Epoch: 3 [46080/51000 (90%)]\tLoss: -101295.726562\n",
            "Train Epoch: 3 [47360/51000 (93%)]\tLoss: -101899.867188\n",
            "Train Epoch: 3 [48640/51000 (95%)]\tLoss: -102459.468750\n",
            "Train Epoch: 3 [49920/51000 (98%)]\tLoss: -103120.445312\n",
            "Test set: Average loss: -103882.0502, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 4 [0/51000 (0%)]\tLoss: -103488.476562\n",
            "Train Epoch: 4 [1280/51000 (3%)]\tLoss: -103996.273438\n",
            "Train Epoch: 4 [2560/51000 (5%)]\tLoss: -104459.781250\n",
            "Train Epoch: 4 [3840/51000 (8%)]\tLoss: -104890.875000\n",
            "Train Epoch: 4 [5120/51000 (10%)]\tLoss: -105229.054688\n",
            "Train Epoch: 4 [6400/51000 (13%)]\tLoss: -105661.750000\n",
            "Train Epoch: 4 [7680/51000 (15%)]\tLoss: -106100.078125\n",
            "Train Epoch: 4 [8960/51000 (18%)]\tLoss: -106495.804688\n",
            "Train Epoch: 4 [10240/51000 (20%)]\tLoss: -107002.843750\n",
            "Train Epoch: 4 [11520/51000 (23%)]\tLoss: -107315.531250\n",
            "Train Epoch: 4 [12800/51000 (25%)]\tLoss: -107794.710938\n",
            "Train Epoch: 4 [14080/51000 (28%)]\tLoss: -108244.578125\n",
            "Train Epoch: 4 [15360/51000 (30%)]\tLoss: -108590.171875\n",
            "Train Epoch: 4 [16640/51000 (33%)]\tLoss: -109054.382812\n",
            "Train Epoch: 4 [17920/51000 (35%)]\tLoss: -109443.242188\n",
            "Train Epoch: 4 [19200/51000 (38%)]\tLoss: -109949.585938\n",
            "Train Epoch: 4 [20480/51000 (40%)]\tLoss: -110250.023438\n",
            "Train Epoch: 4 [21760/51000 (43%)]\tLoss: -110748.109375\n",
            "Train Epoch: 4 [23040/51000 (45%)]\tLoss: -111126.882812\n",
            "Train Epoch: 4 [24320/51000 (48%)]\tLoss: -111521.796875\n",
            "Train Epoch: 4 [25600/51000 (50%)]\tLoss: -112017.289062\n",
            "Train Epoch: 4 [26880/51000 (53%)]\tLoss: -112342.265625\n",
            "Train Epoch: 4 [28160/51000 (55%)]\tLoss: -112829.578125\n",
            "Train Epoch: 4 [29440/51000 (58%)]\tLoss: -113253.328125\n",
            "Train Epoch: 4 [30720/51000 (60%)]\tLoss: -113681.234375\n",
            "Train Epoch: 4 [32000/51000 (63%)]\tLoss: -114123.500000\n",
            "Train Epoch: 4 [33280/51000 (65%)]\tLoss: -114522.062500\n",
            "Train Epoch: 4 [34560/51000 (68%)]\tLoss: -114896.296875\n",
            "Train Epoch: 4 [35840/51000 (70%)]\tLoss: -115394.898438\n",
            "Train Epoch: 4 [37120/51000 (73%)]\tLoss: -115738.750000\n",
            "Train Epoch: 4 [38400/51000 (75%)]\tLoss: -116161.953125\n",
            "Train Epoch: 4 [39680/51000 (78%)]\tLoss: -116681.000000\n",
            "Train Epoch: 4 [40960/51000 (80%)]\tLoss: -117071.312500\n",
            "Train Epoch: 4 [42240/51000 (83%)]\tLoss: -117480.593750\n",
            "Train Epoch: 4 [43520/51000 (85%)]\tLoss: -117891.671875\n",
            "Train Epoch: 4 [44800/51000 (88%)]\tLoss: -118290.851562\n",
            "Train Epoch: 4 [46080/51000 (90%)]\tLoss: -118750.148438\n",
            "Train Epoch: 4 [47360/51000 (93%)]\tLoss: -119188.703125\n",
            "Train Epoch: 4 [48640/51000 (95%)]\tLoss: -119578.203125\n",
            "Train Epoch: 4 [49920/51000 (98%)]\tLoss: -120100.273438\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-04-11 07:12:37,328]\u001b[0m Trial 3 finished with value: -121176.02055555556 and parameters: {'lr': 0.0043463293390448, 'optimizer_name': 'Adam'}. Best is trial 1 with value: -3050000.351111111.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test set: Average loss: -121176.0206, Accuracy: 895/9000 (10%)\n",
            "\n",
            "\n",
            "********************************\n",
            "\n",
            "Suggested hyperparameters: \n",
            "{'lr': 0.055366333760817794, 'optimizer_name': 'Adam'}\n",
            "Train Epoch: 0 [0/51000 (0%)]\tLoss: -0.522275\n",
            "Train Epoch: 0 [1280/51000 (3%)]\tLoss: -1014.849548\n",
            "Train Epoch: 0 [2560/51000 (5%)]\tLoss: -3649.782959\n",
            "Train Epoch: 0 [3840/51000 (8%)]\tLoss: -8687.915039\n",
            "Train Epoch: 0 [5120/51000 (10%)]\tLoss: -16615.343750\n",
            "Train Epoch: 0 [6400/51000 (13%)]\tLoss: -27795.451172\n",
            "Train Epoch: 0 [7680/51000 (15%)]\tLoss: -42533.246094\n",
            "Train Epoch: 0 [8960/51000 (18%)]\tLoss: -61120.746094\n",
            "Train Epoch: 0 [10240/51000 (20%)]\tLoss: -83158.343750\n",
            "Train Epoch: 0 [11520/51000 (23%)]\tLoss: -110001.406250\n",
            "Train Epoch: 0 [12800/51000 (25%)]\tLoss: -139945.687500\n",
            "Train Epoch: 0 [14080/51000 (28%)]\tLoss: -174682.703125\n",
            "Train Epoch: 0 [15360/51000 (30%)]\tLoss: -213399.921875\n",
            "Train Epoch: 0 [16640/51000 (33%)]\tLoss: -256270.218750\n",
            "Train Epoch: 0 [17920/51000 (35%)]\tLoss: -303389.000000\n",
            "Train Epoch: 0 [19200/51000 (38%)]\tLoss: -353603.375000\n",
            "Train Epoch: 0 [20480/51000 (40%)]\tLoss: -410252.406250\n",
            "Train Epoch: 0 [21760/51000 (43%)]\tLoss: -468946.687500\n",
            "Train Epoch: 0 [23040/51000 (45%)]\tLoss: -532493.437500\n",
            "Train Epoch: 0 [24320/51000 (48%)]\tLoss: -600536.812500\n",
            "Train Epoch: 0 [25600/51000 (50%)]\tLoss: -672501.812500\n",
            "Train Epoch: 0 [26880/51000 (53%)]\tLoss: -748158.000000\n",
            "Train Epoch: 0 [28160/51000 (55%)]\tLoss: -826658.687500\n",
            "Train Epoch: 0 [29440/51000 (58%)]\tLoss: -911048.812500\n",
            "Train Epoch: 0 [30720/51000 (60%)]\tLoss: -998794.187500\n",
            "Train Epoch: 0 [32000/51000 (63%)]\tLoss: -1090883.375000\n",
            "Train Epoch: 0 [33280/51000 (65%)]\tLoss: -1188261.375000\n",
            "Train Epoch: 0 [34560/51000 (68%)]\tLoss: -1288269.500000\n",
            "Train Epoch: 0 [35840/51000 (70%)]\tLoss: -1394545.875000\n",
            "Train Epoch: 0 [37120/51000 (73%)]\tLoss: -1499770.125000\n",
            "Train Epoch: 0 [38400/51000 (75%)]\tLoss: -1612861.500000\n",
            "Train Epoch: 0 [39680/51000 (78%)]\tLoss: -1730968.125000\n",
            "Train Epoch: 0 [40960/51000 (80%)]\tLoss: -1842958.125000\n",
            "Train Epoch: 0 [42240/51000 (83%)]\tLoss: -1970967.875000\n",
            "Train Epoch: 0 [43520/51000 (85%)]\tLoss: -2099171.500000\n",
            "Train Epoch: 0 [44800/51000 (88%)]\tLoss: -2231509.500000\n",
            "Train Epoch: 0 [46080/51000 (90%)]\tLoss: -2363433.750000\n",
            "Train Epoch: 0 [47360/51000 (93%)]\tLoss: -2504948.000000\n",
            "Train Epoch: 0 [48640/51000 (95%)]\tLoss: -2647441.500000\n",
            "Train Epoch: 0 [49920/51000 (98%)]\tLoss: -2797723.250000\n",
            "Test set: Average loss: -2943518.3467, Accuracy: 1030/9000 (11%)\n",
            "\n",
            "Train Epoch: 1 [0/51000 (0%)]\tLoss: -2930712.000000\n",
            "Train Epoch: 1 [1280/51000 (3%)]\tLoss: -3038053.000000\n",
            "Train Epoch: 1 [2560/51000 (5%)]\tLoss: -3147559.500000\n",
            "Train Epoch: 1 [3840/51000 (8%)]\tLoss: -3256989.000000\n",
            "Train Epoch: 1 [5120/51000 (10%)]\tLoss: -3367610.000000\n",
            "Train Epoch: 1 [6400/51000 (13%)]\tLoss: -3480421.000000\n",
            "Train Epoch: 1 [7680/51000 (15%)]\tLoss: -3592142.500000\n",
            "Train Epoch: 1 [8960/51000 (18%)]\tLoss: -3705179.500000\n",
            "Train Epoch: 1 [10240/51000 (20%)]\tLoss: -3826873.500000\n",
            "Train Epoch: 1 [11520/51000 (23%)]\tLoss: -3936875.750000\n",
            "Train Epoch: 1 [12800/51000 (25%)]\tLoss: -4060064.000000\n",
            "Train Epoch: 1 [14080/51000 (28%)]\tLoss: -4174150.250000\n",
            "Train Epoch: 1 [15360/51000 (30%)]\tLoss: -4295224.500000\n",
            "Train Epoch: 1 [16640/51000 (33%)]\tLoss: -4418646.000000\n",
            "Train Epoch: 1 [17920/51000 (35%)]\tLoss: -4540539.000000\n",
            "Train Epoch: 1 [19200/51000 (38%)]\tLoss: -4667019.000000\n",
            "Train Epoch: 1 [20480/51000 (40%)]\tLoss: -4789505.000000\n",
            "Train Epoch: 1 [21760/51000 (43%)]\tLoss: -4918859.500000\n",
            "Train Epoch: 1 [23040/51000 (45%)]\tLoss: -5044532.000000\n",
            "Train Epoch: 1 [24320/51000 (48%)]\tLoss: -5171623.000000\n",
            "Train Epoch: 1 [25600/51000 (50%)]\tLoss: -5306098.500000\n",
            "Train Epoch: 1 [26880/51000 (53%)]\tLoss: -5435435.500000\n",
            "Train Epoch: 1 [28160/51000 (55%)]\tLoss: -5573263.500000\n",
            "Train Epoch: 1 [29440/51000 (58%)]\tLoss: -5705035.000000\n",
            "Train Epoch: 1 [30720/51000 (60%)]\tLoss: -5838328.000000\n",
            "Train Epoch: 1 [32000/51000 (63%)]\tLoss: -5978715.500000\n",
            "Train Epoch: 1 [33280/51000 (65%)]\tLoss: -6117851.000000\n",
            "Train Epoch: 1 [34560/51000 (68%)]\tLoss: -6257998.500000\n",
            "Train Epoch: 1 [35840/51000 (70%)]\tLoss: -6400175.000000\n",
            "Train Epoch: 1 [37120/51000 (73%)]\tLoss: -6543306.000000\n",
            "Train Epoch: 1 [38400/51000 (75%)]\tLoss: -6684276.500000\n",
            "Train Epoch: 1 [39680/51000 (78%)]\tLoss: -6837151.000000\n",
            "Train Epoch: 1 [40960/51000 (80%)]\tLoss: -6980686.000000\n",
            "Train Epoch: 1 [42240/51000 (83%)]\tLoss: -7127811.500000\n",
            "Train Epoch: 1 [43520/51000 (85%)]\tLoss: -7279147.000000\n",
            "Train Epoch: 1 [44800/51000 (88%)]\tLoss: -7432181.500000\n",
            "Train Epoch: 1 [46080/51000 (90%)]\tLoss: -7584249.000000\n",
            "Train Epoch: 1 [47360/51000 (93%)]\tLoss: -7738670.000000\n",
            "Train Epoch: 1 [48640/51000 (95%)]\tLoss: -7891513.000000\n",
            "Train Epoch: 1 [49920/51000 (98%)]\tLoss: -8055446.500000\n",
            "Test set: Average loss: -8191936.2489, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 2 [0/51000 (0%)]\tLoss: -8190438.000000\n",
            "Train Epoch: 2 [1280/51000 (3%)]\tLoss: -8304676.000000\n",
            "Train Epoch: 2 [2560/51000 (5%)]\tLoss: -8418191.000000\n",
            "Train Epoch: 2 [3840/51000 (8%)]\tLoss: -8531542.000000\n",
            "Train Epoch: 2 [5120/51000 (10%)]\tLoss: -8638838.000000\n",
            "Train Epoch: 2 [6400/51000 (13%)]\tLoss: -8752191.000000\n",
            "Train Epoch: 2 [7680/51000 (15%)]\tLoss: -8866909.000000\n",
            "Train Epoch: 2 [8960/51000 (18%)]\tLoss: -8979127.000000\n",
            "Train Epoch: 2 [10240/51000 (20%)]\tLoss: -9101807.000000\n",
            "Train Epoch: 2 [11520/51000 (23%)]\tLoss: -9206119.000000\n",
            "Train Epoch: 2 [12800/51000 (25%)]\tLoss: -9326943.000000\n",
            "Train Epoch: 2 [14080/51000 (28%)]\tLoss: -9440971.000000\n",
            "Train Epoch: 2 [15360/51000 (30%)]\tLoss: -9553456.000000\n",
            "Train Epoch: 2 [16640/51000 (33%)]\tLoss: -9672673.000000\n",
            "Train Epoch: 2 [17920/51000 (35%)]\tLoss: -9786686.000000\n",
            "Train Epoch: 2 [19200/51000 (38%)]\tLoss: -9911857.000000\n",
            "Train Epoch: 2 [20480/51000 (40%)]\tLoss: -10021827.000000\n",
            "Train Epoch: 2 [21760/51000 (43%)]\tLoss: -10144624.000000\n",
            "Train Epoch: 2 [23040/51000 (45%)]\tLoss: -10258142.000000\n",
            "Train Epoch: 2 [24320/51000 (48%)]\tLoss: -10376971.000000\n",
            "Train Epoch: 2 [25600/51000 (50%)]\tLoss: -10499785.000000\n",
            "Train Epoch: 2 [26880/51000 (53%)]\tLoss: -10612235.000000\n",
            "Train Epoch: 2 [28160/51000 (55%)]\tLoss: -10740455.000000\n",
            "Train Epoch: 2 [29440/51000 (58%)]\tLoss: -10858278.000000\n",
            "Train Epoch: 2 [30720/51000 (60%)]\tLoss: -10978919.000000\n",
            "Train Epoch: 2 [32000/51000 (63%)]\tLoss: -11102959.000000\n",
            "Train Epoch: 2 [33280/51000 (65%)]\tLoss: -11222679.000000\n",
            "Train Epoch: 2 [34560/51000 (68%)]\tLoss: -11342503.000000\n",
            "Train Epoch: 2 [35840/51000 (70%)]\tLoss: -11469905.000000\n",
            "Train Epoch: 2 [37120/51000 (73%)]\tLoss: -11593627.000000\n",
            "Train Epoch: 2 [38400/51000 (75%)]\tLoss: -11711609.000000\n",
            "Train Epoch: 2 [39680/51000 (78%)]\tLoss: -11846665.000000\n",
            "Train Epoch: 2 [40960/51000 (80%)]\tLoss: -11965545.000000\n",
            "Train Epoch: 2 [42240/51000 (83%)]\tLoss: -12090651.000000\n",
            "Train Epoch: 2 [43520/51000 (85%)]\tLoss: -12215468.000000\n",
            "Train Epoch: 2 [44800/51000 (88%)]\tLoss: -12342319.000000\n",
            "Train Epoch: 2 [46080/51000 (90%)]\tLoss: -12468202.000000\n",
            "Train Epoch: 2 [47360/51000 (93%)]\tLoss: -12598109.000000\n",
            "Train Epoch: 2 [48640/51000 (95%)]\tLoss: -12721085.000000\n",
            "Train Epoch: 2 [49920/51000 (98%)]\tLoss: -12859098.000000\n",
            "Test set: Average loss: -12976272.7573, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 3 [0/51000 (0%)]\tLoss: -12963634.000000\n",
            "Train Epoch: 3 [1280/51000 (3%)]\tLoss: -13059239.000000\n",
            "Train Epoch: 3 [2560/51000 (5%)]\tLoss: -13151996.000000\n",
            "Train Epoch: 3 [3840/51000 (8%)]\tLoss: -13244285.000000\n",
            "Train Epoch: 3 [5120/51000 (10%)]\tLoss: -13327052.000000\n",
            "Train Epoch: 3 [6400/51000 (13%)]\tLoss: -13419600.000000\n",
            "Train Epoch: 3 [7680/51000 (15%)]\tLoss: -13513738.000000\n",
            "Train Epoch: 3 [8960/51000 (18%)]\tLoss: -13600595.000000\n",
            "Train Epoch: 3 [10240/51000 (20%)]\tLoss: -13704293.000000\n",
            "Train Epoch: 3 [11520/51000 (23%)]\tLoss: -13780919.000000\n",
            "Train Epoch: 3 [12800/51000 (25%)]\tLoss: -13881858.000000\n",
            "Train Epoch: 3 [14080/51000 (28%)]\tLoss: -13972307.000000\n",
            "Train Epoch: 3 [15360/51000 (30%)]\tLoss: -14058744.000000\n",
            "Train Epoch: 3 [16640/51000 (33%)]\tLoss: -14154337.000000\n",
            "Train Epoch: 3 [17920/51000 (35%)]\tLoss: -14244248.000000\n",
            "Train Epoch: 3 [19200/51000 (38%)]\tLoss: -14347302.000000\n",
            "Train Epoch: 3 [20480/51000 (40%)]\tLoss: -14427914.000000\n",
            "Train Epoch: 3 [21760/51000 (43%)]\tLoss: -14525841.000000\n",
            "Train Epoch: 3 [23040/51000 (45%)]\tLoss: -14614360.000000\n",
            "Train Epoch: 3 [24320/51000 (48%)]\tLoss: -14705022.000000\n",
            "Train Epoch: 3 [25600/51000 (50%)]\tLoss: -14804090.000000\n",
            "Train Epoch: 3 [26880/51000 (53%)]\tLoss: -14885851.000000\n",
            "Train Epoch: 3 [28160/51000 (55%)]\tLoss: -14991016.000000\n",
            "Train Epoch: 3 [29440/51000 (58%)]\tLoss: -15079827.000000\n",
            "Train Epoch: 3 [30720/51000 (60%)]\tLoss: -15173715.000000\n",
            "Train Epoch: 3 [32000/51000 (63%)]\tLoss: -15269776.000000\n",
            "Train Epoch: 3 [33280/51000 (65%)]\tLoss: -15360542.000000\n",
            "Train Epoch: 3 [34560/51000 (68%)]\tLoss: -15450334.000000\n",
            "Train Epoch: 3 [35840/51000 (70%)]\tLoss: -15550342.000000\n",
            "Train Epoch: 3 [37120/51000 (73%)]\tLoss: -15645401.000000\n",
            "Train Epoch: 3 [38400/51000 (75%)]\tLoss: -15731859.000000\n",
            "Train Epoch: 3 [39680/51000 (78%)]\tLoss: -15840610.000000\n",
            "Train Epoch: 3 [40960/51000 (80%)]\tLoss: -15927477.000000\n",
            "Train Epoch: 3 [42240/51000 (83%)]\tLoss: -16022129.000000\n",
            "Train Epoch: 3 [43520/51000 (85%)]\tLoss: -16115333.000000\n",
            "Train Epoch: 3 [44800/51000 (88%)]\tLoss: -16210997.000000\n",
            "Train Epoch: 3 [46080/51000 (90%)]\tLoss: -16304913.000000\n",
            "Train Epoch: 3 [47360/51000 (93%)]\tLoss: -16404758.000000\n",
            "Train Epoch: 3 [48640/51000 (95%)]\tLoss: -16493776.000000\n",
            "Train Epoch: 3 [49920/51000 (98%)]\tLoss: -16597379.000000\n",
            "Test set: Average loss: -17214207.4738, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 4 [0/51000 (0%)]\tLoss: -16619690.000000\n",
            "Train Epoch: 4 [1280/51000 (3%)]\tLoss: -16723177.000000\n",
            "Train Epoch: 4 [2560/51000 (5%)]\tLoss: -16802326.000000\n",
            "Train Epoch: 4 [3840/51000 (8%)]\tLoss: -16885114.000000\n",
            "Train Epoch: 4 [5120/51000 (10%)]\tLoss: -16936468.000000\n",
            "Train Epoch: 4 [6400/51000 (13%)]\tLoss: -16986758.000000\n",
            "Train Epoch: 4 [7680/51000 (15%)]\tLoss: -17073566.000000\n",
            "Train Epoch: 4 [8960/51000 (18%)]\tLoss: -17146010.000000\n",
            "Train Epoch: 4 [10240/51000 (20%)]\tLoss: -17223830.000000\n",
            "Train Epoch: 4 [11520/51000 (23%)]\tLoss: -17269150.000000\n",
            "Train Epoch: 4 [12800/51000 (25%)]\tLoss: -17329658.000000\n",
            "Train Epoch: 4 [14080/51000 (28%)]\tLoss: -17424920.000000\n",
            "Train Epoch: 4 [15360/51000 (30%)]\tLoss: -17482616.000000\n",
            "Train Epoch: 4 [16640/51000 (33%)]\tLoss: -17558614.000000\n",
            "Train Epoch: 4 [17920/51000 (35%)]\tLoss: -17624858.000000\n",
            "Train Epoch: 4 [19200/51000 (38%)]\tLoss: -17705466.000000\n",
            "Train Epoch: 4 [20480/51000 (40%)]\tLoss: -17759464.000000\n",
            "Train Epoch: 4 [21760/51000 (43%)]\tLoss: -17835428.000000\n",
            "Train Epoch: 4 [23040/51000 (45%)]\tLoss: -17895146.000000\n",
            "Train Epoch: 4 [24320/51000 (48%)]\tLoss: -17961974.000000\n",
            "Train Epoch: 4 [25600/51000 (50%)]\tLoss: -18037662.000000\n",
            "Train Epoch: 4 [26880/51000 (53%)]\tLoss: -18090022.000000\n",
            "Train Epoch: 4 [28160/51000 (55%)]\tLoss: -18171700.000000\n",
            "Train Epoch: 4 [29440/51000 (58%)]\tLoss: -18226504.000000\n",
            "Train Epoch: 4 [30720/51000 (60%)]\tLoss: -18294554.000000\n",
            "Train Epoch: 4 [32000/51000 (63%)]\tLoss: -18375182.000000\n",
            "Train Epoch: 4 [33280/51000 (65%)]\tLoss: -18438468.000000\n",
            "Train Epoch: 4 [34560/51000 (68%)]\tLoss: -18498718.000000\n",
            "Train Epoch: 4 [35840/51000 (70%)]\tLoss: -18575160.000000\n",
            "Train Epoch: 4 [37120/51000 (73%)]\tLoss: -18648122.000000\n",
            "Train Epoch: 4 [38400/51000 (75%)]\tLoss: -18707060.000000\n",
            "Train Epoch: 4 [39680/51000 (78%)]\tLoss: -18791374.000000\n",
            "Train Epoch: 4 [40960/51000 (80%)]\tLoss: -18847902.000000\n",
            "Train Epoch: 4 [42240/51000 (83%)]\tLoss: -18918276.000000\n",
            "Train Epoch: 4 [43520/51000 (85%)]\tLoss: -18986442.000000\n",
            "Train Epoch: 4 [44800/51000 (88%)]\tLoss: -19054534.000000\n",
            "Train Epoch: 4 [46080/51000 (90%)]\tLoss: -19122780.000000\n",
            "Train Epoch: 4 [47360/51000 (93%)]\tLoss: -19196446.000000\n",
            "Train Epoch: 4 [48640/51000 (95%)]\tLoss: -19258450.000000\n",
            "Train Epoch: 4 [49920/51000 (98%)]\tLoss: -19340048.000000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-04-11 07:40:04,169]\u001b[0m Trial 4 finished with value: -19327550.577777777 and parameters: {'lr': 0.055366333760817794, 'optimizer_name': 'Adam'}. Best is trial 4 with value: -19327550.577777777.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test set: Average loss: -19327550.5778, Accuracy: 895/9000 (10%)\n",
            "\n",
            "\n",
            "********************************\n",
            "\n",
            "Suggested hyperparameters: \n",
            "{'lr': 0.011901616124824089, 'optimizer_name': 'Adam'}\n",
            "Train Epoch: 0 [0/51000 (0%)]\tLoss: 0.178764\n",
            "Train Epoch: 0 [1280/51000 (3%)]\tLoss: -129.990875\n",
            "Train Epoch: 0 [2560/51000 (5%)]\tLoss: -334.710327\n",
            "Train Epoch: 0 [3840/51000 (8%)]\tLoss: -645.452026\n",
            "Train Epoch: 0 [5120/51000 (10%)]\tLoss: -1051.976440\n",
            "Train Epoch: 0 [6400/51000 (13%)]\tLoss: -1578.030273\n",
            "Train Epoch: 0 [7680/51000 (15%)]\tLoss: -2233.048340\n",
            "Train Epoch: 0 [8960/51000 (18%)]\tLoss: -3087.159180\n",
            "Train Epoch: 0 [10240/51000 (20%)]\tLoss: -4156.560547\n",
            "Train Epoch: 0 [11520/51000 (23%)]\tLoss: -5365.150391\n",
            "Train Epoch: 0 [12800/51000 (25%)]\tLoss: -6743.520020\n",
            "Train Epoch: 0 [14080/51000 (28%)]\tLoss: -8300.652344\n",
            "Train Epoch: 0 [15360/51000 (30%)]\tLoss: -10017.351562\n",
            "Train Epoch: 0 [16640/51000 (33%)]\tLoss: -11909.981445\n",
            "Train Epoch: 0 [17920/51000 (35%)]\tLoss: -14043.368164\n",
            "Train Epoch: 0 [19200/51000 (38%)]\tLoss: -16277.031250\n",
            "Train Epoch: 0 [20480/51000 (40%)]\tLoss: -18772.578125\n",
            "Train Epoch: 0 [21760/51000 (43%)]\tLoss: -21374.312500\n",
            "Train Epoch: 0 [23040/51000 (45%)]\tLoss: -24169.964844\n",
            "Train Epoch: 0 [24320/51000 (48%)]\tLoss: -27141.681641\n",
            "Train Epoch: 0 [25600/51000 (50%)]\tLoss: -30314.576172\n",
            "Train Epoch: 0 [26880/51000 (53%)]\tLoss: -33618.824219\n",
            "Train Epoch: 0 [28160/51000 (55%)]\tLoss: -37127.699219\n",
            "Train Epoch: 0 [29440/51000 (58%)]\tLoss: -40815.753906\n",
            "Train Epoch: 0 [30720/51000 (60%)]\tLoss: -44618.910156\n",
            "Train Epoch: 0 [32000/51000 (63%)]\tLoss: -48673.218750\n",
            "Train Epoch: 0 [33280/51000 (65%)]\tLoss: -52840.050781\n",
            "Train Epoch: 0 [34560/51000 (68%)]\tLoss: -57161.558594\n",
            "Train Epoch: 0 [35840/51000 (70%)]\tLoss: -61799.527344\n",
            "Train Epoch: 0 [37120/51000 (73%)]\tLoss: -66433.171875\n",
            "Train Epoch: 0 [38400/51000 (75%)]\tLoss: -71266.679688\n",
            "Train Epoch: 0 [39680/51000 (78%)]\tLoss: -76401.765625\n",
            "Train Epoch: 0 [40960/51000 (80%)]\tLoss: -81545.101562\n",
            "Train Epoch: 0 [42240/51000 (83%)]\tLoss: -86901.960938\n",
            "Train Epoch: 0 [43520/51000 (85%)]\tLoss: -92494.875000\n",
            "Train Epoch: 0 [44800/51000 (88%)]\tLoss: -98218.820312\n",
            "Train Epoch: 0 [46080/51000 (90%)]\tLoss: -104084.523438\n",
            "Train Epoch: 0 [47360/51000 (93%)]\tLoss: -110085.078125\n",
            "Train Epoch: 0 [48640/51000 (95%)]\tLoss: -116268.593750\n",
            "Train Epoch: 0 [49920/51000 (98%)]\tLoss: -122819.140625\n",
            "Test set: Average loss: -129099.5721, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 1 [0/51000 (0%)]\tLoss: -128565.343750\n",
            "Train Epoch: 1 [1280/51000 (3%)]\tLoss: -133228.312500\n",
            "Train Epoch: 1 [2560/51000 (5%)]\tLoss: -137962.625000\n",
            "Train Epoch: 1 [3840/51000 (8%)]\tLoss: -142796.359375\n",
            "Train Epoch: 1 [5120/51000 (10%)]\tLoss: -147552.656250\n",
            "Train Epoch: 1 [6400/51000 (13%)]\tLoss: -152449.015625\n",
            "Train Epoch: 1 [7680/51000 (15%)]\tLoss: -157330.953125\n",
            "Train Epoch: 1 [8960/51000 (18%)]\tLoss: -162249.359375\n",
            "Train Epoch: 1 [10240/51000 (20%)]\tLoss: -167490.765625\n",
            "Train Epoch: 1 [11520/51000 (23%)]\tLoss: -172326.843750\n",
            "Train Epoch: 1 [12800/51000 (25%)]\tLoss: -177650.656250\n",
            "Train Epoch: 1 [14080/51000 (28%)]\tLoss: -182709.281250\n",
            "Train Epoch: 1 [15360/51000 (30%)]\tLoss: -187922.375000\n",
            "Train Epoch: 1 [16640/51000 (33%)]\tLoss: -193266.265625\n",
            "Train Epoch: 1 [17920/51000 (35%)]\tLoss: -198586.953125\n",
            "Train Epoch: 1 [19200/51000 (38%)]\tLoss: -204121.140625\n",
            "Train Epoch: 1 [20480/51000 (40%)]\tLoss: -209502.125000\n",
            "Train Epoch: 1 [21760/51000 (43%)]\tLoss: -215055.640625\n",
            "Train Epoch: 1 [23040/51000 (45%)]\tLoss: -220573.531250\n",
            "Train Epoch: 1 [24320/51000 (48%)]\tLoss: -226202.062500\n",
            "Train Epoch: 1 [25600/51000 (50%)]\tLoss: -232012.578125\n",
            "Train Epoch: 1 [26880/51000 (53%)]\tLoss: -237586.765625\n",
            "Train Epoch: 1 [28160/51000 (55%)]\tLoss: -243578.031250\n",
            "Train Epoch: 1 [29440/51000 (58%)]\tLoss: -249411.796875\n",
            "Train Epoch: 1 [30720/51000 (60%)]\tLoss: -255285.781250\n",
            "Train Epoch: 1 [32000/51000 (63%)]\tLoss: -261394.968750\n",
            "Train Epoch: 1 [33280/51000 (65%)]\tLoss: -267344.906250\n",
            "Train Epoch: 1 [34560/51000 (68%)]\tLoss: -273378.500000\n",
            "Train Epoch: 1 [35840/51000 (70%)]\tLoss: -279733.187500\n",
            "Train Epoch: 1 [37120/51000 (73%)]\tLoss: -285870.750000\n",
            "Train Epoch: 1 [38400/51000 (75%)]\tLoss: -292038.312500\n",
            "Train Epoch: 1 [39680/51000 (78%)]\tLoss: -298667.718750\n",
            "Train Epoch: 1 [40960/51000 (80%)]\tLoss: -304927.718750\n",
            "Train Epoch: 1 [42240/51000 (83%)]\tLoss: -311357.906250\n",
            "Train Epoch: 1 [43520/51000 (85%)]\tLoss: -317905.437500\n",
            "Train Epoch: 1 [44800/51000 (88%)]\tLoss: -324513.062500\n",
            "Train Epoch: 1 [46080/51000 (90%)]\tLoss: -331111.593750\n",
            "Train Epoch: 1 [47360/51000 (93%)]\tLoss: -337815.625000\n",
            "Train Epoch: 1 [48640/51000 (95%)]\tLoss: -344483.906250\n",
            "Train Epoch: 1 [49920/51000 (98%)]\tLoss: -351637.000000\n",
            "Test set: Average loss: -357789.2492, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 2 [0/51000 (0%)]\tLoss: -357504.937500\n",
            "Train Epoch: 2 [1280/51000 (3%)]\tLoss: -362449.093750\n",
            "Train Epoch: 2 [2560/51000 (5%)]\tLoss: -367335.781250\n",
            "Train Epoch: 2 [3840/51000 (8%)]\tLoss: -372373.906250\n",
            "Train Epoch: 2 [5120/51000 (10%)]\tLoss: -377084.718750\n",
            "Train Epoch: 2 [6400/51000 (13%)]\tLoss: -382053.125000\n",
            "Train Epoch: 2 [7680/51000 (15%)]\tLoss: -386974.187500\n",
            "Train Epoch: 2 [8960/51000 (18%)]\tLoss: -391783.625000\n",
            "Train Epoch: 2 [10240/51000 (20%)]\tLoss: -397178.843750\n",
            "Train Epoch: 2 [11520/51000 (23%)]\tLoss: -401684.656250\n",
            "Train Epoch: 2 [12800/51000 (25%)]\tLoss: -407012.656250\n",
            "Train Epoch: 2 [14080/51000 (28%)]\tLoss: -411975.812500\n",
            "Train Epoch: 2 [15360/51000 (30%)]\tLoss: -416846.906250\n",
            "Train Epoch: 2 [16640/51000 (33%)]\tLoss: -422029.531250\n",
            "Train Epoch: 2 [17920/51000 (35%)]\tLoss: -427058.281250\n",
            "Train Epoch: 2 [19200/51000 (38%)]\tLoss: -432470.125000\n",
            "Train Epoch: 2 [20480/51000 (40%)]\tLoss: -437311.812500\n",
            "Train Epoch: 2 [21760/51000 (43%)]\tLoss: -442545.875000\n",
            "Train Epoch: 2 [23040/51000 (45%)]\tLoss: -447548.312500\n",
            "Train Epoch: 2 [24320/51000 (48%)]\tLoss: -452679.906250\n",
            "Train Epoch: 2 [25600/51000 (50%)]\tLoss: -458094.937500\n",
            "Train Epoch: 2 [26880/51000 (53%)]\tLoss: -462915.281250\n",
            "Train Epoch: 2 [28160/51000 (55%)]\tLoss: -468521.093750\n",
            "Train Epoch: 2 [29440/51000 (58%)]\tLoss: -473686.500000\n",
            "Train Epoch: 2 [30720/51000 (60%)]\tLoss: -478919.906250\n",
            "Train Epoch: 2 [32000/51000 (63%)]\tLoss: -484365.437500\n",
            "Train Epoch: 2 [33280/51000 (65%)]\tLoss: -489519.687500\n",
            "Train Epoch: 2 [34560/51000 (68%)]\tLoss: -494713.156250\n",
            "Train Epoch: 2 [35840/51000 (70%)]\tLoss: -500372.031250\n",
            "Train Epoch: 2 [37120/51000 (73%)]\tLoss: -505636.468750\n",
            "Train Epoch: 2 [38400/51000 (75%)]\tLoss: -510828.468750\n",
            "Train Epoch: 2 [39680/51000 (78%)]\tLoss: -516687.593750\n",
            "Train Epoch: 2 [40960/51000 (80%)]\tLoss: -521911.437500\n",
            "Train Epoch: 2 [42240/51000 (83%)]\tLoss: -527331.937500\n",
            "Train Epoch: 2 [43520/51000 (85%)]\tLoss: -532813.062500\n",
            "Train Epoch: 2 [44800/51000 (88%)]\tLoss: -538339.250000\n",
            "Train Epoch: 2 [46080/51000 (90%)]\tLoss: -543773.625000\n",
            "Train Epoch: 2 [47360/51000 (93%)]\tLoss: -549395.312500\n",
            "Train Epoch: 2 [48640/51000 (95%)]\tLoss: -554777.437500\n",
            "Train Epoch: 2 [49920/51000 (98%)]\tLoss: -560813.375000\n",
            "Test set: Average loss: -565663.4220, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 3 [0/51000 (0%)]\tLoss: -565364.875000\n",
            "Train Epoch: 3 [1280/51000 (3%)]\tLoss: -569489.062500\n",
            "Train Epoch: 3 [2560/51000 (5%)]\tLoss: -573457.875000\n",
            "Train Epoch: 3 [3840/51000 (8%)]\tLoss: -577625.625000\n",
            "Train Epoch: 3 [5120/51000 (10%)]\tLoss: -581267.750000\n",
            "Train Epoch: 3 [6400/51000 (13%)]\tLoss: -585296.687500\n",
            "Train Epoch: 3 [7680/51000 (15%)]\tLoss: -589271.500000\n",
            "Train Epoch: 3 [8960/51000 (18%)]\tLoss: -593026.562500\n",
            "Train Epoch: 3 [10240/51000 (20%)]\tLoss: -597588.937500\n",
            "Train Epoch: 3 [11520/51000 (23%)]\tLoss: -600898.125000\n",
            "Train Epoch: 3 [12800/51000 (25%)]\tLoss: -605299.125000\n",
            "Train Epoch: 3 [14080/51000 (28%)]\tLoss: -609320.187500\n",
            "Train Epoch: 3 [15360/51000 (30%)]\tLoss: -613010.437500\n",
            "Train Epoch: 3 [16640/51000 (33%)]\tLoss: -617202.312500\n",
            "Train Epoch: 3 [17920/51000 (35%)]\tLoss: -621155.625000\n",
            "Train Epoch: 3 [19200/51000 (38%)]\tLoss: -625641.312500\n",
            "Train Epoch: 3 [20480/51000 (40%)]\tLoss: -629230.437500\n",
            "Train Epoch: 3 [21760/51000 (43%)]\tLoss: -633416.375000\n",
            "Train Epoch: 3 [23040/51000 (45%)]\tLoss: -637219.625000\n",
            "Train Epoch: 3 [24320/51000 (48%)]\tLoss: -641189.187500\n",
            "Train Epoch: 3 [25600/51000 (50%)]\tLoss: -645552.562500\n",
            "Train Epoch: 3 [26880/51000 (53%)]\tLoss: -649030.500000\n",
            "Train Epoch: 3 [28160/51000 (55%)]\tLoss: -653620.437500\n",
            "Train Epoch: 3 [29440/51000 (58%)]\tLoss: -657557.937500\n",
            "Train Epoch: 3 [30720/51000 (60%)]\tLoss: -661611.750000\n",
            "Train Epoch: 3 [32000/51000 (63%)]\tLoss: -665856.625000\n",
            "Train Epoch: 3 [33280/51000 (65%)]\tLoss: -669742.437500\n",
            "Train Epoch: 3 [34560/51000 (68%)]\tLoss: -673630.937500\n",
            "Train Epoch: 3 [35840/51000 (70%)]\tLoss: -678132.937500\n",
            "Train Epoch: 3 [37120/51000 (73%)]\tLoss: -682103.750000\n",
            "Train Epoch: 3 [38400/51000 (75%)]\tLoss: -685938.375000\n",
            "Train Epoch: 3 [39680/51000 (78%)]\tLoss: -690621.375000\n",
            "Train Epoch: 3 [40960/51000 (80%)]\tLoss: -694478.875000\n",
            "Train Epoch: 3 [42240/51000 (83%)]\tLoss: -698572.375000\n",
            "Train Epoch: 3 [43520/51000 (85%)]\tLoss: -702693.375000\n",
            "Train Epoch: 3 [44800/51000 (88%)]\tLoss: -706859.500000\n",
            "Train Epoch: 3 [46080/51000 (90%)]\tLoss: -710889.062500\n",
            "Train Epoch: 3 [47360/51000 (93%)]\tLoss: -715197.625000\n",
            "Train Epoch: 3 [48640/51000 (95%)]\tLoss: -719107.312500\n",
            "Train Epoch: 3 [49920/51000 (98%)]\tLoss: -723817.750000\n",
            "Test set: Average loss: -727442.0580, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 4 [0/51000 (0%)]\tLoss: -726953.812500\n",
            "Train Epoch: 4 [1280/51000 (3%)]\tLoss: -730146.375000\n",
            "Train Epoch: 4 [2560/51000 (5%)]\tLoss: -733112.312500\n",
            "Train Epoch: 4 [3840/51000 (8%)]\tLoss: -736314.312500\n",
            "Train Epoch: 4 [5120/51000 (10%)]\tLoss: -738844.750000\n",
            "Train Epoch: 4 [6400/51000 (13%)]\tLoss: -741868.000000\n",
            "Train Epoch: 4 [7680/51000 (15%)]\tLoss: -744838.437500\n",
            "Train Epoch: 4 [8960/51000 (18%)]\tLoss: -747512.500000\n",
            "Train Epoch: 4 [10240/51000 (20%)]\tLoss: -751161.125000\n",
            "Train Epoch: 4 [11520/51000 (23%)]\tLoss: -753294.062500\n",
            "Train Epoch: 4 [12800/51000 (25%)]\tLoss: -756719.500000\n",
            "Train Epoch: 4 [14080/51000 (28%)]\tLoss: -759773.062500\n",
            "Train Epoch: 4 [15360/51000 (30%)]\tLoss: -762306.625000\n",
            "Train Epoch: 4 [16640/51000 (33%)]\tLoss: -765496.250000\n",
            "Train Epoch: 4 [17920/51000 (35%)]\tLoss: -768390.750000\n",
            "Train Epoch: 4 [19200/51000 (38%)]\tLoss: -771932.687500\n",
            "Train Epoch: 4 [20480/51000 (40%)]\tLoss: -774337.812500\n",
            "Train Epoch: 4 [21760/51000 (43%)]\tLoss: -777500.187500\n",
            "Train Epoch: 4 [23040/51000 (45%)]\tLoss: -780170.500000\n",
            "Train Epoch: 4 [24320/51000 (48%)]\tLoss: -783040.250000\n",
            "Train Epoch: 4 [25600/51000 (50%)]\tLoss: -786392.187500\n",
            "Train Epoch: 4 [26880/51000 (53%)]\tLoss: -788646.500000\n",
            "Train Epoch: 4 [28160/51000 (55%)]\tLoss: -792264.062500\n",
            "Train Epoch: 4 [29440/51000 (58%)]\tLoss: -795076.500000\n",
            "Train Epoch: 4 [30720/51000 (60%)]\tLoss: -798047.812500\n",
            "Train Epoch: 4 [32000/51000 (63%)]\tLoss: -801194.250000\n",
            "Train Epoch: 4 [33280/51000 (65%)]\tLoss: -803940.375000\n",
            "Train Epoch: 4 [34560/51000 (68%)]\tLoss: -806665.250000\n",
            "Train Epoch: 4 [35840/51000 (70%)]\tLoss: -810124.187500\n",
            "Train Epoch: 4 [37120/51000 (73%)]\tLoss: -812952.125000\n",
            "Train Epoch: 4 [38400/51000 (75%)]\tLoss: -815602.375000\n",
            "Train Epoch: 4 [39680/51000 (78%)]\tLoss: -819238.500000\n",
            "Train Epoch: 4 [40960/51000 (80%)]\tLoss: -821915.750000\n",
            "Train Epoch: 4 [42240/51000 (83%)]\tLoss: -824863.187500\n",
            "Train Epoch: 4 [43520/51000 (85%)]\tLoss: -827819.250000\n",
            "Train Epoch: 4 [44800/51000 (88%)]\tLoss: -830828.000000\n",
            "Train Epoch: 4 [46080/51000 (90%)]\tLoss: -833667.000000\n",
            "Train Epoch: 4 [47360/51000 (93%)]\tLoss: -836863.000000\n",
            "Train Epoch: 4 [48640/51000 (95%)]\tLoss: -839544.250000\n",
            "Train Epoch: 4 [49920/51000 (98%)]\tLoss: -843139.750000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-04-11 08:08:17,529]\u001b[0m Trial 5 finished with value: -846018.228 and parameters: {'lr': 0.011901616124824089, 'optimizer_name': 'Adam'}. Best is trial 4 with value: -19327550.577777777.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test set: Average loss: -846018.2280, Accuracy: 895/9000 (10%)\n",
            "\n",
            "\n",
            "********************************\n",
            "\n",
            "Suggested hyperparameters: \n",
            "{'lr': 0.0004921776990037929, 'optimizer_name': 'Adam'}\n",
            "Train Epoch: 0 [0/51000 (0%)]\tLoss: 0.087283\n",
            "Train Epoch: 0 [1280/51000 (3%)]\tLoss: -5.044031\n",
            "Train Epoch: 0 [2560/51000 (5%)]\tLoss: -13.145913\n",
            "Train Epoch: 0 [3840/51000 (8%)]\tLoss: -18.461594\n",
            "Train Epoch: 0 [5120/51000 (10%)]\tLoss: -23.739630\n",
            "Train Epoch: 0 [6400/51000 (13%)]\tLoss: -32.287762\n",
            "Train Epoch: 0 [7680/51000 (15%)]\tLoss: -36.921482\n",
            "Train Epoch: 0 [8960/51000 (18%)]\tLoss: -45.389034\n",
            "Train Epoch: 0 [10240/51000 (20%)]\tLoss: -52.883915\n",
            "Train Epoch: 0 [11520/51000 (23%)]\tLoss: -56.891373\n",
            "Train Epoch: 0 [12800/51000 (25%)]\tLoss: -67.331535\n",
            "Train Epoch: 0 [14080/51000 (28%)]\tLoss: -70.227798\n",
            "Train Epoch: 0 [15360/51000 (30%)]\tLoss: -81.868561\n",
            "Train Epoch: 0 [16640/51000 (33%)]\tLoss: -90.948135\n",
            "Train Epoch: 0 [17920/51000 (35%)]\tLoss: -92.871841\n",
            "Train Epoch: 0 [19200/51000 (38%)]\tLoss: -103.923447\n",
            "Train Epoch: 0 [20480/51000 (40%)]\tLoss: -115.328789\n",
            "Train Epoch: 0 [21760/51000 (43%)]\tLoss: -123.438560\n",
            "Train Epoch: 0 [23040/51000 (45%)]\tLoss: -129.792404\n",
            "Train Epoch: 0 [24320/51000 (48%)]\tLoss: -138.607361\n",
            "Train Epoch: 0 [25600/51000 (50%)]\tLoss: -149.057480\n",
            "Train Epoch: 0 [26880/51000 (53%)]\tLoss: -154.546448\n",
            "Train Epoch: 0 [28160/51000 (55%)]\tLoss: -169.704163\n",
            "Train Epoch: 0 [29440/51000 (58%)]\tLoss: -177.925140\n",
            "Train Epoch: 0 [30720/51000 (60%)]\tLoss: -187.615662\n",
            "Train Epoch: 0 [32000/51000 (63%)]\tLoss: -201.220078\n",
            "Train Epoch: 0 [33280/51000 (65%)]\tLoss: -206.131241\n",
            "Train Epoch: 0 [34560/51000 (68%)]\tLoss: -217.501678\n",
            "Train Epoch: 0 [35840/51000 (70%)]\tLoss: -231.107529\n",
            "Train Epoch: 0 [37120/51000 (73%)]\tLoss: -245.414154\n",
            "Train Epoch: 0 [38400/51000 (75%)]\tLoss: -248.975342\n",
            "Train Epoch: 0 [39680/51000 (78%)]\tLoss: -262.972260\n",
            "Train Epoch: 0 [40960/51000 (80%)]\tLoss: -276.223389\n",
            "Train Epoch: 0 [42240/51000 (83%)]\tLoss: -283.834595\n",
            "Train Epoch: 0 [43520/51000 (85%)]\tLoss: -303.860504\n",
            "Train Epoch: 0 [44800/51000 (88%)]\tLoss: -314.748352\n",
            "Train Epoch: 0 [46080/51000 (90%)]\tLoss: -326.707703\n",
            "Train Epoch: 0 [47360/51000 (93%)]\tLoss: -337.275116\n",
            "Train Epoch: 0 [48640/51000 (95%)]\tLoss: -349.241943\n",
            "Train Epoch: 0 [49920/51000 (98%)]\tLoss: -359.215759\n",
            "Test set: Average loss: -286.4620, Accuracy: 1240/9000 (14%)\n",
            "\n",
            "Train Epoch: 1 [0/51000 (0%)]\tLoss: -370.218567\n",
            "Train Epoch: 1 [1280/51000 (3%)]\tLoss: -391.037445\n",
            "Train Epoch: 1 [2560/51000 (5%)]\tLoss: -393.008331\n",
            "Train Epoch: 1 [3840/51000 (8%)]\tLoss: -397.031128\n",
            "Train Epoch: 1 [5120/51000 (10%)]\tLoss: -416.315247\n",
            "Train Epoch: 1 [6400/51000 (13%)]\tLoss: -422.572601\n",
            "Train Epoch: 1 [7680/51000 (15%)]\tLoss: -444.427399\n",
            "Train Epoch: 1 [8960/51000 (18%)]\tLoss: -449.623352\n",
            "Train Epoch: 1 [10240/51000 (20%)]\tLoss: -458.707458\n",
            "Train Epoch: 1 [11520/51000 (23%)]\tLoss: -457.179352\n",
            "Train Epoch: 1 [12800/51000 (25%)]\tLoss: -476.600220\n",
            "Train Epoch: 1 [14080/51000 (28%)]\tLoss: -487.283264\n",
            "Train Epoch: 1 [15360/51000 (30%)]\tLoss: -492.888336\n",
            "Train Epoch: 1 [16640/51000 (33%)]\tLoss: -508.074890\n",
            "Train Epoch: 1 [17920/51000 (35%)]\tLoss: -525.173035\n",
            "Train Epoch: 1 [19200/51000 (38%)]\tLoss: -537.446411\n",
            "Train Epoch: 1 [20480/51000 (40%)]\tLoss: -548.445923\n",
            "Train Epoch: 1 [21760/51000 (43%)]\tLoss: -553.797363\n",
            "Train Epoch: 1 [23040/51000 (45%)]\tLoss: -549.488953\n",
            "Train Epoch: 1 [24320/51000 (48%)]\tLoss: -566.159790\n",
            "Train Epoch: 1 [25600/51000 (50%)]\tLoss: -589.094788\n",
            "Train Epoch: 1 [26880/51000 (53%)]\tLoss: -591.615112\n",
            "Train Epoch: 1 [28160/51000 (55%)]\tLoss: -613.220825\n",
            "Train Epoch: 1 [29440/51000 (58%)]\tLoss: -631.535095\n",
            "Train Epoch: 1 [30720/51000 (60%)]\tLoss: -632.016113\n",
            "Train Epoch: 1 [32000/51000 (63%)]\tLoss: -643.297607\n",
            "Train Epoch: 1 [33280/51000 (65%)]\tLoss: -658.880127\n",
            "Train Epoch: 1 [34560/51000 (68%)]\tLoss: -663.454529\n",
            "Train Epoch: 1 [35840/51000 (70%)]\tLoss: -675.807190\n",
            "Train Epoch: 1 [37120/51000 (73%)]\tLoss: -695.924255\n",
            "Train Epoch: 1 [38400/51000 (75%)]\tLoss: -687.336487\n",
            "Train Epoch: 1 [39680/51000 (78%)]\tLoss: -712.124023\n",
            "Train Epoch: 1 [40960/51000 (80%)]\tLoss: -723.632629\n",
            "Train Epoch: 1 [42240/51000 (83%)]\tLoss: -721.782043\n",
            "Train Epoch: 1 [43520/51000 (85%)]\tLoss: -756.955444\n",
            "Train Epoch: 1 [44800/51000 (88%)]\tLoss: -764.849670\n",
            "Train Epoch: 1 [46080/51000 (90%)]\tLoss: -777.659424\n",
            "Train Epoch: 1 [47360/51000 (93%)]\tLoss: -792.115540\n",
            "Train Epoch: 1 [48640/51000 (95%)]\tLoss: -799.703003\n",
            "Train Epoch: 1 [49920/51000 (98%)]\tLoss: -805.961182\n",
            "Test set: Average loss: -843.0233, Accuracy: 1527/9000 (17%)\n",
            "\n",
            "Train Epoch: 2 [0/51000 (0%)]\tLoss: -798.127686\n",
            "Train Epoch: 2 [1280/51000 (3%)]\tLoss: -835.125488\n",
            "Train Epoch: 2 [2560/51000 (5%)]\tLoss: -838.886536\n",
            "Train Epoch: 2 [3840/51000 (8%)]\tLoss: -844.921265\n",
            "Train Epoch: 2 [5120/51000 (10%)]\tLoss: -853.042114\n",
            "Train Epoch: 2 [6400/51000 (13%)]\tLoss: -871.446777\n",
            "Train Epoch: 2 [7680/51000 (15%)]\tLoss: -896.941406\n",
            "Train Epoch: 2 [8960/51000 (18%)]\tLoss: -898.199951\n",
            "Train Epoch: 2 [10240/51000 (20%)]\tLoss: -894.262390\n",
            "Train Epoch: 2 [11520/51000 (23%)]\tLoss: -901.353943\n",
            "Train Epoch: 2 [12800/51000 (25%)]\tLoss: -918.661255\n",
            "Train Epoch: 2 [14080/51000 (28%)]\tLoss: -925.429382\n",
            "Train Epoch: 2 [15360/51000 (30%)]\tLoss: -923.167236\n",
            "Train Epoch: 2 [16640/51000 (33%)]\tLoss: -940.725769\n",
            "Train Epoch: 2 [17920/51000 (35%)]\tLoss: -929.811707\n",
            "Train Epoch: 2 [19200/51000 (38%)]\tLoss: -963.066956\n",
            "Train Epoch: 2 [20480/51000 (40%)]\tLoss: -978.268311\n",
            "Train Epoch: 2 [21760/51000 (43%)]\tLoss: -974.349243\n",
            "Train Epoch: 2 [23040/51000 (45%)]\tLoss: -971.154358\n",
            "Train Epoch: 2 [24320/51000 (48%)]\tLoss: -983.324829\n",
            "Train Epoch: 2 [25600/51000 (50%)]\tLoss: -1008.212036\n",
            "Train Epoch: 2 [26880/51000 (53%)]\tLoss: -1016.911560\n",
            "Train Epoch: 2 [28160/51000 (55%)]\tLoss: -1034.362671\n",
            "Train Epoch: 2 [29440/51000 (58%)]\tLoss: -1039.043335\n",
            "Train Epoch: 2 [30720/51000 (60%)]\tLoss: -1047.295532\n",
            "Train Epoch: 2 [32000/51000 (63%)]\tLoss: -1062.298828\n",
            "Train Epoch: 2 [33280/51000 (65%)]\tLoss: -1075.098633\n",
            "Train Epoch: 2 [34560/51000 (68%)]\tLoss: -1075.586914\n",
            "Train Epoch: 2 [35840/51000 (70%)]\tLoss: -1092.163696\n",
            "Train Epoch: 2 [37120/51000 (73%)]\tLoss: -1100.811890\n",
            "Train Epoch: 2 [38400/51000 (75%)]\tLoss: -1080.911255\n",
            "Train Epoch: 2 [39680/51000 (78%)]\tLoss: -1122.843140\n",
            "Train Epoch: 2 [40960/51000 (80%)]\tLoss: -1128.694702\n",
            "Train Epoch: 2 [42240/51000 (83%)]\tLoss: -1125.906250\n",
            "Train Epoch: 2 [43520/51000 (85%)]\tLoss: -1149.743652\n",
            "Train Epoch: 2 [44800/51000 (88%)]\tLoss: -1147.834106\n",
            "Train Epoch: 2 [46080/51000 (90%)]\tLoss: -1163.122559\n",
            "Train Epoch: 2 [47360/51000 (93%)]\tLoss: -1180.004639\n",
            "Train Epoch: 2 [48640/51000 (95%)]\tLoss: -1190.049805\n",
            "Train Epoch: 2 [49920/51000 (98%)]\tLoss: -1188.800415\n",
            "Test set: Average loss: -1187.6166, Accuracy: 1670/9000 (19%)\n",
            "\n",
            "Train Epoch: 3 [0/51000 (0%)]\tLoss: -1181.809326\n",
            "Train Epoch: 3 [1280/51000 (3%)]\tLoss: -1214.570679\n",
            "Train Epoch: 3 [2560/51000 (5%)]\tLoss: -1207.669189\n",
            "Train Epoch: 3 [3840/51000 (8%)]\tLoss: -1204.490479\n",
            "Train Epoch: 3 [5120/51000 (10%)]\tLoss: -1210.943726\n",
            "Train Epoch: 3 [6400/51000 (13%)]\tLoss: -1236.859131\n",
            "Train Epoch: 3 [7680/51000 (15%)]\tLoss: -1260.156372\n",
            "Train Epoch: 3 [8960/51000 (18%)]\tLoss: -1268.104492\n",
            "Train Epoch: 3 [10240/51000 (20%)]\tLoss: -1262.263184\n",
            "Train Epoch: 3 [11520/51000 (23%)]\tLoss: -1263.445068\n",
            "Train Epoch: 3 [12800/51000 (25%)]\tLoss: -1275.592041\n",
            "Train Epoch: 3 [14080/51000 (28%)]\tLoss: -1288.796875\n",
            "Train Epoch: 3 [15360/51000 (30%)]\tLoss: -1281.338989\n",
            "Train Epoch: 3 [16640/51000 (33%)]\tLoss: -1296.020142\n",
            "Train Epoch: 3 [17920/51000 (35%)]\tLoss: -1305.130005\n",
            "Train Epoch: 3 [19200/51000 (38%)]\tLoss: -1297.607788\n",
            "Train Epoch: 3 [20480/51000 (40%)]\tLoss: -1332.972168\n",
            "Train Epoch: 3 [21760/51000 (43%)]\tLoss: -1325.495239\n",
            "Train Epoch: 3 [23040/51000 (45%)]\tLoss: -1334.733521\n",
            "Train Epoch: 3 [24320/51000 (48%)]\tLoss: -1317.803101\n",
            "Train Epoch: 3 [25600/51000 (50%)]\tLoss: -1365.886230\n",
            "Train Epoch: 3 [26880/51000 (53%)]\tLoss: -1369.626831\n",
            "Train Epoch: 3 [28160/51000 (55%)]\tLoss: -1372.015991\n",
            "Train Epoch: 3 [29440/51000 (58%)]\tLoss: -1397.386475\n",
            "Train Epoch: 3 [30720/51000 (60%)]\tLoss: -1377.263428\n",
            "Train Epoch: 3 [32000/51000 (63%)]\tLoss: -1388.323120\n",
            "Train Epoch: 3 [33280/51000 (65%)]\tLoss: -1391.968018\n",
            "Train Epoch: 3 [34560/51000 (68%)]\tLoss: -1398.587402\n",
            "Train Epoch: 3 [35840/51000 (70%)]\tLoss: -1408.407104\n",
            "Train Epoch: 3 [37120/51000 (73%)]\tLoss: -1421.524048\n",
            "Train Epoch: 3 [38400/51000 (75%)]\tLoss: -1381.462158\n",
            "Train Epoch: 3 [39680/51000 (78%)]\tLoss: -1434.673340\n",
            "Train Epoch: 3 [40960/51000 (80%)]\tLoss: -1439.490845\n",
            "Train Epoch: 3 [42240/51000 (83%)]\tLoss: -1414.620483\n",
            "Train Epoch: 3 [43520/51000 (85%)]\tLoss: -1459.874268\n",
            "Train Epoch: 3 [44800/51000 (88%)]\tLoss: -1457.766113\n",
            "Train Epoch: 3 [46080/51000 (90%)]\tLoss: -1458.706787\n",
            "Train Epoch: 3 [47360/51000 (93%)]\tLoss: -1487.469727\n",
            "Train Epoch: 3 [48640/51000 (95%)]\tLoss: -1487.965210\n",
            "Train Epoch: 3 [49920/51000 (98%)]\tLoss: -1476.283936\n",
            "Test set: Average loss: -1518.6218, Accuracy: 1677/9000 (19%)\n",
            "\n",
            "Train Epoch: 4 [0/51000 (0%)]\tLoss: -1466.983521\n",
            "Train Epoch: 4 [1280/51000 (3%)]\tLoss: -1508.821655\n",
            "Train Epoch: 4 [2560/51000 (5%)]\tLoss: -1513.330566\n",
            "Train Epoch: 4 [3840/51000 (8%)]\tLoss: -1479.112183\n",
            "Train Epoch: 4 [5120/51000 (10%)]\tLoss: -1506.313477\n",
            "Train Epoch: 4 [6400/51000 (13%)]\tLoss: -1520.290161\n",
            "Train Epoch: 4 [7680/51000 (15%)]\tLoss: -1543.875488\n",
            "Train Epoch: 4 [8960/51000 (18%)]\tLoss: -1546.102173\n",
            "Train Epoch: 4 [10240/51000 (20%)]\tLoss: -1530.545166\n",
            "Train Epoch: 4 [11520/51000 (23%)]\tLoss: -1547.211914\n",
            "Train Epoch: 4 [12800/51000 (25%)]\tLoss: -1552.455688\n",
            "Train Epoch: 4 [14080/51000 (28%)]\tLoss: -1565.329712\n",
            "Train Epoch: 4 [15360/51000 (30%)]\tLoss: -1543.333008\n",
            "Train Epoch: 4 [16640/51000 (33%)]\tLoss: -1549.476929\n",
            "Train Epoch: 4 [17920/51000 (35%)]\tLoss: -1560.373779\n",
            "Train Epoch: 4 [19200/51000 (38%)]\tLoss: -1575.007324\n",
            "Train Epoch: 4 [20480/51000 (40%)]\tLoss: -1593.566650\n",
            "Train Epoch: 4 [21760/51000 (43%)]\tLoss: -1583.913696\n",
            "Train Epoch: 4 [23040/51000 (45%)]\tLoss: -1579.347046\n",
            "Train Epoch: 4 [24320/51000 (48%)]\tLoss: -1565.234741\n",
            "Train Epoch: 4 [25600/51000 (50%)]\tLoss: -1624.792114\n",
            "Train Epoch: 4 [26880/51000 (53%)]\tLoss: -1629.648682\n",
            "Train Epoch: 4 [28160/51000 (55%)]\tLoss: -1618.769775\n",
            "Train Epoch: 4 [29440/51000 (58%)]\tLoss: -1637.135986\n",
            "Train Epoch: 4 [30720/51000 (60%)]\tLoss: -1617.621338\n",
            "Train Epoch: 4 [32000/51000 (63%)]\tLoss: -1634.550049\n",
            "Train Epoch: 4 [33280/51000 (65%)]\tLoss: -1649.608643\n",
            "Train Epoch: 4 [34560/51000 (68%)]\tLoss: -1645.255737\n",
            "Train Epoch: 4 [35840/51000 (70%)]\tLoss: -1640.373657\n",
            "Train Epoch: 4 [37120/51000 (73%)]\tLoss: -1658.675537\n",
            "Train Epoch: 4 [38400/51000 (75%)]\tLoss: -1610.758545\n",
            "Train Epoch: 4 [39680/51000 (78%)]\tLoss: -1661.907227\n",
            "Train Epoch: 4 [40960/51000 (80%)]\tLoss: -1665.336792\n",
            "Train Epoch: 4 [42240/51000 (83%)]\tLoss: -1645.835449\n",
            "Train Epoch: 4 [43520/51000 (85%)]\tLoss: -1679.030396\n",
            "Train Epoch: 4 [44800/51000 (88%)]\tLoss: -1677.093140\n",
            "Train Epoch: 4 [46080/51000 (90%)]\tLoss: -1682.281982\n",
            "Train Epoch: 4 [47360/51000 (93%)]\tLoss: -1707.363281\n",
            "Train Epoch: 4 [48640/51000 (95%)]\tLoss: -1696.998291\n",
            "Train Epoch: 4 [49920/51000 (98%)]\tLoss: -1699.416382\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-04-11 08:36:34,506]\u001b[0m Trial 6 finished with value: -1696.21609375 and parameters: {'lr': 0.0004921776990037929, 'optimizer_name': 'Adam'}. Best is trial 4 with value: -19327550.577777777.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test set: Average loss: -1696.2161, Accuracy: 1700/9000 (19%)\n",
            "\n",
            "\n",
            "********************************\n",
            "\n",
            "Suggested hyperparameters: \n",
            "{'lr': 0.0028501459455535258, 'optimizer_name': 'RMSprop'}\n",
            "Train Epoch: 0 [0/51000 (0%)]\tLoss: 0.159608\n",
            "Train Epoch: 0 [1280/51000 (3%)]\tLoss: -211.880219\n",
            "Train Epoch: 0 [2560/51000 (5%)]\tLoss: -411.555328\n",
            "Train Epoch: 0 [3840/51000 (8%)]\tLoss: -592.794739\n",
            "Train Epoch: 0 [5120/51000 (10%)]\tLoss: -765.658630\n",
            "Train Epoch: 0 [6400/51000 (13%)]\tLoss: -947.593994\n",
            "Train Epoch: 0 [7680/51000 (15%)]\tLoss: -1129.245972\n",
            "Train Epoch: 0 [8960/51000 (18%)]\tLoss: -1310.652710\n",
            "Train Epoch: 0 [10240/51000 (20%)]\tLoss: -1512.281372\n",
            "Train Epoch: 0 [11520/51000 (23%)]\tLoss: -1724.276978\n",
            "Train Epoch: 0 [12800/51000 (25%)]\tLoss: -1928.082031\n",
            "Train Epoch: 0 [14080/51000 (28%)]\tLoss: -2136.920166\n",
            "Train Epoch: 0 [15360/51000 (30%)]\tLoss: -2348.055176\n",
            "Train Epoch: 0 [16640/51000 (33%)]\tLoss: -2564.225098\n",
            "Train Epoch: 0 [17920/51000 (35%)]\tLoss: -2784.444824\n",
            "Train Epoch: 0 [19200/51000 (38%)]\tLoss: -2994.185059\n",
            "Train Epoch: 0 [20480/51000 (40%)]\tLoss: -3231.191650\n",
            "Train Epoch: 0 [21760/51000 (43%)]\tLoss: -3475.340576\n",
            "Train Epoch: 0 [23040/51000 (45%)]\tLoss: -3691.771484\n",
            "Train Epoch: 0 [24320/51000 (48%)]\tLoss: -3935.178955\n",
            "Train Epoch: 0 [25600/51000 (50%)]\tLoss: -4189.080566\n",
            "Train Epoch: 0 [26880/51000 (53%)]\tLoss: -4430.466309\n",
            "Train Epoch: 0 [28160/51000 (55%)]\tLoss: -4684.690918\n",
            "Train Epoch: 0 [29440/51000 (58%)]\tLoss: -4940.020996\n",
            "Train Epoch: 0 [30720/51000 (60%)]\tLoss: -5190.993652\n",
            "Train Epoch: 0 [32000/51000 (63%)]\tLoss: -5461.911133\n",
            "Train Epoch: 0 [33280/51000 (65%)]\tLoss: -5731.805664\n",
            "Train Epoch: 0 [34560/51000 (68%)]\tLoss: -5982.537109\n",
            "Train Epoch: 0 [35840/51000 (70%)]\tLoss: -6267.712891\n",
            "Train Epoch: 0 [37120/51000 (73%)]\tLoss: -6543.455566\n",
            "Train Epoch: 0 [38400/51000 (75%)]\tLoss: -6822.764648\n",
            "Train Epoch: 0 [39680/51000 (78%)]\tLoss: -7127.783203\n",
            "Train Epoch: 0 [40960/51000 (80%)]\tLoss: -7407.229980\n",
            "Train Epoch: 0 [42240/51000 (83%)]\tLoss: -7707.734375\n",
            "Train Epoch: 0 [43520/51000 (85%)]\tLoss: -8005.520996\n",
            "Train Epoch: 0 [44800/51000 (88%)]\tLoss: -8311.474609\n",
            "Train Epoch: 0 [46080/51000 (90%)]\tLoss: -8609.367188\n",
            "Train Epoch: 0 [47360/51000 (93%)]\tLoss: -8940.639648\n",
            "Train Epoch: 0 [48640/51000 (95%)]\tLoss: -9193.752930\n",
            "Train Epoch: 0 [49920/51000 (98%)]\tLoss: -9553.521484\n",
            "Test set: Average loss: -9520.3709, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 1 [0/51000 (0%)]\tLoss: -9815.270508\n",
            "Train Epoch: 1 [1280/51000 (3%)]\tLoss: -10092.072266\n",
            "Train Epoch: 1 [2560/51000 (5%)]\tLoss: -10330.606445\n",
            "Train Epoch: 1 [3840/51000 (8%)]\tLoss: -10565.523438\n",
            "Train Epoch: 1 [5120/51000 (10%)]\tLoss: -10778.348633\n",
            "Train Epoch: 1 [6400/51000 (13%)]\tLoss: -11027.775391\n",
            "Train Epoch: 1 [7680/51000 (15%)]\tLoss: -11265.569336\n",
            "Train Epoch: 1 [8960/51000 (18%)]\tLoss: -11497.983398\n",
            "Train Epoch: 1 [10240/51000 (20%)]\tLoss: -11752.567383\n",
            "Train Epoch: 1 [11520/51000 (23%)]\tLoss: -11973.315430\n",
            "Train Epoch: 1 [12800/51000 (25%)]\tLoss: -12223.065430\n",
            "Train Epoch: 1 [14080/51000 (28%)]\tLoss: -12476.978516\n",
            "Train Epoch: 1 [15360/51000 (30%)]\tLoss: -12712.119141\n",
            "Train Epoch: 1 [16640/51000 (33%)]\tLoss: -12950.192383\n",
            "Train Epoch: 1 [17920/51000 (35%)]\tLoss: -13186.835938\n",
            "Train Epoch: 1 [19200/51000 (38%)]\tLoss: -13453.425781\n",
            "Train Epoch: 1 [20480/51000 (40%)]\tLoss: -13712.856445\n",
            "Train Epoch: 1 [21760/51000 (43%)]\tLoss: -13975.745117\n",
            "Train Epoch: 1 [23040/51000 (45%)]\tLoss: -14216.228516\n",
            "Train Epoch: 1 [24320/51000 (48%)]\tLoss: -14460.348633\n",
            "Train Epoch: 1 [25600/51000 (50%)]\tLoss: -14749.738281\n",
            "Train Epoch: 1 [26880/51000 (53%)]\tLoss: -14972.156250\n",
            "Train Epoch: 1 [28160/51000 (55%)]\tLoss: -15257.887695\n",
            "Train Epoch: 1 [29440/51000 (58%)]\tLoss: -15530.079102\n",
            "Train Epoch: 1 [30720/51000 (60%)]\tLoss: -15787.347656\n",
            "Train Epoch: 1 [32000/51000 (63%)]\tLoss: -16059.586914\n",
            "Train Epoch: 1 [33280/51000 (65%)]\tLoss: -16335.684570\n",
            "Train Epoch: 1 [34560/51000 (68%)]\tLoss: -16598.062500\n",
            "Train Epoch: 1 [35840/51000 (70%)]\tLoss: -16854.835938\n",
            "Train Epoch: 1 [37120/51000 (73%)]\tLoss: -17138.650391\n",
            "Train Epoch: 1 [38400/51000 (75%)]\tLoss: -17412.761719\n",
            "Train Epoch: 1 [39680/51000 (78%)]\tLoss: -17728.390625\n",
            "Train Epoch: 1 [40960/51000 (80%)]\tLoss: -17955.111328\n",
            "Train Epoch: 1 [42240/51000 (83%)]\tLoss: -18278.455078\n",
            "Train Epoch: 1 [43520/51000 (85%)]\tLoss: -18554.400391\n",
            "Train Epoch: 1 [44800/51000 (88%)]\tLoss: -18852.160156\n",
            "Train Epoch: 1 [46080/51000 (90%)]\tLoss: -19139.037109\n",
            "Train Epoch: 1 [47360/51000 (93%)]\tLoss: -19403.093750\n",
            "Train Epoch: 1 [48640/51000 (95%)]\tLoss: -19717.484375\n",
            "Train Epoch: 1 [49920/51000 (98%)]\tLoss: -20030.630859\n",
            "Test set: Average loss: -20238.7237, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 2 [0/51000 (0%)]\tLoss: -20247.935547\n",
            "Train Epoch: 2 [1280/51000 (3%)]\tLoss: -20463.019531\n",
            "Train Epoch: 2 [2560/51000 (5%)]\tLoss: -20719.478516\n",
            "Train Epoch: 2 [3840/51000 (8%)]\tLoss: -20950.501953\n",
            "Train Epoch: 2 [5120/51000 (10%)]\tLoss: -21138.060547\n",
            "Train Epoch: 2 [6400/51000 (13%)]\tLoss: -21354.173828\n",
            "Train Epoch: 2 [7680/51000 (15%)]\tLoss: -21562.314453\n",
            "Train Epoch: 2 [8960/51000 (18%)]\tLoss: -21782.666016\n",
            "Train Epoch: 2 [10240/51000 (20%)]\tLoss: -22023.835938\n",
            "Train Epoch: 2 [11520/51000 (23%)]\tLoss: -22211.085938\n",
            "Train Epoch: 2 [12800/51000 (25%)]\tLoss: -22439.064453\n",
            "Train Epoch: 2 [14080/51000 (28%)]\tLoss: -22654.888672\n",
            "Train Epoch: 2 [15360/51000 (30%)]\tLoss: -22849.210938\n",
            "Train Epoch: 2 [16640/51000 (33%)]\tLoss: -23074.212891\n",
            "Train Epoch: 2 [17920/51000 (35%)]\tLoss: -23299.082031\n",
            "Train Epoch: 2 [19200/51000 (38%)]\tLoss: -23528.503906\n",
            "Train Epoch: 2 [20480/51000 (40%)]\tLoss: -23751.894531\n",
            "Train Epoch: 2 [21760/51000 (43%)]\tLoss: -23981.558594\n",
            "Train Epoch: 2 [23040/51000 (45%)]\tLoss: -24175.296875\n",
            "Train Epoch: 2 [24320/51000 (48%)]\tLoss: -24386.541016\n",
            "Train Epoch: 2 [25600/51000 (50%)]\tLoss: -24642.041016\n",
            "Train Epoch: 2 [26880/51000 (53%)]\tLoss: -24846.000000\n",
            "Train Epoch: 2 [28160/51000 (55%)]\tLoss: -25091.748047\n",
            "Train Epoch: 2 [29440/51000 (58%)]\tLoss: -25325.193359\n",
            "Train Epoch: 2 [30720/51000 (60%)]\tLoss: -25541.812500\n",
            "Train Epoch: 2 [32000/51000 (63%)]\tLoss: -25745.712891\n",
            "Train Epoch: 2 [33280/51000 (65%)]\tLoss: -26004.640625\n",
            "Train Epoch: 2 [34560/51000 (68%)]\tLoss: -26217.125000\n",
            "Train Epoch: 2 [35840/51000 (70%)]\tLoss: -26474.519531\n",
            "Train Epoch: 2 [37120/51000 (73%)]\tLoss: -26701.994141\n",
            "Train Epoch: 2 [38400/51000 (75%)]\tLoss: -26916.498047\n",
            "Train Epoch: 2 [39680/51000 (78%)]\tLoss: -27169.818359\n",
            "Train Epoch: 2 [40960/51000 (80%)]\tLoss: -27402.263672\n",
            "Train Epoch: 2 [42240/51000 (83%)]\tLoss: -27567.458984\n",
            "Train Epoch: 2 [43520/51000 (85%)]\tLoss: -27867.882812\n",
            "Train Epoch: 2 [44800/51000 (88%)]\tLoss: -28097.511719\n",
            "Train Epoch: 2 [46080/51000 (90%)]\tLoss: -28349.910156\n",
            "Train Epoch: 2 [47360/51000 (93%)]\tLoss: -28585.699219\n",
            "Train Epoch: 2 [48640/51000 (95%)]\tLoss: -28844.277344\n",
            "Train Epoch: 2 [49920/51000 (98%)]\tLoss: -29103.261719\n",
            "Test set: Average loss: -27978.5737, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 3 [0/51000 (0%)]\tLoss: -29257.667969\n",
            "Train Epoch: 3 [1280/51000 (3%)]\tLoss: -29478.630859\n",
            "Train Epoch: 3 [2560/51000 (5%)]\tLoss: -29653.267578\n",
            "Train Epoch: 3 [3840/51000 (8%)]\tLoss: -29832.412109\n",
            "Train Epoch: 3 [5120/51000 (10%)]\tLoss: -29960.974609\n",
            "Train Epoch: 3 [6400/51000 (13%)]\tLoss: -30138.859375\n",
            "Train Epoch: 3 [7680/51000 (15%)]\tLoss: -30329.103516\n",
            "Train Epoch: 3 [8960/51000 (18%)]\tLoss: -30503.523438\n",
            "Train Epoch: 3 [10240/51000 (20%)]\tLoss: -30709.158203\n",
            "Train Epoch: 3 [11520/51000 (23%)]\tLoss: -30833.302734\n",
            "Train Epoch: 3 [12800/51000 (25%)]\tLoss: -31030.937500\n",
            "Train Epoch: 3 [14080/51000 (28%)]\tLoss: -31208.267578\n",
            "Train Epoch: 3 [15360/51000 (30%)]\tLoss: -31321.769531\n",
            "Train Epoch: 3 [16640/51000 (33%)]\tLoss: -31523.626953\n",
            "Train Epoch: 3 [17920/51000 (35%)]\tLoss: -31721.369141\n",
            "Train Epoch: 3 [19200/51000 (38%)]\tLoss: -31921.007812\n",
            "Train Epoch: 3 [20480/51000 (40%)]\tLoss: -32088.169922\n",
            "Train Epoch: 3 [21760/51000 (43%)]\tLoss: -32246.720703\n",
            "Train Epoch: 3 [23040/51000 (45%)]\tLoss: -32406.173828\n",
            "Train Epoch: 3 [24320/51000 (48%)]\tLoss: -32575.855469\n",
            "Train Epoch: 3 [25600/51000 (50%)]\tLoss: -32800.937500\n",
            "Train Epoch: 3 [26880/51000 (53%)]\tLoss: -32949.281250\n",
            "Train Epoch: 3 [28160/51000 (55%)]\tLoss: -33149.984375\n",
            "Train Epoch: 3 [29440/51000 (58%)]\tLoss: -33327.246094\n",
            "Train Epoch: 3 [30720/51000 (60%)]\tLoss: -33459.667969\n",
            "Train Epoch: 3 [32000/51000 (63%)]\tLoss: -33676.203125\n",
            "Train Epoch: 3 [33280/51000 (65%)]\tLoss: -33864.460938\n",
            "Train Epoch: 3 [34560/51000 (68%)]\tLoss: -34011.507812\n",
            "Train Epoch: 3 [35840/51000 (70%)]\tLoss: -34229.792969\n",
            "Train Epoch: 3 [37120/51000 (73%)]\tLoss: -34396.980469\n",
            "Train Epoch: 3 [38400/51000 (75%)]\tLoss: -34551.640625\n",
            "Train Epoch: 3 [39680/51000 (78%)]\tLoss: -34791.324219\n",
            "Train Epoch: 3 [40960/51000 (80%)]\tLoss: -34954.660156\n",
            "Train Epoch: 3 [42240/51000 (83%)]\tLoss: -35150.402344\n",
            "Train Epoch: 3 [43520/51000 (85%)]\tLoss: -35304.390625\n",
            "Train Epoch: 3 [44800/51000 (88%)]\tLoss: -35511.878906\n",
            "Train Epoch: 3 [46080/51000 (90%)]\tLoss: -35642.410156\n",
            "Train Epoch: 3 [47360/51000 (93%)]\tLoss: -35887.144531\n",
            "Train Epoch: 3 [48640/51000 (95%)]\tLoss: -36065.367188\n",
            "Train Epoch: 3 [49920/51000 (98%)]\tLoss: -36276.472656\n",
            "Test set: Average loss: -36790.8250, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 4 [0/51000 (0%)]\tLoss: -36378.937500\n",
            "Train Epoch: 4 [1280/51000 (3%)]\tLoss: -36549.023438\n",
            "Train Epoch: 4 [2560/51000 (5%)]\tLoss: -36687.421875\n",
            "Train Epoch: 4 [3840/51000 (8%)]\tLoss: -36835.746094\n",
            "Train Epoch: 4 [5120/51000 (10%)]\tLoss: -36922.296875\n",
            "Train Epoch: 4 [6400/51000 (13%)]\tLoss: -37065.394531\n",
            "Train Epoch: 4 [7680/51000 (15%)]\tLoss: -37188.414062\n",
            "Train Epoch: 4 [8960/51000 (18%)]\tLoss: -37334.535156\n",
            "Train Epoch: 4 [10240/51000 (20%)]\tLoss: -37500.414062\n",
            "Train Epoch: 4 [11520/51000 (23%)]\tLoss: -37571.562500\n",
            "Train Epoch: 4 [12800/51000 (25%)]\tLoss: -37723.179688\n",
            "Train Epoch: 4 [14080/51000 (28%)]\tLoss: -37870.218750\n",
            "Train Epoch: 4 [15360/51000 (30%)]\tLoss: -37985.984375\n",
            "Train Epoch: 4 [16640/51000 (33%)]\tLoss: -38105.937500\n",
            "Train Epoch: 4 [17920/51000 (35%)]\tLoss: -38252.601562\n",
            "Train Epoch: 4 [19200/51000 (38%)]\tLoss: -38418.191406\n",
            "Train Epoch: 4 [20480/51000 (40%)]\tLoss: -38540.109375\n",
            "Train Epoch: 4 [21760/51000 (43%)]\tLoss: -38680.855469\n",
            "Train Epoch: 4 [23040/51000 (45%)]\tLoss: -38798.941406\n",
            "Train Epoch: 4 [24320/51000 (48%)]\tLoss: -38907.097656\n",
            "Train Epoch: 4 [25600/51000 (50%)]\tLoss: -39068.042969\n",
            "Train Epoch: 4 [26880/51000 (53%)]\tLoss: -39176.652344\n",
            "Train Epoch: 4 [28160/51000 (55%)]\tLoss: -39350.117188\n",
            "Train Epoch: 4 [29440/51000 (58%)]\tLoss: -39484.976562\n",
            "Train Epoch: 4 [30720/51000 (60%)]\tLoss: -39605.523438\n",
            "Train Epoch: 4 [32000/51000 (63%)]\tLoss: -39765.691406\n",
            "Train Epoch: 4 [33280/51000 (65%)]\tLoss: -39904.105469\n",
            "Train Epoch: 4 [34560/51000 (68%)]\tLoss: -40007.656250\n",
            "Train Epoch: 4 [35840/51000 (70%)]\tLoss: -40165.718750\n",
            "Train Epoch: 4 [37120/51000 (73%)]\tLoss: -40294.171875\n",
            "Train Epoch: 4 [38400/51000 (75%)]\tLoss: -40403.921875\n",
            "Train Epoch: 4 [39680/51000 (78%)]\tLoss: -40602.625000\n",
            "Train Epoch: 4 [40960/51000 (80%)]\tLoss: -40705.800781\n",
            "Train Epoch: 4 [42240/51000 (83%)]\tLoss: -40849.375000\n",
            "Train Epoch: 4 [43520/51000 (85%)]\tLoss: -40954.875000\n",
            "Train Epoch: 4 [44800/51000 (88%)]\tLoss: -41088.640625\n",
            "Train Epoch: 4 [46080/51000 (90%)]\tLoss: -41217.960938\n",
            "Train Epoch: 4 [47360/51000 (93%)]\tLoss: -41375.953125\n",
            "Train Epoch: 4 [48640/51000 (95%)]\tLoss: -41503.867188\n",
            "Train Epoch: 4 [49920/51000 (98%)]\tLoss: -41675.582031\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-04-11 09:04:38,506]\u001b[0m Trial 7 finished with value: -41231.53572222222 and parameters: {'lr': 0.0028501459455535258, 'optimizer_name': 'RMSprop'}. Best is trial 4 with value: -19327550.577777777.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test set: Average loss: -41231.5357, Accuracy: 895/9000 (10%)\n",
            "\n",
            "\n",
            "********************************\n",
            "\n",
            "Suggested hyperparameters: \n",
            "{'lr': 0.007172912768922306, 'optimizer_name': 'RMSprop'}\n",
            "Train Epoch: 0 [0/51000 (0%)]\tLoss: 0.096019\n",
            "Train Epoch: 0 [1280/51000 (3%)]\tLoss: -814.177551\n",
            "Train Epoch: 0 [2560/51000 (5%)]\tLoss: -1776.977661\n",
            "Train Epoch: 0 [3840/51000 (8%)]\tLoss: -2779.577148\n",
            "Train Epoch: 0 [5120/51000 (10%)]\tLoss: -3814.858887\n",
            "Train Epoch: 0 [6400/51000 (13%)]\tLoss: -4887.177246\n",
            "Train Epoch: 0 [7680/51000 (15%)]\tLoss: -6041.904785\n",
            "Train Epoch: 0 [8960/51000 (18%)]\tLoss: -7190.743652\n",
            "Train Epoch: 0 [10240/51000 (20%)]\tLoss: -8407.656250\n",
            "Train Epoch: 0 [11520/51000 (23%)]\tLoss: -9551.738281\n",
            "Train Epoch: 0 [12800/51000 (25%)]\tLoss: -10802.731445\n",
            "Train Epoch: 0 [14080/51000 (28%)]\tLoss: -12054.379883\n",
            "Train Epoch: 0 [15360/51000 (30%)]\tLoss: -13318.097656\n",
            "Train Epoch: 0 [16640/51000 (33%)]\tLoss: -14593.533203\n",
            "Train Epoch: 0 [17920/51000 (35%)]\tLoss: -15848.604492\n",
            "Train Epoch: 0 [19200/51000 (38%)]\tLoss: -17235.996094\n",
            "Train Epoch: 0 [20480/51000 (40%)]\tLoss: -18630.701172\n",
            "Train Epoch: 0 [21760/51000 (43%)]\tLoss: -19910.671875\n",
            "Train Epoch: 0 [23040/51000 (45%)]\tLoss: -21496.396484\n",
            "Train Epoch: 0 [24320/51000 (48%)]\tLoss: -22954.972656\n",
            "Train Epoch: 0 [25600/51000 (50%)]\tLoss: -24440.953125\n",
            "Train Epoch: 0 [26880/51000 (53%)]\tLoss: -25950.041016\n",
            "Train Epoch: 0 [28160/51000 (55%)]\tLoss: -27470.876953\n",
            "Train Epoch: 0 [29440/51000 (58%)]\tLoss: -29045.279297\n",
            "Train Epoch: 0 [30720/51000 (60%)]\tLoss: -30536.898438\n",
            "Train Epoch: 0 [32000/51000 (63%)]\tLoss: -32255.320312\n",
            "Train Epoch: 0 [33280/51000 (65%)]\tLoss: -33872.695312\n",
            "Train Epoch: 0 [34560/51000 (68%)]\tLoss: -35459.835938\n",
            "Train Epoch: 0 [35840/51000 (70%)]\tLoss: -37221.773438\n",
            "Train Epoch: 0 [37120/51000 (73%)]\tLoss: -38926.429688\n",
            "Train Epoch: 0 [38400/51000 (75%)]\tLoss: -40662.351562\n",
            "Train Epoch: 0 [39680/51000 (78%)]\tLoss: -42499.183594\n",
            "Train Epoch: 0 [40960/51000 (80%)]\tLoss: -44293.339844\n",
            "Train Epoch: 0 [42240/51000 (83%)]\tLoss: -45990.695312\n",
            "Train Epoch: 0 [43520/51000 (85%)]\tLoss: -47904.140625\n",
            "Train Epoch: 0 [44800/51000 (88%)]\tLoss: -49809.117188\n",
            "Train Epoch: 0 [46080/51000 (90%)]\tLoss: -51776.632812\n",
            "Train Epoch: 0 [47360/51000 (93%)]\tLoss: -53624.550781\n",
            "Train Epoch: 0 [48640/51000 (95%)]\tLoss: -55637.718750\n",
            "Train Epoch: 0 [49920/51000 (98%)]\tLoss: -57631.625000\n",
            "Test set: Average loss: -62592.9583, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 1 [0/51000 (0%)]\tLoss: -59334.296875\n",
            "Train Epoch: 1 [1280/51000 (3%)]\tLoss: -60913.597656\n",
            "Train Epoch: 1 [2560/51000 (5%)]\tLoss: -62353.417969\n",
            "Train Epoch: 1 [3840/51000 (8%)]\tLoss: -63811.574219\n",
            "Train Epoch: 1 [5120/51000 (10%)]\tLoss: -65121.371094\n",
            "Train Epoch: 1 [6400/51000 (13%)]\tLoss: -66691.101562\n",
            "Train Epoch: 1 [7680/51000 (15%)]\tLoss: -68122.320312\n",
            "Train Epoch: 1 [8960/51000 (18%)]\tLoss: -69528.265625\n",
            "Train Epoch: 1 [10240/51000 (20%)]\tLoss: -71133.398438\n",
            "Train Epoch: 1 [11520/51000 (23%)]\tLoss: -72515.304688\n",
            "Train Epoch: 1 [12800/51000 (25%)]\tLoss: -74053.070312\n",
            "Train Epoch: 1 [14080/51000 (28%)]\tLoss: -75565.375000\n",
            "Train Epoch: 1 [15360/51000 (30%)]\tLoss: -77032.273438\n",
            "Train Epoch: 1 [16640/51000 (33%)]\tLoss: -78490.437500\n",
            "Train Epoch: 1 [17920/51000 (35%)]\tLoss: -79977.203125\n",
            "Train Epoch: 1 [19200/51000 (38%)]\tLoss: -81676.781250\n",
            "Train Epoch: 1 [20480/51000 (40%)]\tLoss: -83223.000000\n",
            "Train Epoch: 1 [21760/51000 (43%)]\tLoss: -84713.312500\n",
            "Train Epoch: 1 [23040/51000 (45%)]\tLoss: -86338.796875\n",
            "Train Epoch: 1 [24320/51000 (48%)]\tLoss: -87941.085938\n",
            "Train Epoch: 1 [25600/51000 (50%)]\tLoss: -89660.992188\n",
            "Train Epoch: 1 [26880/51000 (53%)]\tLoss: -91262.617188\n",
            "Train Epoch: 1 [28160/51000 (55%)]\tLoss: -92906.585938\n",
            "Train Epoch: 1 [29440/51000 (58%)]\tLoss: -94526.171875\n",
            "Train Epoch: 1 [30720/51000 (60%)]\tLoss: -96161.242188\n",
            "Train Epoch: 1 [32000/51000 (63%)]\tLoss: -97877.335938\n",
            "Train Epoch: 1 [33280/51000 (65%)]\tLoss: -99535.265625\n",
            "Train Epoch: 1 [34560/51000 (68%)]\tLoss: -101183.679688\n",
            "Train Epoch: 1 [35840/51000 (70%)]\tLoss: -102871.382812\n",
            "Train Epoch: 1 [37120/51000 (73%)]\tLoss: -104622.445312\n",
            "Train Epoch: 1 [38400/51000 (75%)]\tLoss: -106332.265625\n",
            "Train Epoch: 1 [39680/51000 (78%)]\tLoss: -108214.757812\n",
            "Train Epoch: 1 [40960/51000 (80%)]\tLoss: -109805.023438\n",
            "Train Epoch: 1 [42240/51000 (83%)]\tLoss: -111634.093750\n",
            "Train Epoch: 1 [43520/51000 (85%)]\tLoss: -112987.101562\n",
            "Train Epoch: 1 [44800/51000 (88%)]\tLoss: -115198.867188\n",
            "Train Epoch: 1 [46080/51000 (90%)]\tLoss: -116890.953125\n",
            "Train Epoch: 1 [47360/51000 (93%)]\tLoss: -118761.984375\n",
            "Train Epoch: 1 [48640/51000 (95%)]\tLoss: -120611.445312\n",
            "Train Epoch: 1 [49920/51000 (98%)]\tLoss: -122546.781250\n",
            "Test set: Average loss: -123957.3203, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 2 [0/51000 (0%)]\tLoss: -124008.796875\n",
            "Train Epoch: 2 [1280/51000 (3%)]\tLoss: -125437.468750\n",
            "Train Epoch: 2 [2560/51000 (5%)]\tLoss: -126819.093750\n",
            "Train Epoch: 2 [3840/51000 (8%)]\tLoss: -128117.570312\n",
            "Train Epoch: 2 [5120/51000 (10%)]\tLoss: -129235.250000\n",
            "Train Epoch: 2 [6400/51000 (13%)]\tLoss: -130703.601562\n",
            "Train Epoch: 2 [7680/51000 (15%)]\tLoss: -131970.703125\n",
            "Train Epoch: 2 [8960/51000 (18%)]\tLoss: -133330.328125\n",
            "Train Epoch: 2 [10240/51000 (20%)]\tLoss: -134816.421875\n",
            "Train Epoch: 2 [11520/51000 (23%)]\tLoss: -135936.125000\n",
            "Train Epoch: 2 [12800/51000 (25%)]\tLoss: -137095.609375\n",
            "Train Epoch: 2 [14080/51000 (28%)]\tLoss: -138766.296875\n",
            "Train Epoch: 2 [15360/51000 (30%)]\tLoss: -140060.203125\n",
            "Train Epoch: 2 [16640/51000 (33%)]\tLoss: -141336.046875\n",
            "Train Epoch: 2 [17920/51000 (35%)]\tLoss: -142773.750000\n",
            "Train Epoch: 2 [19200/51000 (38%)]\tLoss: -144094.187500\n",
            "Train Epoch: 2 [20480/51000 (40%)]\tLoss: -145602.843750\n",
            "Train Epoch: 2 [21760/51000 (43%)]\tLoss: -147010.953125\n",
            "Train Epoch: 2 [23040/51000 (45%)]\tLoss: -148398.687500\n",
            "Train Epoch: 2 [24320/51000 (48%)]\tLoss: -149673.671875\n",
            "Train Epoch: 2 [25600/51000 (50%)]\tLoss: -151237.296875\n",
            "Train Epoch: 2 [26880/51000 (53%)]\tLoss: -152396.140625\n",
            "Train Epoch: 2 [28160/51000 (55%)]\tLoss: -153987.375000\n",
            "Train Epoch: 2 [29440/51000 (58%)]\tLoss: -155342.671875\n",
            "Train Epoch: 2 [30720/51000 (60%)]\tLoss: -156713.203125\n",
            "Train Epoch: 2 [32000/51000 (63%)]\tLoss: -158189.453125\n",
            "Train Epoch: 2 [33280/51000 (65%)]\tLoss: -159585.671875\n",
            "Train Epoch: 2 [34560/51000 (68%)]\tLoss: -161002.515625\n",
            "Train Epoch: 2 [35840/51000 (70%)]\tLoss: -162562.765625\n",
            "Train Epoch: 2 [37120/51000 (73%)]\tLoss: -163902.468750\n",
            "Train Epoch: 2 [38400/51000 (75%)]\tLoss: -165181.140625\n",
            "Train Epoch: 2 [39680/51000 (78%)]\tLoss: -166991.593750\n",
            "Train Epoch: 2 [40960/51000 (80%)]\tLoss: -168323.078125\n",
            "Train Epoch: 2 [42240/51000 (83%)]\tLoss: -169893.640625\n",
            "Train Epoch: 2 [43520/51000 (85%)]\tLoss: -171165.328125\n",
            "Train Epoch: 2 [44800/51000 (88%)]\tLoss: -172835.765625\n",
            "Train Epoch: 2 [46080/51000 (90%)]\tLoss: -174232.562500\n",
            "Train Epoch: 2 [47360/51000 (93%)]\tLoss: -175851.421875\n",
            "Train Epoch: 2 [48640/51000 (95%)]\tLoss: -177169.562500\n",
            "Train Epoch: 2 [49920/51000 (98%)]\tLoss: -178864.328125\n",
            "Test set: Average loss: -181363.7577, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 3 [0/51000 (0%)]\tLoss: -180051.062500\n",
            "Train Epoch: 3 [1280/51000 (3%)]\tLoss: -181306.000000\n",
            "Train Epoch: 3 [2560/51000 (5%)]\tLoss: -182421.093750\n",
            "Train Epoch: 3 [3840/51000 (8%)]\tLoss: -183386.234375\n",
            "Train Epoch: 3 [5120/51000 (10%)]\tLoss: -184349.765625\n",
            "Train Epoch: 3 [6400/51000 (13%)]\tLoss: -185502.453125\n",
            "Train Epoch: 3 [7680/51000 (15%)]\tLoss: -186606.468750\n",
            "Train Epoch: 3 [8960/51000 (18%)]\tLoss: -187573.750000\n",
            "Train Epoch: 3 [10240/51000 (20%)]\tLoss: -189016.640625\n",
            "Train Epoch: 3 [11520/51000 (23%)]\tLoss: -189881.859375\n",
            "Train Epoch: 3 [12800/51000 (25%)]\tLoss: -190894.109375\n",
            "Train Epoch: 3 [14080/51000 (28%)]\tLoss: -192148.578125\n",
            "Train Epoch: 3 [15360/51000 (30%)]\tLoss: -193122.765625\n",
            "Train Epoch: 3 [16640/51000 (33%)]\tLoss: -194326.937500\n",
            "Train Epoch: 3 [17920/51000 (35%)]\tLoss: -195429.484375\n",
            "Train Epoch: 3 [19200/51000 (38%)]\tLoss: -196659.218750\n",
            "Train Epoch: 3 [20480/51000 (40%)]\tLoss: -197708.062500\n",
            "Train Epoch: 3 [21760/51000 (43%)]\tLoss: -198812.546875\n",
            "Train Epoch: 3 [23040/51000 (45%)]\tLoss: -199931.000000\n",
            "Train Epoch: 3 [24320/51000 (48%)]\tLoss: -200866.531250\n",
            "Train Epoch: 3 [25600/51000 (50%)]\tLoss: -202159.796875\n",
            "Train Epoch: 3 [26880/51000 (53%)]\tLoss: -203062.625000\n",
            "Train Epoch: 3 [28160/51000 (55%)]\tLoss: -204325.937500\n",
            "Train Epoch: 3 [29440/51000 (58%)]\tLoss: -205421.453125\n",
            "Train Epoch: 3 [30720/51000 (60%)]\tLoss: -206350.687500\n",
            "Train Epoch: 3 [32000/51000 (63%)]\tLoss: -207757.421875\n",
            "Train Epoch: 3 [33280/51000 (65%)]\tLoss: -208851.468750\n",
            "Train Epoch: 3 [34560/51000 (68%)]\tLoss: -209822.843750\n",
            "Train Epoch: 3 [35840/51000 (70%)]\tLoss: -211054.687500\n",
            "Train Epoch: 3 [37120/51000 (73%)]\tLoss: -212141.437500\n",
            "Train Epoch: 3 [38400/51000 (75%)]\tLoss: -213190.296875\n",
            "Train Epoch: 3 [39680/51000 (78%)]\tLoss: -214709.953125\n",
            "Train Epoch: 3 [40960/51000 (80%)]\tLoss: -215675.671875\n",
            "Train Epoch: 3 [42240/51000 (83%)]\tLoss: -216840.140625\n",
            "Train Epoch: 3 [43520/51000 (85%)]\tLoss: -217923.484375\n",
            "Train Epoch: 3 [44800/51000 (88%)]\tLoss: -219142.453125\n",
            "Train Epoch: 3 [46080/51000 (90%)]\tLoss: -220181.640625\n",
            "Train Epoch: 3 [47360/51000 (93%)]\tLoss: -221346.984375\n",
            "Train Epoch: 3 [48640/51000 (95%)]\tLoss: -222569.125000\n",
            "Train Epoch: 3 [49920/51000 (98%)]\tLoss: -223891.265625\n",
            "Test set: Average loss: -224955.1291, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 4 [0/51000 (0%)]\tLoss: -224496.921875\n",
            "Train Epoch: 4 [1280/51000 (3%)]\tLoss: -225622.859375\n",
            "Train Epoch: 4 [2560/51000 (5%)]\tLoss: -226455.343750\n",
            "Train Epoch: 4 [3840/51000 (8%)]\tLoss: -227440.406250\n",
            "Train Epoch: 4 [5120/51000 (10%)]\tLoss: -227910.531250\n",
            "Train Epoch: 4 [6400/51000 (13%)]\tLoss: -228786.984375\n",
            "Train Epoch: 4 [7680/51000 (15%)]\tLoss: -229646.906250\n",
            "Train Epoch: 4 [8960/51000 (18%)]\tLoss: -230414.937500\n",
            "Train Epoch: 4 [10240/51000 (20%)]\tLoss: -231542.875000\n",
            "Train Epoch: 4 [11520/51000 (23%)]\tLoss: -232046.890625\n",
            "Train Epoch: 4 [12800/51000 (25%)]\tLoss: -233074.093750\n",
            "Train Epoch: 4 [14080/51000 (28%)]\tLoss: -233970.390625\n",
            "Train Epoch: 4 [15360/51000 (30%)]\tLoss: -234664.531250\n",
            "Train Epoch: 4 [16640/51000 (33%)]\tLoss: -235453.375000\n",
            "Train Epoch: 4 [17920/51000 (35%)]\tLoss: -236230.171875\n",
            "Train Epoch: 4 [19200/51000 (38%)]\tLoss: -237199.562500\n",
            "Train Epoch: 4 [20480/51000 (40%)]\tLoss: -238051.203125\n",
            "Train Epoch: 4 [21760/51000 (43%)]\tLoss: -238958.265625\n",
            "Train Epoch: 4 [23040/51000 (45%)]\tLoss: -239760.562500\n",
            "Train Epoch: 4 [24320/51000 (48%)]\tLoss: -240489.750000\n",
            "Train Epoch: 4 [25600/51000 (50%)]\tLoss: -241511.484375\n",
            "Train Epoch: 4 [26880/51000 (53%)]\tLoss: -242129.046875\n",
            "Train Epoch: 4 [28160/51000 (55%)]\tLoss: -243140.734375\n",
            "Train Epoch: 4 [29440/51000 (58%)]\tLoss: -243917.093750\n",
            "Train Epoch: 4 [30720/51000 (60%)]\tLoss: -244726.359375\n",
            "Train Epoch: 4 [32000/51000 (63%)]\tLoss: -245732.406250\n",
            "Train Epoch: 4 [33280/51000 (65%)]\tLoss: -246515.250000\n",
            "Train Epoch: 4 [34560/51000 (68%)]\tLoss: -247204.031250\n",
            "Train Epoch: 4 [35840/51000 (70%)]\tLoss: -248188.218750\n",
            "Train Epoch: 4 [37120/51000 (73%)]\tLoss: -249038.593750\n",
            "Train Epoch: 4 [38400/51000 (75%)]\tLoss: -249746.968750\n",
            "Train Epoch: 4 [39680/51000 (78%)]\tLoss: -250992.859375\n",
            "Train Epoch: 4 [40960/51000 (80%)]\tLoss: -251622.859375\n",
            "Train Epoch: 4 [42240/51000 (83%)]\tLoss: -252533.312500\n",
            "Train Epoch: 4 [43520/51000 (85%)]\tLoss: -253239.140625\n",
            "Train Epoch: 4 [44800/51000 (88%)]\tLoss: -254178.031250\n",
            "Train Epoch: 4 [46080/51000 (90%)]\tLoss: -254914.765625\n",
            "Train Epoch: 4 [47360/51000 (93%)]\tLoss: -255941.296875\n",
            "Train Epoch: 4 [48640/51000 (95%)]\tLoss: -256727.765625\n",
            "Train Epoch: 4 [49920/51000 (98%)]\tLoss: -257793.484375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-04-11 09:32:36,012]\u001b[0m Trial 8 finished with value: -257964.29055555555 and parameters: {'lr': 0.007172912768922306, 'optimizer_name': 'RMSprop'}. Best is trial 4 with value: -19327550.577777777.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test set: Average loss: -257964.2906, Accuracy: 895/9000 (10%)\n",
            "\n",
            "\n",
            "********************************\n",
            "\n",
            "Suggested hyperparameters: \n",
            "{'lr': 0.003159516199927372, 'optimizer_name': 'RMSprop'}\n",
            "Train Epoch: 0 [0/51000 (0%)]\tLoss: -0.044565\n",
            "Train Epoch: 0 [1280/51000 (3%)]\tLoss: -247.787689\n",
            "Train Epoch: 0 [2560/51000 (5%)]\tLoss: -476.987030\n",
            "Train Epoch: 0 [3840/51000 (8%)]\tLoss: -683.428833\n",
            "Train Epoch: 0 [5120/51000 (10%)]\tLoss: -898.760620\n",
            "Train Epoch: 0 [6400/51000 (13%)]\tLoss: -1120.825562\n",
            "Train Epoch: 0 [7680/51000 (15%)]\tLoss: -1342.681274\n",
            "Train Epoch: 0 [8960/51000 (18%)]\tLoss: -1573.678101\n",
            "Train Epoch: 0 [10240/51000 (20%)]\tLoss: -1826.651611\n",
            "Train Epoch: 0 [11520/51000 (23%)]\tLoss: -2060.516113\n",
            "Train Epoch: 0 [12800/51000 (25%)]\tLoss: -2315.714355\n",
            "Train Epoch: 0 [14080/51000 (28%)]\tLoss: -2562.707764\n",
            "Train Epoch: 0 [15360/51000 (30%)]\tLoss: -2818.428223\n",
            "Train Epoch: 0 [16640/51000 (33%)]\tLoss: -3081.555420\n",
            "Train Epoch: 0 [17920/51000 (35%)]\tLoss: -3350.179443\n",
            "Train Epoch: 0 [19200/51000 (38%)]\tLoss: -3618.535400\n",
            "Train Epoch: 0 [20480/51000 (40%)]\tLoss: -3907.283203\n",
            "Train Epoch: 0 [21760/51000 (43%)]\tLoss: -4187.176270\n",
            "Train Epoch: 0 [23040/51000 (45%)]\tLoss: -4472.539062\n",
            "Train Epoch: 0 [24320/51000 (48%)]\tLoss: -4675.244629\n",
            "Train Epoch: 0 [25600/51000 (50%)]\tLoss: -5059.363770\n",
            "Train Epoch: 0 [26880/51000 (53%)]\tLoss: -5351.003418\n",
            "Train Epoch: 0 [28160/51000 (55%)]\tLoss: -5655.565918\n",
            "Train Epoch: 0 [29440/51000 (58%)]\tLoss: -5971.792480\n",
            "Train Epoch: 0 [30720/51000 (60%)]\tLoss: -6261.296875\n",
            "Train Epoch: 0 [32000/51000 (63%)]\tLoss: -6613.711914\n",
            "Train Epoch: 0 [33280/51000 (65%)]\tLoss: -6936.395020\n",
            "Train Epoch: 0 [34560/51000 (68%)]\tLoss: -7239.072754\n",
            "Train Epoch: 0 [35840/51000 (70%)]\tLoss: -7607.162598\n",
            "Train Epoch: 0 [37120/51000 (73%)]\tLoss: -7931.013184\n",
            "Train Epoch: 0 [38400/51000 (75%)]\tLoss: -8266.208008\n",
            "Train Epoch: 0 [39680/51000 (78%)]\tLoss: -8638.967773\n",
            "Train Epoch: 0 [40960/51000 (80%)]\tLoss: -8994.595703\n",
            "Train Epoch: 0 [42240/51000 (83%)]\tLoss: -9352.443359\n",
            "Train Epoch: 0 [43520/51000 (85%)]\tLoss: -9694.565430\n",
            "Train Epoch: 0 [44800/51000 (88%)]\tLoss: -10089.881836\n",
            "Train Epoch: 0 [46080/51000 (90%)]\tLoss: -10468.791992\n",
            "Train Epoch: 0 [47360/51000 (93%)]\tLoss: -10859.719727\n",
            "Train Epoch: 0 [48640/51000 (95%)]\tLoss: -11229.685547\n",
            "Train Epoch: 0 [49920/51000 (98%)]\tLoss: -11650.541016\n",
            "Test set: Average loss: -11985.7920, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 1 [0/51000 (0%)]\tLoss: -11975.845703\n",
            "Train Epoch: 1 [1280/51000 (3%)]\tLoss: -12274.044922\n",
            "Train Epoch: 1 [2560/51000 (5%)]\tLoss: -12557.876953\n",
            "Train Epoch: 1 [3840/51000 (8%)]\tLoss: -12837.475586\n",
            "Train Epoch: 1 [5120/51000 (10%)]\tLoss: -13091.516602\n",
            "Train Epoch: 1 [6400/51000 (13%)]\tLoss: -13407.461914\n",
            "Train Epoch: 1 [7680/51000 (15%)]\tLoss: -13691.617188\n",
            "Train Epoch: 1 [8960/51000 (18%)]\tLoss: -13984.802734\n",
            "Train Epoch: 1 [10240/51000 (20%)]\tLoss: -14293.638672\n",
            "Train Epoch: 1 [11520/51000 (23%)]\tLoss: -14572.087891\n",
            "Train Epoch: 1 [12800/51000 (25%)]\tLoss: -14868.945312\n",
            "Train Epoch: 1 [14080/51000 (28%)]\tLoss: -15165.411133\n",
            "Train Epoch: 1 [15360/51000 (30%)]\tLoss: -15446.614258\n",
            "Train Epoch: 1 [16640/51000 (33%)]\tLoss: -15761.840820\n",
            "Train Epoch: 1 [17920/51000 (35%)]\tLoss: -15956.747070\n",
            "Train Epoch: 1 [19200/51000 (38%)]\tLoss: -16389.599609\n",
            "Train Epoch: 1 [20480/51000 (40%)]\tLoss: -16700.060547\n",
            "Train Epoch: 1 [21760/51000 (43%)]\tLoss: -17018.576172\n",
            "Train Epoch: 1 [23040/51000 (45%)]\tLoss: -17329.568359\n",
            "Train Epoch: 1 [24320/51000 (48%)]\tLoss: -17604.656250\n",
            "Train Epoch: 1 [25600/51000 (50%)]\tLoss: -17970.927734\n",
            "Train Epoch: 1 [26880/51000 (53%)]\tLoss: -18273.958984\n",
            "Train Epoch: 1 [28160/51000 (55%)]\tLoss: -18594.507812\n",
            "Train Epoch: 1 [29440/51000 (58%)]\tLoss: -18933.957031\n",
            "Train Epoch: 1 [30720/51000 (60%)]\tLoss: -19240.628906\n",
            "Train Epoch: 1 [32000/51000 (63%)]\tLoss: -19574.367188\n",
            "Train Epoch: 1 [33280/51000 (65%)]\tLoss: -19902.345703\n",
            "Train Epoch: 1 [34560/51000 (68%)]\tLoss: -20221.939453\n",
            "Train Epoch: 1 [35840/51000 (70%)]\tLoss: -20577.998047\n",
            "Train Epoch: 1 [37120/51000 (73%)]\tLoss: -20915.193359\n",
            "Train Epoch: 1 [38400/51000 (75%)]\tLoss: -21246.640625\n",
            "Train Epoch: 1 [39680/51000 (78%)]\tLoss: -21618.798828\n",
            "Train Epoch: 1 [40960/51000 (80%)]\tLoss: -21943.296875\n",
            "Train Epoch: 1 [42240/51000 (83%)]\tLoss: -22299.121094\n",
            "Train Epoch: 1 [43520/51000 (85%)]\tLoss: -22644.128906\n",
            "Train Epoch: 1 [44800/51000 (88%)]\tLoss: -23002.183594\n",
            "Train Epoch: 1 [46080/51000 (90%)]\tLoss: -23345.353516\n",
            "Train Epoch: 1 [47360/51000 (93%)]\tLoss: -23678.074219\n",
            "Train Epoch: 1 [48640/51000 (95%)]\tLoss: -24016.814453\n",
            "Train Epoch: 1 [49920/51000 (98%)]\tLoss: -24434.750000\n",
            "Test set: Average loss: -23457.3572, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 2 [0/51000 (0%)]\tLoss: -24691.193359\n",
            "Train Epoch: 2 [1280/51000 (3%)]\tLoss: -24997.035156\n",
            "Train Epoch: 2 [2560/51000 (5%)]\tLoss: -25261.646484\n",
            "Train Epoch: 2 [3840/51000 (8%)]\tLoss: -25532.103516\n",
            "Train Epoch: 2 [5120/51000 (10%)]\tLoss: -25747.232422\n",
            "Train Epoch: 2 [6400/51000 (13%)]\tLoss: -26005.060547\n",
            "Train Epoch: 2 [7680/51000 (15%)]\tLoss: -26295.158203\n",
            "Train Epoch: 2 [8960/51000 (18%)]\tLoss: -26546.341797\n",
            "Train Epoch: 2 [10240/51000 (20%)]\tLoss: -26827.753906\n",
            "Train Epoch: 2 [11520/51000 (23%)]\tLoss: -27073.031250\n",
            "Train Epoch: 2 [12800/51000 (25%)]\tLoss: -27352.386719\n",
            "Train Epoch: 2 [14080/51000 (28%)]\tLoss: -27608.212891\n",
            "Train Epoch: 2 [15360/51000 (30%)]\tLoss: -27854.046875\n",
            "Train Epoch: 2 [16640/51000 (33%)]\tLoss: -28069.455078\n",
            "Train Epoch: 2 [17920/51000 (35%)]\tLoss: -28353.300781\n",
            "Train Epoch: 2 [19200/51000 (38%)]\tLoss: -28678.316406\n",
            "Train Epoch: 2 [20480/51000 (40%)]\tLoss: -28928.691406\n",
            "Train Epoch: 2 [21760/51000 (43%)]\tLoss: -29231.644531\n",
            "Train Epoch: 2 [23040/51000 (45%)]\tLoss: -29502.845703\n",
            "Train Epoch: 2 [24320/51000 (48%)]\tLoss: -29730.087891\n",
            "Train Epoch: 2 [25600/51000 (50%)]\tLoss: -30075.376953\n",
            "Train Epoch: 2 [26880/51000 (53%)]\tLoss: -30293.060547\n",
            "Train Epoch: 2 [28160/51000 (55%)]\tLoss: -30567.798828\n",
            "Train Epoch: 2 [29440/51000 (58%)]\tLoss: -30881.078125\n",
            "Train Epoch: 2 [30720/51000 (60%)]\tLoss: -31134.994141\n",
            "Train Epoch: 2 [32000/51000 (63%)]\tLoss: -31463.218750\n",
            "Train Epoch: 2 [33280/51000 (65%)]\tLoss: -31745.515625\n",
            "Train Epoch: 2 [34560/51000 (68%)]\tLoss: -31925.396484\n",
            "Train Epoch: 2 [35840/51000 (70%)]\tLoss: -32283.566406\n",
            "Train Epoch: 2 [37120/51000 (73%)]\tLoss: -32558.529297\n",
            "Train Epoch: 2 [38400/51000 (75%)]\tLoss: -32773.246094\n",
            "Train Epoch: 2 [39680/51000 (78%)]\tLoss: -33140.789062\n",
            "Train Epoch: 2 [40960/51000 (80%)]\tLoss: -33348.546875\n",
            "Train Epoch: 2 [42240/51000 (83%)]\tLoss: -33674.902344\n",
            "Train Epoch: 2 [43520/51000 (85%)]\tLoss: -33951.320312\n",
            "Train Epoch: 2 [44800/51000 (88%)]\tLoss: -34239.476562\n",
            "Train Epoch: 2 [46080/51000 (90%)]\tLoss: -34522.304688\n",
            "Train Epoch: 2 [47360/51000 (93%)]\tLoss: -34848.945312\n",
            "Train Epoch: 2 [48640/51000 (95%)]\tLoss: -35112.003906\n",
            "Train Epoch: 2 [49920/51000 (98%)]\tLoss: -35446.480469\n",
            "Test set: Average loss: -35808.8434, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 3 [0/51000 (0%)]\tLoss: -35643.378906\n",
            "Train Epoch: 3 [1280/51000 (3%)]\tLoss: -35903.558594\n",
            "Train Epoch: 3 [2560/51000 (5%)]\tLoss: -36129.636719\n",
            "Train Epoch: 3 [3840/51000 (8%)]\tLoss: -36354.851562\n",
            "Train Epoch: 3 [5120/51000 (10%)]\tLoss: -36533.445312\n",
            "Train Epoch: 3 [6400/51000 (13%)]\tLoss: -36733.839844\n",
            "Train Epoch: 3 [7680/51000 (15%)]\tLoss: -36962.132812\n",
            "Train Epoch: 3 [8960/51000 (18%)]\tLoss: -37181.062500\n",
            "Train Epoch: 3 [10240/51000 (20%)]\tLoss: -37399.515625\n",
            "Train Epoch: 3 [11520/51000 (23%)]\tLoss: -37584.640625\n",
            "Train Epoch: 3 [12800/51000 (25%)]\tLoss: -37829.296875\n",
            "Train Epoch: 3 [14080/51000 (28%)]\tLoss: -38039.640625\n",
            "Train Epoch: 3 [15360/51000 (30%)]\tLoss: -38247.195312\n",
            "Train Epoch: 3 [16640/51000 (33%)]\tLoss: -38453.488281\n",
            "Train Epoch: 3 [17920/51000 (35%)]\tLoss: -38657.355469\n",
            "Train Epoch: 3 [19200/51000 (38%)]\tLoss: -38922.609375\n",
            "Train Epoch: 3 [20480/51000 (40%)]\tLoss: -39113.554688\n",
            "Train Epoch: 3 [21760/51000 (43%)]\tLoss: -39345.046875\n",
            "Train Epoch: 3 [23040/51000 (45%)]\tLoss: -39555.523438\n",
            "Train Epoch: 3 [24320/51000 (48%)]\tLoss: -39728.617188\n",
            "Train Epoch: 3 [25600/51000 (50%)]\tLoss: -39994.855469\n",
            "Train Epoch: 3 [26880/51000 (53%)]\tLoss: -40174.964844\n",
            "Train Epoch: 3 [28160/51000 (55%)]\tLoss: -40416.718750\n",
            "Train Epoch: 3 [29440/51000 (58%)]\tLoss: -40643.773438\n",
            "Train Epoch: 3 [30720/51000 (60%)]\tLoss: -40862.550781\n",
            "Train Epoch: 3 [32000/51000 (63%)]\tLoss: -41089.765625\n",
            "Train Epoch: 3 [33280/51000 (65%)]\tLoss: -41313.839844\n",
            "Train Epoch: 3 [34560/51000 (68%)]\tLoss: -41509.847656\n",
            "Train Epoch: 3 [35840/51000 (70%)]\tLoss: -41757.035156\n",
            "Train Epoch: 3 [37120/51000 (73%)]\tLoss: -41939.570312\n",
            "Train Epoch: 3 [38400/51000 (75%)]\tLoss: -42146.359375\n",
            "Train Epoch: 3 [39680/51000 (78%)]\tLoss: -42441.554688\n",
            "Train Epoch: 3 [40960/51000 (80%)]\tLoss: -42634.445312\n",
            "Train Epoch: 3 [42240/51000 (83%)]\tLoss: -42878.570312\n",
            "Train Epoch: 3 [43520/51000 (85%)]\tLoss: -43081.003906\n",
            "Train Epoch: 3 [44800/51000 (88%)]\tLoss: -43313.976562\n",
            "Train Epoch: 3 [46080/51000 (90%)]\tLoss: -43510.664062\n",
            "Train Epoch: 3 [47360/51000 (93%)]\tLoss: -43778.640625\n",
            "Train Epoch: 3 [48640/51000 (95%)]\tLoss: -43977.562500\n",
            "Train Epoch: 3 [49920/51000 (98%)]\tLoss: -44253.996094\n",
            "Test set: Average loss: -44666.9398, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 4 [0/51000 (0%)]\tLoss: -44380.242188\n",
            "Train Epoch: 4 [1280/51000 (3%)]\tLoss: -44590.039062\n",
            "Train Epoch: 4 [2560/51000 (5%)]\tLoss: -44762.171875\n",
            "Train Epoch: 4 [3840/51000 (8%)]\tLoss: -44950.441406\n",
            "Train Epoch: 4 [5120/51000 (10%)]\tLoss: -45041.492188\n",
            "Train Epoch: 4 [6400/51000 (13%)]\tLoss: -45233.054688\n",
            "Train Epoch: 4 [7680/51000 (15%)]\tLoss: -45364.035156\n",
            "Train Epoch: 4 [8960/51000 (18%)]\tLoss: -45545.562500\n",
            "Train Epoch: 4 [10240/51000 (20%)]\tLoss: -45737.589844\n",
            "Train Epoch: 4 [11520/51000 (23%)]\tLoss: -45863.667969\n",
            "Train Epoch: 4 [12800/51000 (25%)]\tLoss: -46059.578125\n",
            "Train Epoch: 4 [14080/51000 (28%)]\tLoss: -46234.019531\n",
            "Train Epoch: 4 [15360/51000 (30%)]\tLoss: -46350.992188\n",
            "Train Epoch: 4 [16640/51000 (33%)]\tLoss: -46524.269531\n",
            "Train Epoch: 4 [17920/51000 (35%)]\tLoss: -46694.941406\n",
            "Train Epoch: 4 [19200/51000 (38%)]\tLoss: -46897.937500\n",
            "Train Epoch: 4 [20480/51000 (40%)]\tLoss: -47051.109375\n",
            "Train Epoch: 4 [21760/51000 (43%)]\tLoss: -47221.437500\n",
            "Train Epoch: 4 [23040/51000 (45%)]\tLoss: -47378.800781\n",
            "Train Epoch: 4 [24320/51000 (48%)]\tLoss: -47505.777344\n",
            "Train Epoch: 4 [25600/51000 (50%)]\tLoss: -47730.265625\n",
            "Train Epoch: 4 [26880/51000 (53%)]\tLoss: -47835.070312\n",
            "Train Epoch: 4 [28160/51000 (55%)]\tLoss: -48025.042969\n",
            "Train Epoch: 4 [29440/51000 (58%)]\tLoss: -48168.917969\n",
            "Train Epoch: 4 [30720/51000 (60%)]\tLoss: -48357.777344\n",
            "Train Epoch: 4 [32000/51000 (63%)]\tLoss: -48567.265625\n",
            "Train Epoch: 4 [33280/51000 (65%)]\tLoss: -48681.179688\n",
            "Train Epoch: 4 [34560/51000 (68%)]\tLoss: -48835.835938\n",
            "Train Epoch: 4 [35840/51000 (70%)]\tLoss: -49023.917969\n",
            "Train Epoch: 4 [37120/51000 (73%)]\tLoss: -49164.175781\n",
            "Train Epoch: 4 [38400/51000 (75%)]\tLoss: -49304.371094\n",
            "Train Epoch: 4 [39680/51000 (78%)]\tLoss: -49555.265625\n",
            "Train Epoch: 4 [40960/51000 (80%)]\tLoss: -49691.750000\n",
            "Train Epoch: 4 [42240/51000 (83%)]\tLoss: -49859.140625\n",
            "Train Epoch: 4 [43520/51000 (85%)]\tLoss: -49993.136719\n",
            "Train Epoch: 4 [44800/51000 (88%)]\tLoss: -50178.386719\n",
            "Train Epoch: 4 [46080/51000 (90%)]\tLoss: -50339.378906\n",
            "Train Epoch: 4 [47360/51000 (93%)]\tLoss: -50528.765625\n",
            "Train Epoch: 4 [48640/51000 (95%)]\tLoss: -50662.691406\n",
            "Train Epoch: 4 [49920/51000 (98%)]\tLoss: -50899.941406\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-04-11 10:00:35,820]\u001b[0m Trial 9 finished with value: -46643.89763888889 and parameters: {'lr': 0.003159516199927372, 'optimizer_name': 'RMSprop'}. Best is trial 4 with value: -19327550.577777777.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test set: Average loss: -46643.8976, Accuracy: 895/9000 (10%)\n",
            "\n",
            "\n",
            "++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Study statistics: \n",
            "  Number of finished trials:  10\n",
            "Best trial:\n",
            "  Trial number:  4\n",
            "  Loss (trial value):  -19327550.577777777\n",
            "  Params: \n",
            "    lr: 0.055366333760817794\n",
            "    optimizer_name: Adam\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JyhQSHtY1dh",
        "outputId": "eeb99c7a-49cc-4780-d249-ddef34802345"
      },
      "source": [
        "    # Create the optuna study which shares the experiment name\n",
        "    study = optuna.create_study(study_name=\"pytorch-mlflow-optuna\", direction=\"minimize\")\n",
        "    study.optimize(objective, n_trials=5)\n",
        "\n",
        "    # Print optuna study statistics\n",
        "    print(\"\\n++++++++++++++++++++++++++++++++++\\n\")\n",
        "    print(\"Study statistics: \")\n",
        "    print(\"  Number of finished trials: \", len(study.trials))\n",
        "\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "\n",
        "    print(\"  Trial number: \", trial.number)\n",
        "    print(\"  Loss (trial value): \", trial.value)\n",
        "\n",
        "    print(\"  Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(\"    {}: {}\".format(key, value))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-04-10 02:01:39,833]\u001b[0m A new study created in memory with name: pytorch-mlflow-optuna\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "********************************\n",
            "\n",
            "Suggested hyperparameters: \n",
            "{'lr': 0.0036371172103828496, 'optimizer_name': 'RMSprop'}\n",
            "Train Epoch: 0 [0/51000 (0%)]\tLoss: -0.236404\n",
            "Train Epoch: 0 [1280/51000 (3%)]\tLoss: -223.694443\n",
            "Train Epoch: 0 [2560/51000 (5%)]\tLoss: -439.426666\n",
            "Train Epoch: 0 [3840/51000 (8%)]\tLoss: -648.061401\n",
            "Train Epoch: 0 [5120/51000 (10%)]\tLoss: -850.893860\n",
            "Train Epoch: 0 [6400/51000 (13%)]\tLoss: -1073.871704\n",
            "Train Epoch: 0 [7680/51000 (15%)]\tLoss: -1305.845337\n",
            "Train Epoch: 0 [8960/51000 (18%)]\tLoss: -1544.559204\n",
            "Train Epoch: 0 [10240/51000 (20%)]\tLoss: -1784.295654\n",
            "Train Epoch: 0 [11520/51000 (23%)]\tLoss: -2019.162598\n",
            "Train Epoch: 0 [12800/51000 (25%)]\tLoss: -2258.934570\n",
            "Train Epoch: 0 [14080/51000 (28%)]\tLoss: -2464.309082\n",
            "Train Epoch: 0 [15360/51000 (30%)]\tLoss: -2755.195557\n",
            "Train Epoch: 0 [16640/51000 (33%)]\tLoss: -3015.674316\n",
            "Train Epoch: 0 [17920/51000 (35%)]\tLoss: -3282.557129\n",
            "Train Epoch: 0 [19200/51000 (38%)]\tLoss: -3545.443115\n",
            "Train Epoch: 0 [20480/51000 (40%)]\tLoss: -3824.847656\n",
            "Train Epoch: 0 [21760/51000 (43%)]\tLoss: -4076.361572\n",
            "Train Epoch: 0 [23040/51000 (45%)]\tLoss: -4382.284668\n",
            "Train Epoch: 0 [24320/51000 (48%)]\tLoss: -4666.735840\n",
            "Train Epoch: 0 [25600/51000 (50%)]\tLoss: -4967.488281\n",
            "Train Epoch: 0 [26880/51000 (53%)]\tLoss: -5258.544434\n",
            "Train Epoch: 0 [28160/51000 (55%)]\tLoss: -5558.514648\n",
            "Train Epoch: 0 [29440/51000 (58%)]\tLoss: -5873.615234\n",
            "Train Epoch: 0 [30720/51000 (60%)]\tLoss: -6178.522461\n",
            "Train Epoch: 0 [32000/51000 (63%)]\tLoss: -6511.823730\n",
            "Train Epoch: 0 [33280/51000 (65%)]\tLoss: -6830.324219\n",
            "Train Epoch: 0 [34560/51000 (68%)]\tLoss: -7145.152832\n",
            "Train Epoch: 0 [35840/51000 (70%)]\tLoss: -7457.914551\n",
            "Train Epoch: 0 [37120/51000 (73%)]\tLoss: -7814.715820\n",
            "Train Epoch: 0 [38400/51000 (75%)]\tLoss: -8154.258789\n",
            "Train Epoch: 0 [39680/51000 (78%)]\tLoss: -8513.688477\n",
            "Train Epoch: 0 [40960/51000 (80%)]\tLoss: -8854.725586\n",
            "Train Epoch: 0 [42240/51000 (83%)]\tLoss: -9231.837891\n",
            "Train Epoch: 0 [43520/51000 (85%)]\tLoss: -9581.598633\n",
            "Train Epoch: 0 [44800/51000 (88%)]\tLoss: -9957.938477\n",
            "Train Epoch: 0 [46080/51000 (90%)]\tLoss: -10329.461914\n",
            "Train Epoch: 0 [47360/51000 (93%)]\tLoss: -10708.324219\n",
            "Train Epoch: 0 [48640/51000 (95%)]\tLoss: -11086.352539\n",
            "Train Epoch: 0 [49920/51000 (98%)]\tLoss: -11493.466797\n",
            "Test set: Average loss: -12311.6150, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 1 [0/51000 (0%)]\tLoss: -11793.436523\n",
            "Train Epoch: 1 [1280/51000 (3%)]\tLoss: -12115.492188\n",
            "Train Epoch: 1 [2560/51000 (5%)]\tLoss: -12389.153320\n",
            "Train Epoch: 1 [3840/51000 (8%)]\tLoss: -12693.375977\n",
            "Train Epoch: 1 [5120/51000 (10%)]\tLoss: -12954.161133\n",
            "Train Epoch: 1 [6400/51000 (13%)]\tLoss: -13248.144531\n",
            "Train Epoch: 1 [7680/51000 (15%)]\tLoss: -13536.168945\n",
            "Train Epoch: 1 [8960/51000 (18%)]\tLoss: -13818.937500\n",
            "Train Epoch: 1 [10240/51000 (20%)]\tLoss: -14125.068359\n",
            "Train Epoch: 1 [11520/51000 (23%)]\tLoss: -14396.685547\n",
            "Train Epoch: 1 [12800/51000 (25%)]\tLoss: -14687.458984\n",
            "Train Epoch: 1 [14080/51000 (28%)]\tLoss: -14996.078125\n",
            "Train Epoch: 1 [15360/51000 (30%)]\tLoss: -15277.318359\n",
            "Train Epoch: 1 [16640/51000 (33%)]\tLoss: -15574.207031\n",
            "Train Epoch: 1 [17920/51000 (35%)]\tLoss: -15885.976562\n",
            "Train Epoch: 1 [19200/51000 (38%)]\tLoss: -16179.164062\n",
            "Train Epoch: 1 [20480/51000 (40%)]\tLoss: -16494.347656\n",
            "Train Epoch: 1 [21760/51000 (43%)]\tLoss: -16814.603516\n",
            "Train Epoch: 1 [23040/51000 (45%)]\tLoss: -17110.394531\n",
            "Train Epoch: 1 [24320/51000 (48%)]\tLoss: -17413.007812\n",
            "Train Epoch: 1 [25600/51000 (50%)]\tLoss: -17753.345703\n",
            "Train Epoch: 1 [26880/51000 (53%)]\tLoss: -18047.568359\n",
            "Train Epoch: 1 [28160/51000 (55%)]\tLoss: -18369.378906\n",
            "Train Epoch: 1 [29440/51000 (58%)]\tLoss: -18697.062500\n",
            "Train Epoch: 1 [30720/51000 (60%)]\tLoss: -19014.099609\n",
            "Train Epoch: 1 [32000/51000 (63%)]\tLoss: -19353.566406\n",
            "Train Epoch: 1 [33280/51000 (65%)]\tLoss: -19684.154297\n",
            "Train Epoch: 1 [34560/51000 (68%)]\tLoss: -19991.564453\n",
            "Train Epoch: 1 [35840/51000 (70%)]\tLoss: -20329.746094\n",
            "Train Epoch: 1 [37120/51000 (73%)]\tLoss: -20669.265625\n",
            "Train Epoch: 1 [38400/51000 (75%)]\tLoss: -20993.796875\n",
            "Train Epoch: 1 [39680/51000 (78%)]\tLoss: -21364.673828\n",
            "Train Epoch: 1 [40960/51000 (80%)]\tLoss: -21695.179688\n",
            "Train Epoch: 1 [42240/51000 (83%)]\tLoss: -22027.546875\n",
            "Train Epoch: 1 [43520/51000 (85%)]\tLoss: -22377.210938\n",
            "Train Epoch: 1 [44800/51000 (88%)]\tLoss: -22741.712891\n",
            "Train Epoch: 1 [46080/51000 (90%)]\tLoss: -23082.957031\n",
            "Train Epoch: 1 [47360/51000 (93%)]\tLoss: -23422.857422\n",
            "Train Epoch: 1 [48640/51000 (95%)]\tLoss: -23796.031250\n",
            "Train Epoch: 1 [49920/51000 (98%)]\tLoss: -24173.757812\n",
            "Test set: Average loss: -24412.6181, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 2 [0/51000 (0%)]\tLoss: -24448.328125\n",
            "Train Epoch: 2 [1280/51000 (3%)]\tLoss: -24727.238281\n",
            "Train Epoch: 2 [2560/51000 (5%)]\tLoss: -24990.636719\n",
            "Train Epoch: 2 [3840/51000 (8%)]\tLoss: -25278.474609\n",
            "Train Epoch: 2 [5120/51000 (10%)]\tLoss: -25501.822266\n",
            "Train Epoch: 2 [6400/51000 (13%)]\tLoss: -25762.785156\n",
            "Train Epoch: 2 [7680/51000 (15%)]\tLoss: -26029.951172\n",
            "Train Epoch: 2 [8960/51000 (18%)]\tLoss: -26273.183594\n",
            "Train Epoch: 2 [10240/51000 (20%)]\tLoss: -26572.835938\n",
            "Train Epoch: 2 [11520/51000 (23%)]\tLoss: -26795.689453\n",
            "Train Epoch: 2 [12800/51000 (25%)]\tLoss: -27061.691406\n",
            "Train Epoch: 2 [14080/51000 (28%)]\tLoss: -27327.933594\n",
            "Train Epoch: 2 [15360/51000 (30%)]\tLoss: -27572.982422\n",
            "Train Epoch: 2 [16640/51000 (33%)]\tLoss: -27845.636719\n",
            "Train Epoch: 2 [17920/51000 (35%)]\tLoss: -28121.388672\n",
            "Train Epoch: 2 [19200/51000 (38%)]\tLoss: -28365.423828\n",
            "Train Epoch: 2 [20480/51000 (40%)]\tLoss: -28659.376953\n",
            "Train Epoch: 2 [21760/51000 (43%)]\tLoss: -28943.154297\n",
            "Train Epoch: 2 [23040/51000 (45%)]\tLoss: -29191.167969\n",
            "Train Epoch: 2 [24320/51000 (48%)]\tLoss: -29448.437500\n",
            "Train Epoch: 2 [25600/51000 (50%)]\tLoss: -29755.572266\n",
            "Train Epoch: 2 [26880/51000 (53%)]\tLoss: -29981.736328\n",
            "Train Epoch: 2 [28160/51000 (55%)]\tLoss: -30296.269531\n",
            "Train Epoch: 2 [29440/51000 (58%)]\tLoss: -30575.382812\n",
            "Train Epoch: 2 [30720/51000 (60%)]\tLoss: -30827.806641\n",
            "Train Epoch: 2 [32000/51000 (63%)]\tLoss: -31143.427734\n",
            "Train Epoch: 2 [33280/51000 (65%)]\tLoss: -31410.208984\n",
            "Train Epoch: 2 [34560/51000 (68%)]\tLoss: -31652.660156\n",
            "Train Epoch: 2 [35840/51000 (70%)]\tLoss: -31966.468750\n",
            "Train Epoch: 2 [37120/51000 (73%)]\tLoss: -32244.912109\n",
            "Train Epoch: 2 [38400/51000 (75%)]\tLoss: -32488.773438\n",
            "Train Epoch: 2 [39680/51000 (78%)]\tLoss: -32840.925781\n",
            "Train Epoch: 2 [40960/51000 (80%)]\tLoss: -33094.191406\n",
            "Train Epoch: 2 [42240/51000 (83%)]\tLoss: -33397.886719\n",
            "Train Epoch: 2 [43520/51000 (85%)]\tLoss: -33662.332031\n",
            "Train Epoch: 2 [44800/51000 (88%)]\tLoss: -33969.558594\n",
            "Train Epoch: 2 [46080/51000 (90%)]\tLoss: -34242.363281\n",
            "Train Epoch: 2 [47360/51000 (93%)]\tLoss: -34538.617188\n",
            "Train Epoch: 2 [48640/51000 (95%)]\tLoss: -34827.386719\n",
            "Train Epoch: 2 [49920/51000 (98%)]\tLoss: -35143.335938\n",
            "Test set: Average loss: -34784.5621, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 3 [0/51000 (0%)]\tLoss: -35308.957031\n",
            "Train Epoch: 3 [1280/51000 (3%)]\tLoss: -35574.187500\n",
            "Train Epoch: 3 [2560/51000 (5%)]\tLoss: -35787.953125\n",
            "Train Epoch: 3 [3840/51000 (8%)]\tLoss: -36036.906250\n",
            "Train Epoch: 3 [5120/51000 (10%)]\tLoss: -36193.789062\n",
            "Train Epoch: 3 [6400/51000 (13%)]\tLoss: -36409.355469\n",
            "Train Epoch: 3 [7680/51000 (15%)]\tLoss: -36626.250000\n",
            "Train Epoch: 3 [8960/51000 (18%)]\tLoss: -36835.437500\n",
            "Train Epoch: 3 [10240/51000 (20%)]\tLoss: -37080.597656\n",
            "Train Epoch: 3 [11520/51000 (23%)]\tLoss: -37228.160156\n",
            "Train Epoch: 3 [12800/51000 (25%)]\tLoss: -37483.578125\n",
            "Train Epoch: 3 [14080/51000 (28%)]\tLoss: -37694.167969\n",
            "Train Epoch: 3 [15360/51000 (30%)]\tLoss: -37867.324219\n",
            "Train Epoch: 3 [16640/51000 (33%)]\tLoss: -38092.929688\n",
            "Train Epoch: 3 [17920/51000 (35%)]\tLoss: -38315.601562\n",
            "Train Epoch: 3 [19200/51000 (38%)]\tLoss: -38529.445312\n",
            "Train Epoch: 3 [20480/51000 (40%)]\tLoss: -38758.105469\n",
            "Train Epoch: 3 [21760/51000 (43%)]\tLoss: -38991.835938\n",
            "Train Epoch: 3 [23040/51000 (45%)]\tLoss: -39177.359375\n",
            "Train Epoch: 3 [24320/51000 (48%)]\tLoss: -39368.492188\n",
            "Train Epoch: 3 [25600/51000 (50%)]\tLoss: -39624.523438\n",
            "Train Epoch: 3 [26880/51000 (53%)]\tLoss: -39813.476562\n",
            "Train Epoch: 3 [28160/51000 (55%)]\tLoss: -40019.957031\n",
            "Train Epoch: 3 [29440/51000 (58%)]\tLoss: -40261.082031\n",
            "Train Epoch: 3 [30720/51000 (60%)]\tLoss: -40473.929688\n",
            "Train Epoch: 3 [32000/51000 (63%)]\tLoss: -40725.953125\n",
            "Train Epoch: 3 [33280/51000 (65%)]\tLoss: -40926.941406\n",
            "Train Epoch: 3 [34560/51000 (68%)]\tLoss: -41119.496094\n",
            "Train Epoch: 3 [35840/51000 (70%)]\tLoss: -41367.507812\n",
            "Train Epoch: 3 [37120/51000 (73%)]\tLoss: -41602.976562\n",
            "Train Epoch: 3 [38400/51000 (75%)]\tLoss: -41791.105469\n",
            "Train Epoch: 3 [39680/51000 (78%)]\tLoss: -42066.265625\n",
            "Train Epoch: 3 [40960/51000 (80%)]\tLoss: -42258.257812\n",
            "Train Epoch: 3 [42240/51000 (83%)]\tLoss: -42484.031250\n",
            "Train Epoch: 3 [43520/51000 (85%)]\tLoss: -42680.933594\n",
            "Train Epoch: 3 [44800/51000 (88%)]\tLoss: -42934.550781\n",
            "Train Epoch: 3 [46080/51000 (90%)]\tLoss: -43134.214844\n",
            "Train Epoch: 3 [47360/51000 (93%)]\tLoss: -43380.722656\n",
            "Train Epoch: 3 [48640/51000 (95%)]\tLoss: -43561.593750\n",
            "Train Epoch: 3 [49920/51000 (98%)]\tLoss: -43842.062500\n",
            "Test set: Average loss: -44100.4578, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 4 [0/51000 (0%)]\tLoss: -43979.304688\n",
            "Train Epoch: 4 [1280/51000 (3%)]\tLoss: -44182.929688\n",
            "Train Epoch: 4 [2560/51000 (5%)]\tLoss: -44359.914062\n",
            "Train Epoch: 4 [3840/51000 (8%)]\tLoss: -44526.148438\n",
            "Train Epoch: 4 [5120/51000 (10%)]\tLoss: -44650.992188\n",
            "Train Epoch: 4 [6400/51000 (13%)]\tLoss: -44807.738281\n",
            "Train Epoch: 4 [7680/51000 (15%)]\tLoss: -44985.691406\n",
            "Train Epoch: 4 [8960/51000 (18%)]\tLoss: -45134.875000\n",
            "Train Epoch: 4 [10240/51000 (20%)]\tLoss: -45348.558594\n",
            "Train Epoch: 4 [11520/51000 (23%)]\tLoss: -45446.332031\n",
            "Train Epoch: 4 [12800/51000 (25%)]\tLoss: -45624.601562\n",
            "Train Epoch: 4 [14080/51000 (28%)]\tLoss: -45818.679688\n",
            "Train Epoch: 4 [15360/51000 (30%)]\tLoss: -45934.246094\n",
            "Train Epoch: 4 [16640/51000 (33%)]\tLoss: -46093.542969\n",
            "Train Epoch: 4 [17920/51000 (35%)]\tLoss: -46269.785156\n",
            "Train Epoch: 4 [19200/51000 (38%)]\tLoss: -46435.625000\n",
            "Train Epoch: 4 [20480/51000 (40%)]\tLoss: -46612.199219\n",
            "Train Epoch: 4 [21760/51000 (43%)]\tLoss: -46799.390625\n",
            "Train Epoch: 4 [23040/51000 (45%)]\tLoss: -46930.367188\n",
            "Train Epoch: 4 [24320/51000 (48%)]\tLoss: -47081.296875\n",
            "Train Epoch: 4 [25600/51000 (50%)]\tLoss: -47291.171875\n",
            "Train Epoch: 4 [26880/51000 (53%)]\tLoss: -47406.253906\n",
            "Train Epoch: 4 [28160/51000 (55%)]\tLoss: -47599.800781\n",
            "Train Epoch: 4 [29440/51000 (58%)]\tLoss: -47768.785156\n",
            "Train Epoch: 4 [30720/51000 (60%)]\tLoss: -47915.753906\n",
            "Train Epoch: 4 [32000/51000 (63%)]\tLoss: -48111.789062\n",
            "Train Epoch: 4 [33280/51000 (65%)]\tLoss: -48261.156250\n",
            "Train Epoch: 4 [34560/51000 (68%)]\tLoss: -48399.558594\n",
            "Train Epoch: 4 [35840/51000 (70%)]\tLoss: -48573.460938\n",
            "Train Epoch: 4 [37120/51000 (73%)]\tLoss: -48747.839844\n",
            "Train Epoch: 4 [38400/51000 (75%)]\tLoss: -48885.738281\n",
            "Train Epoch: 4 [39680/51000 (78%)]\tLoss: -49110.074219\n",
            "Train Epoch: 4 [40960/51000 (80%)]\tLoss: -49241.457031\n",
            "Train Epoch: 4 [42240/51000 (83%)]\tLoss: -49414.281250\n",
            "Train Epoch: 4 [43520/51000 (85%)]\tLoss: -49559.968750\n",
            "Train Epoch: 4 [44800/51000 (88%)]\tLoss: -49749.859375\n",
            "Train Epoch: 4 [46080/51000 (90%)]\tLoss: -49904.546875\n",
            "Train Epoch: 4 [47360/51000 (93%)]\tLoss: -50070.820312\n",
            "Train Epoch: 4 [48640/51000 (95%)]\tLoss: -50218.035156\n",
            "Train Epoch: 4 [49920/51000 (98%)]\tLoss: -50428.566406\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-04-10 02:15:51,764]\u001b[0m Trial 0 finished with value: -50743.825666666664 and parameters: {'lr': 0.0036371172103828496, 'optimizer_name': 'RMSprop'}. Best is trial 0 with value: -50743.825666666664.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test set: Average loss: -50743.8257, Accuracy: 895/9000 (10%)\n",
            "\n",
            "\n",
            "********************************\n",
            "\n",
            "Suggested hyperparameters: \n",
            "{'lr': 0.022381284365837674, 'optimizer_name': 'Adam'}\n",
            "Train Epoch: 0 [0/51000 (0%)]\tLoss: -0.084154\n",
            "Train Epoch: 0 [1280/51000 (3%)]\tLoss: -215.963669\n",
            "Train Epoch: 0 [2560/51000 (5%)]\tLoss: -634.797363\n",
            "Train Epoch: 0 [3840/51000 (8%)]\tLoss: -1294.661499\n",
            "Train Epoch: 0 [5120/51000 (10%)]\tLoss: -2291.163818\n",
            "Train Epoch: 0 [6400/51000 (13%)]\tLoss: -3741.662842\n",
            "Train Epoch: 0 [7680/51000 (15%)]\tLoss: -5594.219727\n",
            "Train Epoch: 0 [8960/51000 (18%)]\tLoss: -7929.376953\n",
            "Train Epoch: 0 [10240/51000 (20%)]\tLoss: -10733.704102\n",
            "Train Epoch: 0 [11520/51000 (23%)]\tLoss: -14033.695312\n",
            "Train Epoch: 0 [12800/51000 (25%)]\tLoss: -17789.322266\n",
            "Train Epoch: 0 [14080/51000 (28%)]\tLoss: -22077.333984\n",
            "Train Epoch: 0 [15360/51000 (30%)]\tLoss: -26829.294922\n",
            "Train Epoch: 0 [16640/51000 (33%)]\tLoss: -32034.367188\n",
            "Train Epoch: 0 [17920/51000 (35%)]\tLoss: -37894.160156\n",
            "Train Epoch: 0 [19200/51000 (38%)]\tLoss: -44116.753906\n",
            "Train Epoch: 0 [20480/51000 (40%)]\tLoss: -51068.921875\n",
            "Train Epoch: 0 [21760/51000 (43%)]\tLoss: -58305.710938\n",
            "Train Epoch: 0 [23040/51000 (45%)]\tLoss: -66197.132812\n",
            "Train Epoch: 0 [24320/51000 (48%)]\tLoss: -74465.640625\n",
            "Train Epoch: 0 [25600/51000 (50%)]\tLoss: -83386.953125\n",
            "Train Epoch: 0 [26880/51000 (53%)]\tLoss: -92637.882812\n",
            "Train Epoch: 0 [28160/51000 (55%)]\tLoss: -102476.984375\n",
            "Train Epoch: 0 [29440/51000 (58%)]\tLoss: -112810.632812\n",
            "Train Epoch: 0 [30720/51000 (60%)]\tLoss: -123487.679688\n",
            "Train Epoch: 0 [32000/51000 (63%)]\tLoss: -134925.546875\n",
            "Train Epoch: 0 [33280/51000 (65%)]\tLoss: -146638.312500\n",
            "Train Epoch: 0 [34560/51000 (68%)]\tLoss: -158812.296875\n",
            "Train Epoch: 0 [35840/51000 (70%)]\tLoss: -171746.734375\n",
            "Train Epoch: 0 [37120/51000 (73%)]\tLoss: -184893.484375\n",
            "Train Epoch: 0 [38400/51000 (75%)]\tLoss: -198541.890625\n",
            "Train Epoch: 0 [39680/51000 (78%)]\tLoss: -212916.343750\n",
            "Train Epoch: 0 [40960/51000 (80%)]\tLoss: -227382.156250\n",
            "Train Epoch: 0 [42240/51000 (83%)]\tLoss: -242442.203125\n",
            "Train Epoch: 0 [43520/51000 (85%)]\tLoss: -258148.078125\n",
            "Train Epoch: 0 [44800/51000 (88%)]\tLoss: -274225.656250\n",
            "Train Epoch: 0 [46080/51000 (90%)]\tLoss: -290453.718750\n",
            "Train Epoch: 0 [47360/51000 (93%)]\tLoss: -307457.437500\n",
            "Train Epoch: 0 [48640/51000 (95%)]\tLoss: -325148.750000\n",
            "Train Epoch: 0 [49920/51000 (98%)]\tLoss: -343542.500000\n",
            "Test set: Average loss: -343121.6773, Accuracy: 892/9000 (10%)\n",
            "\n",
            "Train Epoch: 1 [0/51000 (0%)]\tLoss: -359778.593750\n",
            "Train Epoch: 1 [1280/51000 (3%)]\tLoss: -372830.187500\n",
            "Train Epoch: 1 [2560/51000 (5%)]\tLoss: -386348.156250\n",
            "Train Epoch: 1 [3840/51000 (8%)]\tLoss: -399954.812500\n",
            "Train Epoch: 1 [5120/51000 (10%)]\tLoss: -413292.718750\n",
            "Train Epoch: 1 [6400/51000 (13%)]\tLoss: -427059.781250\n",
            "Train Epoch: 1 [7680/51000 (15%)]\tLoss: -440880.562500\n",
            "Train Epoch: 1 [8960/51000 (18%)]\tLoss: -454817.156250\n",
            "Train Epoch: 1 [10240/51000 (20%)]\tLoss: -469228.812500\n",
            "Train Epoch: 1 [11520/51000 (23%)]\tLoss: -482877.187500\n",
            "Train Epoch: 1 [12800/51000 (25%)]\tLoss: -497658.281250\n",
            "Train Epoch: 1 [14080/51000 (28%)]\tLoss: -512062.437500\n",
            "Train Epoch: 1 [15360/51000 (30%)]\tLoss: -526899.187500\n",
            "Train Epoch: 1 [16640/51000 (33%)]\tLoss: -541796.625000\n",
            "Train Epoch: 1 [17920/51000 (35%)]\tLoss: -556878.000000\n",
            "Train Epoch: 1 [19200/51000 (38%)]\tLoss: -572303.062500\n",
            "Train Epoch: 1 [20480/51000 (40%)]\tLoss: -587403.625000\n",
            "Train Epoch: 1 [21760/51000 (43%)]\tLoss: -603231.812500\n",
            "Train Epoch: 1 [23040/51000 (45%)]\tLoss: -618918.562500\n",
            "Train Epoch: 1 [24320/51000 (48%)]\tLoss: -634679.062500\n",
            "Train Epoch: 1 [25600/51000 (50%)]\tLoss: -650986.125000\n",
            "Train Epoch: 1 [26880/51000 (53%)]\tLoss: -666639.750000\n",
            "Train Epoch: 1 [28160/51000 (55%)]\tLoss: -683707.562500\n",
            "Train Epoch: 1 [29440/51000 (58%)]\tLoss: -699891.062500\n",
            "Train Epoch: 1 [30720/51000 (60%)]\tLoss: -716328.750000\n",
            "Train Epoch: 1 [32000/51000 (63%)]\tLoss: -733763.875000\n",
            "Train Epoch: 1 [33280/51000 (65%)]\tLoss: -750478.125000\n",
            "Train Epoch: 1 [34560/51000 (68%)]\tLoss: -767439.562500\n",
            "Train Epoch: 1 [35840/51000 (70%)]\tLoss: -785351.312500\n",
            "Train Epoch: 1 [37120/51000 (73%)]\tLoss: -802720.500000\n",
            "Train Epoch: 1 [38400/51000 (75%)]\tLoss: -819815.062500\n",
            "Train Epoch: 1 [39680/51000 (78%)]\tLoss: -838312.562500\n",
            "Train Epoch: 1 [40960/51000 (80%)]\tLoss: -856178.125000\n",
            "Train Epoch: 1 [42240/51000 (83%)]\tLoss: -874375.875000\n",
            "Train Epoch: 1 [43520/51000 (85%)]\tLoss: -892831.500000\n",
            "Train Epoch: 1 [44800/51000 (88%)]\tLoss: -911392.000000\n",
            "Train Epoch: 1 [46080/51000 (90%)]\tLoss: -930181.062500\n",
            "Train Epoch: 1 [47360/51000 (93%)]\tLoss: -948881.687500\n",
            "Train Epoch: 1 [48640/51000 (95%)]\tLoss: -967749.625000\n",
            "Train Epoch: 1 [49920/51000 (98%)]\tLoss: -987772.750000\n",
            "Test set: Average loss: -990183.4569, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 2 [0/51000 (0%)]\tLoss: -1004387.250000\n",
            "Train Epoch: 2 [1280/51000 (3%)]\tLoss: -1018216.125000\n",
            "Train Epoch: 2 [2560/51000 (5%)]\tLoss: -1032339.000000\n",
            "Train Epoch: 2 [3840/51000 (8%)]\tLoss: -1046379.250000\n",
            "Train Epoch: 2 [5120/51000 (10%)]\tLoss: -1059607.250000\n",
            "Train Epoch: 2 [6400/51000 (13%)]\tLoss: -1072676.875000\n",
            "Train Epoch: 2 [7680/51000 (15%)]\tLoss: -1087132.750000\n",
            "Train Epoch: 2 [8960/51000 (18%)]\tLoss: -1100647.875000\n",
            "Train Epoch: 2 [10240/51000 (20%)]\tLoss: -1115752.500000\n",
            "Train Epoch: 2 [11520/51000 (23%)]\tLoss: -1128603.500000\n",
            "Train Epoch: 2 [12800/51000 (25%)]\tLoss: -1143560.000000\n",
            "Train Epoch: 2 [14080/51000 (28%)]\tLoss: -1157493.750000\n",
            "Train Epoch: 2 [15360/51000 (30%)]\tLoss: -1171357.500000\n",
            "Train Epoch: 2 [16640/51000 (33%)]\tLoss: -1185986.000000\n",
            "Train Epoch: 2 [17920/51000 (35%)]\tLoss: -1200142.000000\n",
            "Train Epoch: 2 [19200/51000 (38%)]\tLoss: -1215016.375000\n",
            "Train Epoch: 2 [20480/51000 (40%)]\tLoss: -1228970.375000\n",
            "Train Epoch: 2 [21760/51000 (43%)]\tLoss: -1243917.875000\n",
            "Train Epoch: 2 [23040/51000 (45%)]\tLoss: -1258017.125000\n",
            "Train Epoch: 2 [24320/51000 (48%)]\tLoss: -1272388.500000\n",
            "Train Epoch: 2 [25600/51000 (50%)]\tLoss: -1287627.750000\n",
            "Train Epoch: 2 [26880/51000 (53%)]\tLoss: -1301209.250000\n",
            "Train Epoch: 2 [28160/51000 (55%)]\tLoss: -1316985.750000\n",
            "Train Epoch: 2 [29440/51000 (58%)]\tLoss: -1331234.750000\n",
            "Train Epoch: 2 [30720/51000 (60%)]\tLoss: -1345532.875000\n",
            "Train Epoch: 2 [32000/51000 (63%)]\tLoss: -1361355.750000\n",
            "Train Epoch: 2 [33280/51000 (65%)]\tLoss: -1375974.375000\n",
            "Train Epoch: 2 [34560/51000 (68%)]\tLoss: -1390216.500000\n",
            "Train Epoch: 2 [35840/51000 (70%)]\tLoss: -1406295.500000\n",
            "Train Epoch: 2 [37120/51000 (73%)]\tLoss: -1421122.250000\n",
            "Train Epoch: 2 [38400/51000 (75%)]\tLoss: -1435916.000000\n",
            "Train Epoch: 2 [39680/51000 (78%)]\tLoss: -1451819.625000\n",
            "Train Epoch: 2 [40960/51000 (80%)]\tLoss: -1467052.250000\n",
            "Train Epoch: 2 [42240/51000 (83%)]\tLoss: -1482429.500000\n",
            "Train Epoch: 2 [43520/51000 (85%)]\tLoss: -1497660.125000\n",
            "Train Epoch: 2 [44800/51000 (88%)]\tLoss: -1513464.375000\n",
            "Train Epoch: 2 [46080/51000 (90%)]\tLoss: -1528584.875000\n",
            "Train Epoch: 2 [47360/51000 (93%)]\tLoss: -1544530.125000\n",
            "Train Epoch: 2 [48640/51000 (95%)]\tLoss: -1559450.125000\n",
            "Train Epoch: 2 [49920/51000 (98%)]\tLoss: -1576273.000000\n",
            "Test set: Average loss: -1608275.5080, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 3 [0/51000 (0%)]\tLoss: -1589132.000000\n",
            "Train Epoch: 3 [1280/51000 (3%)]\tLoss: -1600827.250000\n",
            "Train Epoch: 3 [2560/51000 (5%)]\tLoss: -1612272.250000\n",
            "Train Epoch: 3 [3840/51000 (8%)]\tLoss: -1623727.500000\n",
            "Train Epoch: 3 [5120/51000 (10%)]\tLoss: -1634352.750000\n",
            "Train Epoch: 3 [6400/51000 (13%)]\tLoss: -1645632.375000\n",
            "Train Epoch: 3 [7680/51000 (15%)]\tLoss: -1656903.375000\n",
            "Train Epoch: 3 [8960/51000 (18%)]\tLoss: -1667682.625000\n",
            "Train Epoch: 3 [10240/51000 (20%)]\tLoss: -1680507.625000\n",
            "Train Epoch: 3 [11520/51000 (23%)]\tLoss: -1689756.125000\n",
            "Train Epoch: 3 [12800/51000 (25%)]\tLoss: -1701779.750000\n",
            "Train Epoch: 3 [14080/51000 (28%)]\tLoss: -1713173.875000\n",
            "Train Epoch: 3 [15360/51000 (30%)]\tLoss: -1723921.500000\n",
            "Train Epoch: 3 [16640/51000 (33%)]\tLoss: -1735596.375000\n",
            "Train Epoch: 3 [17920/51000 (35%)]\tLoss: -1746604.125000\n",
            "Train Epoch: 3 [19200/51000 (38%)]\tLoss: -1758965.750000\n",
            "Train Epoch: 3 [20480/51000 (40%)]\tLoss: -1769399.500000\n",
            "Train Epoch: 3 [21760/51000 (43%)]\tLoss: -1781161.625000\n",
            "Train Epoch: 3 [23040/51000 (45%)]\tLoss: -1791888.375000\n",
            "Train Epoch: 3 [24320/51000 (48%)]\tLoss: -1802942.125000\n",
            "Train Epoch: 3 [25600/51000 (50%)]\tLoss: -1815018.500000\n",
            "Train Epoch: 3 [26880/51000 (53%)]\tLoss: -1824941.000000\n",
            "Train Epoch: 3 [28160/51000 (55%)]\tLoss: -1838302.500000\n",
            "Train Epoch: 3 [29440/51000 (58%)]\tLoss: -1848926.000000\n",
            "Train Epoch: 3 [30720/51000 (60%)]\tLoss: -1859884.250000\n",
            "Train Epoch: 3 [32000/51000 (63%)]\tLoss: -1872568.875000\n",
            "Train Epoch: 3 [33280/51000 (65%)]\tLoss: -1883411.500000\n",
            "Train Epoch: 3 [34560/51000 (68%)]\tLoss: -1893776.625000\n",
            "Train Epoch: 3 [35840/51000 (70%)]\tLoss: -1906814.250000\n",
            "Train Epoch: 3 [37120/51000 (73%)]\tLoss: -1918296.875000\n",
            "Train Epoch: 3 [38400/51000 (75%)]\tLoss: -1929331.875000\n",
            "Train Epoch: 3 [39680/51000 (78%)]\tLoss: -1942043.000000\n",
            "Train Epoch: 3 [40960/51000 (80%)]\tLoss: -1953137.000000\n",
            "Train Epoch: 3 [42240/51000 (83%)]\tLoss: -1964850.875000\n",
            "Train Epoch: 3 [43520/51000 (85%)]\tLoss: -1976148.125000\n",
            "Train Epoch: 3 [44800/51000 (88%)]\tLoss: -1987915.500000\n",
            "Train Epoch: 3 [46080/51000 (90%)]\tLoss: -1999438.250000\n",
            "Train Epoch: 3 [47360/51000 (93%)]\tLoss: -2011629.625000\n",
            "Train Epoch: 3 [48640/51000 (95%)]\tLoss: -2022678.500000\n",
            "Train Epoch: 3 [49920/51000 (98%)]\tLoss: -2035800.125000\n",
            "Test set: Average loss: -2107640.9120, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 4 [0/51000 (0%)]\tLoss: -2044422.500000\n",
            "Train Epoch: 4 [1280/51000 (3%)]\tLoss: -2053293.875000\n",
            "Train Epoch: 4 [2560/51000 (5%)]\tLoss: -2062177.250000\n",
            "Train Epoch: 4 [3840/51000 (8%)]\tLoss: -2070590.250000\n",
            "Train Epoch: 4 [5120/51000 (10%)]\tLoss: -2078091.000000\n",
            "Train Epoch: 4 [6400/51000 (13%)]\tLoss: -2086481.750000\n",
            "Train Epoch: 4 [7680/51000 (15%)]\tLoss: -2094908.250000\n",
            "Train Epoch: 4 [8960/51000 (18%)]\tLoss: -2102409.250000\n",
            "Train Epoch: 4 [10240/51000 (20%)]\tLoss: -2112721.750000\n",
            "Train Epoch: 4 [11520/51000 (23%)]\tLoss: -2118723.750000\n",
            "Train Epoch: 4 [12800/51000 (25%)]\tLoss: -2127877.500000\n",
            "Train Epoch: 4 [14080/51000 (28%)]\tLoss: -2136705.000000\n",
            "Train Epoch: 4 [15360/51000 (30%)]\tLoss: -2143591.000000\n",
            "Train Epoch: 4 [16640/51000 (33%)]\tLoss: -2152312.000000\n",
            "Train Epoch: 4 [17920/51000 (35%)]\tLoss: -2159915.500000\n",
            "Train Epoch: 4 [19200/51000 (38%)]\tLoss: -2170955.250000\n",
            "Train Epoch: 4 [20480/51000 (40%)]\tLoss: -2177257.000000\n",
            "Train Epoch: 4 [21760/51000 (43%)]\tLoss: -2186693.250000\n",
            "Train Epoch: 4 [23040/51000 (45%)]\tLoss: -2194350.000000\n",
            "Train Epoch: 4 [24320/51000 (48%)]\tLoss: -2202183.500000\n",
            "Train Epoch: 4 [25600/51000 (50%)]\tLoss: -2211801.500000\n",
            "Train Epoch: 4 [26880/51000 (53%)]\tLoss: -2217921.000000\n",
            "Train Epoch: 4 [28160/51000 (55%)]\tLoss: -2228284.500000\n",
            "Train Epoch: 4 [29440/51000 (58%)]\tLoss: -2236235.250000\n",
            "Train Epoch: 4 [30720/51000 (60%)]\tLoss: -2244391.250000\n",
            "Train Epoch: 4 [32000/51000 (63%)]\tLoss: -2253560.000000\n",
            "Train Epoch: 4 [33280/51000 (65%)]\tLoss: -2261265.750000\n",
            "Train Epoch: 4 [34560/51000 (68%)]\tLoss: -2268669.500000\n",
            "Train Epoch: 4 [35840/51000 (70%)]\tLoss: -2278466.500000\n",
            "Train Epoch: 4 [37120/51000 (73%)]\tLoss: -2286378.750000\n",
            "Train Epoch: 4 [38400/51000 (75%)]\tLoss: -2294312.500000\n",
            "Train Epoch: 4 [39680/51000 (78%)]\tLoss: -2304213.000000\n",
            "Train Epoch: 4 [40960/51000 (80%)]\tLoss: -2311617.250000\n",
            "Train Epoch: 4 [42240/51000 (83%)]\tLoss: -2320291.250000\n",
            "Train Epoch: 4 [43520/51000 (85%)]\tLoss: -2328288.750000\n",
            "Train Epoch: 4 [44800/51000 (88%)]\tLoss: -2336779.500000\n",
            "Train Epoch: 4 [46080/51000 (90%)]\tLoss: -2345053.250000\n",
            "Train Epoch: 4 [47360/51000 (93%)]\tLoss: -2353990.250000\n",
            "Train Epoch: 4 [48640/51000 (95%)]\tLoss: -2361714.250000\n",
            "Train Epoch: 4 [49920/51000 (98%)]\tLoss: -2371544.750000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-04-10 02:29:42,342]\u001b[0m Trial 1 finished with value: -2367085.104 and parameters: {'lr': 0.022381284365837674, 'optimizer_name': 'Adam'}. Best is trial 1 with value: -2367085.104.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test set: Average loss: -2367085.1040, Accuracy: 895/9000 (10%)\n",
            "\n",
            "\n",
            "********************************\n",
            "\n",
            "Suggested hyperparameters: \n",
            "{'lr': 0.001937336250352337, 'optimizer_name': 'Adam'}\n",
            "Train Epoch: 0 [0/51000 (0%)]\tLoss: 0.084478\n",
            "Train Epoch: 0 [1280/51000 (3%)]\tLoss: -12.527074\n",
            "Train Epoch: 0 [2560/51000 (5%)]\tLoss: -29.100332\n",
            "Train Epoch: 0 [3840/51000 (8%)]\tLoss: -48.154404\n",
            "Train Epoch: 0 [5120/51000 (10%)]\tLoss: -68.911339\n",
            "Train Epoch: 0 [6400/51000 (13%)]\tLoss: -92.642693\n",
            "Train Epoch: 0 [7680/51000 (15%)]\tLoss: -117.458221\n",
            "Train Epoch: 0 [8960/51000 (18%)]\tLoss: -144.820496\n",
            "Train Epoch: 0 [10240/51000 (20%)]\tLoss: -174.894196\n",
            "Train Epoch: 0 [11520/51000 (23%)]\tLoss: -207.203400\n",
            "Train Epoch: 0 [12800/51000 (25%)]\tLoss: -241.282028\n",
            "Train Epoch: 0 [14080/51000 (28%)]\tLoss: -278.831421\n",
            "Train Epoch: 0 [15360/51000 (30%)]\tLoss: -318.036469\n",
            "Train Epoch: 0 [16640/51000 (33%)]\tLoss: -360.982300\n",
            "Train Epoch: 0 [17920/51000 (35%)]\tLoss: -406.845367\n",
            "Train Epoch: 0 [19200/51000 (38%)]\tLoss: -453.617432\n",
            "Train Epoch: 0 [20480/51000 (40%)]\tLoss: -505.899780\n",
            "Train Epoch: 0 [21760/51000 (43%)]\tLoss: -559.134827\n",
            "Train Epoch: 0 [23040/51000 (45%)]\tLoss: -615.109619\n",
            "Train Epoch: 0 [24320/51000 (48%)]\tLoss: -675.121399\n",
            "Train Epoch: 0 [25600/51000 (50%)]\tLoss: -736.781616\n",
            "Train Epoch: 0 [26880/51000 (53%)]\tLoss: -801.445007\n",
            "Train Epoch: 0 [28160/51000 (55%)]\tLoss: -869.045593\n",
            "Train Epoch: 0 [29440/51000 (58%)]\tLoss: -943.491394\n",
            "Train Epoch: 0 [30720/51000 (60%)]\tLoss: -1031.960449\n",
            "Train Epoch: 0 [32000/51000 (63%)]\tLoss: -1132.586670\n",
            "Train Epoch: 0 [33280/51000 (65%)]\tLoss: -1227.648438\n",
            "Train Epoch: 0 [34560/51000 (68%)]\tLoss: -1324.847900\n",
            "Train Epoch: 0 [35840/51000 (70%)]\tLoss: -1427.990234\n",
            "Train Epoch: 0 [37120/51000 (73%)]\tLoss: -1528.395508\n",
            "Train Epoch: 0 [38400/51000 (75%)]\tLoss: -1632.096924\n",
            "Train Epoch: 0 [39680/51000 (78%)]\tLoss: -1743.065430\n",
            "Train Epoch: 0 [40960/51000 (80%)]\tLoss: -1854.221802\n",
            "Train Epoch: 0 [42240/51000 (83%)]\tLoss: -1967.489746\n",
            "Train Epoch: 0 [43520/51000 (85%)]\tLoss: -2091.171387\n",
            "Train Epoch: 0 [44800/51000 (88%)]\tLoss: -2210.322998\n",
            "Train Epoch: 0 [46080/51000 (90%)]\tLoss: -2337.898926\n",
            "Train Epoch: 0 [47360/51000 (93%)]\tLoss: -2467.675537\n",
            "Train Epoch: 0 [48640/51000 (95%)]\tLoss: -2599.770020\n",
            "Train Epoch: 0 [49920/51000 (98%)]\tLoss: -2741.627441\n",
            "Test set: Average loss: -2855.5322, Accuracy: 948/9000 (11%)\n",
            "\n",
            "Train Epoch: 1 [0/51000 (0%)]\tLoss: -2865.730225\n",
            "Train Epoch: 1 [1280/51000 (3%)]\tLoss: -2966.739014\n",
            "Train Epoch: 1 [2560/51000 (5%)]\tLoss: -3066.590088\n",
            "Train Epoch: 1 [3840/51000 (8%)]\tLoss: -3173.870117\n",
            "Train Epoch: 1 [5120/51000 (10%)]\tLoss: -3275.430908\n",
            "Train Epoch: 1 [6400/51000 (13%)]\tLoss: -3380.552734\n",
            "Train Epoch: 1 [7680/51000 (15%)]\tLoss: -3485.865234\n",
            "Train Epoch: 1 [8960/51000 (18%)]\tLoss: -3584.016602\n",
            "Train Epoch: 1 [10240/51000 (20%)]\tLoss: -3701.448975\n",
            "Train Epoch: 1 [11520/51000 (23%)]\tLoss: -3804.723145\n",
            "Train Epoch: 1 [12800/51000 (25%)]\tLoss: -3918.576172\n",
            "Train Epoch: 1 [14080/51000 (28%)]\tLoss: -4028.541748\n",
            "Train Epoch: 1 [15360/51000 (30%)]\tLoss: -4138.563965\n",
            "Train Epoch: 1 [16640/51000 (33%)]\tLoss: -4253.545410\n",
            "Train Epoch: 1 [17920/51000 (35%)]\tLoss: -4369.781738\n",
            "Train Epoch: 1 [19200/51000 (38%)]\tLoss: -4484.179688\n",
            "Train Epoch: 1 [20480/51000 (40%)]\tLoss: -4602.627930\n",
            "Train Epoch: 1 [21760/51000 (43%)]\tLoss: -4718.896973\n",
            "Train Epoch: 1 [23040/51000 (45%)]\tLoss: -4836.658203\n",
            "Train Epoch: 1 [24320/51000 (48%)]\tLoss: -4957.484375\n",
            "Train Epoch: 1 [25600/51000 (50%)]\tLoss: -5083.941406\n",
            "Train Epoch: 1 [26880/51000 (53%)]\tLoss: -5203.072266\n",
            "Train Epoch: 1 [28160/51000 (55%)]\tLoss: -5327.775879\n",
            "Train Epoch: 1 [29440/51000 (58%)]\tLoss: -5444.442383\n",
            "Train Epoch: 1 [30720/51000 (60%)]\tLoss: -5575.759277\n",
            "Train Epoch: 1 [32000/51000 (63%)]\tLoss: -5705.867188\n",
            "Train Epoch: 1 [33280/51000 (65%)]\tLoss: -5833.591797\n",
            "Train Epoch: 1 [34560/51000 (68%)]\tLoss: -5968.118164\n",
            "Train Epoch: 1 [35840/51000 (70%)]\tLoss: -6106.215820\n",
            "Train Epoch: 1 [37120/51000 (73%)]\tLoss: -6235.963379\n",
            "Train Epoch: 1 [38400/51000 (75%)]\tLoss: -6365.903809\n",
            "Train Epoch: 1 [39680/51000 (78%)]\tLoss: -6507.149902\n",
            "Train Epoch: 1 [40960/51000 (80%)]\tLoss: -6638.071777\n",
            "Train Epoch: 1 [42240/51000 (83%)]\tLoss: -6774.026367\n",
            "Train Epoch: 1 [43520/51000 (85%)]\tLoss: -6919.226074\n",
            "Train Epoch: 1 [44800/51000 (88%)]\tLoss: -7056.822754\n",
            "Train Epoch: 1 [46080/51000 (90%)]\tLoss: -7202.097656\n",
            "Train Epoch: 1 [47360/51000 (93%)]\tLoss: -7339.854004\n",
            "Train Epoch: 1 [48640/51000 (95%)]\tLoss: -7483.268555\n",
            "Train Epoch: 1 [49920/51000 (98%)]\tLoss: -7637.917480\n",
            "Test set: Average loss: -8039.2698, Accuracy: 899/9000 (10%)\n",
            "\n",
            "Train Epoch: 2 [0/51000 (0%)]\tLoss: -7762.779785\n",
            "Train Epoch: 2 [1280/51000 (3%)]\tLoss: -7869.008301\n",
            "Train Epoch: 2 [2560/51000 (5%)]\tLoss: -7965.919434\n",
            "Train Epoch: 2 [3840/51000 (8%)]\tLoss: -8080.153320\n",
            "Train Epoch: 2 [5120/51000 (10%)]\tLoss: -8180.790039\n",
            "Train Epoch: 2 [6400/51000 (13%)]\tLoss: -8283.183594\n",
            "Train Epoch: 2 [7680/51000 (15%)]\tLoss: -8390.757812\n",
            "Train Epoch: 2 [8960/51000 (18%)]\tLoss: -8488.011719\n",
            "Train Epoch: 2 [10240/51000 (20%)]\tLoss: -8603.827148\n",
            "Train Epoch: 2 [11520/51000 (23%)]\tLoss: -8700.580078\n",
            "Train Epoch: 2 [12800/51000 (25%)]\tLoss: -8817.605469\n",
            "Train Epoch: 2 [14080/51000 (28%)]\tLoss: -8921.506836\n",
            "Train Epoch: 2 [15360/51000 (30%)]\tLoss: -9022.120117\n",
            "Train Epoch: 2 [16640/51000 (33%)]\tLoss: -9139.214844\n",
            "Train Epoch: 2 [17920/51000 (35%)]\tLoss: -9248.918945\n",
            "Train Epoch: 2 [19200/51000 (38%)]\tLoss: -9364.782227\n",
            "Train Epoch: 2 [20480/51000 (40%)]\tLoss: -9467.196289\n",
            "Train Epoch: 2 [21760/51000 (43%)]\tLoss: -9579.122070\n",
            "Train Epoch: 2 [23040/51000 (45%)]\tLoss: -9685.289062\n",
            "Train Epoch: 2 [24320/51000 (48%)]\tLoss: -9797.678711\n",
            "Train Epoch: 2 [25600/51000 (50%)]\tLoss: -9909.556641\n",
            "Train Epoch: 2 [26880/51000 (53%)]\tLoss: -10006.395508\n",
            "Train Epoch: 2 [28160/51000 (55%)]\tLoss: -10131.098633\n",
            "Train Epoch: 2 [29440/51000 (58%)]\tLoss: -10240.996094\n",
            "Train Epoch: 2 [30720/51000 (60%)]\tLoss: -10360.935547\n",
            "Train Epoch: 2 [32000/51000 (63%)]\tLoss: -10470.648438\n",
            "Train Epoch: 2 [33280/51000 (65%)]\tLoss: -10577.833008\n",
            "Train Epoch: 2 [34560/51000 (68%)]\tLoss: -10692.440430\n",
            "Train Epoch: 2 [35840/51000 (70%)]\tLoss: -10815.908203\n",
            "Train Epoch: 2 [37120/51000 (73%)]\tLoss: -10927.400391\n",
            "Train Epoch: 2 [38400/51000 (75%)]\tLoss: -11035.338867\n",
            "Train Epoch: 2 [39680/51000 (78%)]\tLoss: -11160.736328\n",
            "Train Epoch: 2 [40960/51000 (80%)]\tLoss: -11272.474609\n",
            "Train Epoch: 2 [42240/51000 (83%)]\tLoss: -11384.385742\n",
            "Train Epoch: 2 [43520/51000 (85%)]\tLoss: -11503.572266\n",
            "Train Epoch: 2 [44800/51000 (88%)]\tLoss: -11620.884766\n",
            "Train Epoch: 2 [46080/51000 (90%)]\tLoss: -11741.265625\n",
            "Train Epoch: 2 [47360/51000 (93%)]\tLoss: -11857.580078\n",
            "Train Epoch: 2 [48640/51000 (95%)]\tLoss: -11967.472656\n",
            "Train Epoch: 2 [49920/51000 (98%)]\tLoss: -12099.685547\n",
            "Test set: Average loss: -12139.7291, Accuracy: 914/9000 (10%)\n",
            "\n",
            "Train Epoch: 3 [0/51000 (0%)]\tLoss: -12198.679688\n",
            "Train Epoch: 3 [1280/51000 (3%)]\tLoss: -12279.153320\n",
            "Train Epoch: 3 [2560/51000 (5%)]\tLoss: -12360.272461\n",
            "Train Epoch: 3 [3840/51000 (8%)]\tLoss: -12461.277344\n",
            "Train Epoch: 3 [5120/51000 (10%)]\tLoss: -12538.165039\n",
            "Train Epoch: 3 [6400/51000 (13%)]\tLoss: -12623.369141\n",
            "Train Epoch: 3 [7680/51000 (15%)]\tLoss: -12710.714844\n",
            "Train Epoch: 3 [8960/51000 (18%)]\tLoss: -12781.742188\n",
            "Train Epoch: 3 [10240/51000 (20%)]\tLoss: -12885.288086\n",
            "Train Epoch: 3 [11520/51000 (23%)]\tLoss: -12954.858398\n",
            "Train Epoch: 3 [12800/51000 (25%)]\tLoss: -13051.903320\n",
            "Train Epoch: 3 [14080/51000 (28%)]\tLoss: -13138.370117\n",
            "Train Epoch: 3 [15360/51000 (30%)]\tLoss: -13210.111328\n",
            "Train Epoch: 3 [16640/51000 (33%)]\tLoss: -13302.605469\n",
            "Train Epoch: 3 [17920/51000 (35%)]\tLoss: -13393.473633\n",
            "Train Epoch: 3 [19200/51000 (38%)]\tLoss: -13483.940430\n",
            "Train Epoch: 3 [20480/51000 (40%)]\tLoss: -13564.185547\n",
            "Train Epoch: 3 [21760/51000 (43%)]\tLoss: -13647.848633\n",
            "Train Epoch: 3 [23040/51000 (45%)]\tLoss: -13727.946289\n",
            "Train Epoch: 3 [24320/51000 (48%)]\tLoss: -13817.655273\n",
            "Train Epoch: 3 [25600/51000 (50%)]\tLoss: -13909.906250\n",
            "Train Epoch: 3 [26880/51000 (53%)]\tLoss: -13979.997070\n",
            "Train Epoch: 3 [28160/51000 (55%)]\tLoss: -14079.366211\n",
            "Train Epoch: 3 [29440/51000 (58%)]\tLoss: -14160.971680\n",
            "Train Epoch: 3 [30720/51000 (60%)]\tLoss: -14258.363281\n",
            "Train Epoch: 3 [32000/51000 (63%)]\tLoss: -14341.695312\n",
            "Train Epoch: 3 [33280/51000 (65%)]\tLoss: -14415.294922\n",
            "Train Epoch: 3 [34560/51000 (68%)]\tLoss: -14509.095703\n",
            "Train Epoch: 3 [35840/51000 (70%)]\tLoss: -14607.312500\n",
            "Train Epoch: 3 [37120/51000 (73%)]\tLoss: -14694.149414\n",
            "Train Epoch: 3 [38400/51000 (75%)]\tLoss: -14770.656250\n",
            "Train Epoch: 3 [39680/51000 (78%)]\tLoss: -14871.058594\n",
            "Train Epoch: 3 [40960/51000 (80%)]\tLoss: -14952.900391\n",
            "Train Epoch: 3 [42240/51000 (83%)]\tLoss: -15035.643555\n",
            "Train Epoch: 3 [43520/51000 (85%)]\tLoss: -15130.414062\n",
            "Train Epoch: 3 [44800/51000 (88%)]\tLoss: -15215.624023\n",
            "Train Epoch: 3 [46080/51000 (90%)]\tLoss: -15300.400391\n",
            "Train Epoch: 3 [47360/51000 (93%)]\tLoss: -15394.188477\n",
            "Train Epoch: 3 [48640/51000 (95%)]\tLoss: -15475.796875\n",
            "Train Epoch: 3 [49920/51000 (98%)]\tLoss: -15579.578125\n",
            "Test set: Average loss: -15670.9152, Accuracy: 917/9000 (10%)\n",
            "\n",
            "Train Epoch: 4 [0/51000 (0%)]\tLoss: -15649.699219\n",
            "Train Epoch: 4 [1280/51000 (3%)]\tLoss: -15708.584961\n",
            "Train Epoch: 4 [2560/51000 (5%)]\tLoss: -15765.678711\n",
            "Train Epoch: 4 [3840/51000 (8%)]\tLoss: -15841.301758\n",
            "Train Epoch: 4 [5120/51000 (10%)]\tLoss: -15894.043945\n",
            "Train Epoch: 4 [6400/51000 (13%)]\tLoss: -15958.585938\n",
            "Train Epoch: 4 [7680/51000 (15%)]\tLoss: -16023.247070\n",
            "Train Epoch: 4 [8960/51000 (18%)]\tLoss: -16070.826172\n",
            "Train Epoch: 4 [10240/51000 (20%)]\tLoss: -16150.040039\n",
            "Train Epoch: 4 [11520/51000 (23%)]\tLoss: -16199.927734\n",
            "Train Epoch: 4 [12800/51000 (25%)]\tLoss: -16266.745117\n",
            "Train Epoch: 4 [14080/51000 (28%)]\tLoss: -16339.576172\n",
            "Train Epoch: 4 [15360/51000 (30%)]\tLoss: -16392.863281\n",
            "Train Epoch: 4 [16640/51000 (33%)]\tLoss: -16461.632812\n",
            "Train Epoch: 4 [17920/51000 (35%)]\tLoss: -16526.912109\n",
            "Train Epoch: 4 [19200/51000 (38%)]\tLoss: -16593.683594\n",
            "Train Epoch: 4 [20480/51000 (40%)]\tLoss: -16648.775391\n",
            "Train Epoch: 4 [21760/51000 (43%)]\tLoss: -16712.439453\n",
            "Train Epoch: 4 [23040/51000 (45%)]\tLoss: -16768.062500\n",
            "Train Epoch: 4 [24320/51000 (48%)]\tLoss: -16833.439453\n",
            "Train Epoch: 4 [25600/51000 (50%)]\tLoss: -16908.476562\n",
            "Train Epoch: 4 [26880/51000 (53%)]\tLoss: -16955.406250\n",
            "Train Epoch: 4 [28160/51000 (55%)]\tLoss: -17035.345703\n",
            "Train Epoch: 4 [29440/51000 (58%)]\tLoss: -17095.535156\n",
            "Train Epoch: 4 [30720/51000 (60%)]\tLoss: -17161.457031\n",
            "Train Epoch: 4 [32000/51000 (63%)]\tLoss: -17227.324219\n",
            "Train Epoch: 4 [33280/51000 (65%)]\tLoss: -17279.601562\n",
            "Train Epoch: 4 [34560/51000 (68%)]\tLoss: -17345.658203\n",
            "Train Epoch: 4 [35840/51000 (70%)]\tLoss: -17421.486328\n",
            "Train Epoch: 4 [37120/51000 (73%)]\tLoss: -17478.724609\n",
            "Train Epoch: 4 [38400/51000 (75%)]\tLoss: -17534.810547\n",
            "Train Epoch: 4 [39680/51000 (78%)]\tLoss: -17611.097656\n",
            "Train Epoch: 4 [40960/51000 (80%)]\tLoss: -17672.136719\n",
            "Train Epoch: 4 [42240/51000 (83%)]\tLoss: -17727.642578\n",
            "Train Epoch: 4 [43520/51000 (85%)]\tLoss: -17798.660156\n",
            "Train Epoch: 4 [44800/51000 (88%)]\tLoss: -17857.214844\n",
            "Train Epoch: 4 [46080/51000 (90%)]\tLoss: -17921.683594\n",
            "Train Epoch: 4 [47360/51000 (93%)]\tLoss: -17986.304688\n",
            "Train Epoch: 4 [48640/51000 (95%)]\tLoss: -18042.984375\n",
            "Train Epoch: 4 [49920/51000 (98%)]\tLoss: -18123.603516\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-04-10 02:43:55,646]\u001b[0m Trial 2 finished with value: -18087.707361111112 and parameters: {'lr': 0.001937336250352337, 'optimizer_name': 'Adam'}. Best is trial 1 with value: -2367085.104.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test set: Average loss: -18087.7074, Accuracy: 898/9000 (10%)\n",
            "\n",
            "\n",
            "********************************\n",
            "\n",
            "Suggested hyperparameters: \n",
            "{'lr': 0.00010643990215300088, 'optimizer_name': 'RMSprop'}\n",
            "Train Epoch: 0 [0/51000 (0%)]\tLoss: -0.038683\n",
            "Train Epoch: 0 [1280/51000 (3%)]\tLoss: -3.832893\n",
            "Train Epoch: 0 [2560/51000 (5%)]\tLoss: -7.857562\n",
            "Train Epoch: 0 [3840/51000 (8%)]\tLoss: -9.771683\n",
            "Train Epoch: 0 [5120/51000 (10%)]\tLoss: -11.484155\n",
            "Train Epoch: 0 [6400/51000 (13%)]\tLoss: -13.741072\n",
            "Train Epoch: 0 [7680/51000 (15%)]\tLoss: -14.211802\n",
            "Train Epoch: 0 [8960/51000 (18%)]\tLoss: -16.770096\n",
            "Train Epoch: 0 [10240/51000 (20%)]\tLoss: -18.993002\n",
            "Train Epoch: 0 [11520/51000 (23%)]\tLoss: -19.435329\n",
            "Train Epoch: 0 [12800/51000 (25%)]\tLoss: -21.023743\n",
            "Train Epoch: 0 [14080/51000 (28%)]\tLoss: -22.513855\n",
            "Train Epoch: 0 [15360/51000 (30%)]\tLoss: -24.788149\n",
            "Train Epoch: 0 [16640/51000 (33%)]\tLoss: -25.546314\n",
            "Train Epoch: 0 [17920/51000 (35%)]\tLoss: -25.184944\n",
            "Train Epoch: 0 [19200/51000 (38%)]\tLoss: -27.128344\n",
            "Train Epoch: 0 [20480/51000 (40%)]\tLoss: -28.560072\n",
            "Train Epoch: 0 [21760/51000 (43%)]\tLoss: -29.756819\n",
            "Train Epoch: 0 [23040/51000 (45%)]\tLoss: -31.296196\n",
            "Train Epoch: 0 [24320/51000 (48%)]\tLoss: -32.658779\n",
            "Train Epoch: 0 [25600/51000 (50%)]\tLoss: -32.279572\n",
            "Train Epoch: 0 [26880/51000 (53%)]\tLoss: -33.890469\n",
            "Train Epoch: 0 [28160/51000 (55%)]\tLoss: -36.179432\n",
            "Train Epoch: 0 [29440/51000 (58%)]\tLoss: -35.718708\n",
            "Train Epoch: 0 [30720/51000 (60%)]\tLoss: -37.016506\n",
            "Train Epoch: 0 [32000/51000 (63%)]\tLoss: -37.931206\n",
            "Train Epoch: 0 [33280/51000 (65%)]\tLoss: -39.330559\n",
            "Train Epoch: 0 [34560/51000 (68%)]\tLoss: -40.599865\n",
            "Train Epoch: 0 [35840/51000 (70%)]\tLoss: -40.460609\n",
            "Train Epoch: 0 [37120/51000 (73%)]\tLoss: -43.723518\n",
            "Train Epoch: 0 [38400/51000 (75%)]\tLoss: -43.892952\n",
            "Train Epoch: 0 [39680/51000 (78%)]\tLoss: -44.501129\n",
            "Train Epoch: 0 [40960/51000 (80%)]\tLoss: -46.139885\n",
            "Train Epoch: 0 [42240/51000 (83%)]\tLoss: -46.377689\n",
            "Train Epoch: 0 [43520/51000 (85%)]\tLoss: -47.488785\n",
            "Train Epoch: 0 [44800/51000 (88%)]\tLoss: -48.861343\n",
            "Train Epoch: 0 [46080/51000 (90%)]\tLoss: -50.145302\n",
            "Train Epoch: 0 [47360/51000 (93%)]\tLoss: -51.493603\n",
            "Train Epoch: 0 [48640/51000 (95%)]\tLoss: -52.124432\n",
            "Train Epoch: 0 [49920/51000 (98%)]\tLoss: -53.203358\n",
            "Test set: Average loss: -51.5477, Accuracy: 7396/9000 (82%)\n",
            "\n",
            "Train Epoch: 1 [0/51000 (0%)]\tLoss: -53.040092\n",
            "Train Epoch: 1 [1280/51000 (3%)]\tLoss: -54.678848\n",
            "Train Epoch: 1 [2560/51000 (5%)]\tLoss: -56.198723\n",
            "Train Epoch: 1 [3840/51000 (8%)]\tLoss: -56.092644\n",
            "Train Epoch: 1 [5120/51000 (10%)]\tLoss: -56.684456\n",
            "Train Epoch: 1 [6400/51000 (13%)]\tLoss: -58.030479\n",
            "Train Epoch: 1 [7680/51000 (15%)]\tLoss: -57.601727\n",
            "Train Epoch: 1 [8960/51000 (18%)]\tLoss: -58.845913\n",
            "Train Epoch: 1 [10240/51000 (20%)]\tLoss: -60.474903\n",
            "Train Epoch: 1 [11520/51000 (23%)]\tLoss: -59.836277\n",
            "Train Epoch: 1 [12800/51000 (25%)]\tLoss: -61.058067\n",
            "Train Epoch: 1 [14080/51000 (28%)]\tLoss: -62.334953\n",
            "Train Epoch: 1 [15360/51000 (30%)]\tLoss: -63.674561\n",
            "Train Epoch: 1 [16640/51000 (33%)]\tLoss: -64.223495\n",
            "Train Epoch: 1 [17920/51000 (35%)]\tLoss: -63.512871\n",
            "Train Epoch: 1 [19200/51000 (38%)]\tLoss: -65.069061\n",
            "Train Epoch: 1 [20480/51000 (40%)]\tLoss: -66.140404\n",
            "Train Epoch: 1 [21760/51000 (43%)]\tLoss: -66.788628\n",
            "Train Epoch: 1 [23040/51000 (45%)]\tLoss: -67.552505\n",
            "Train Epoch: 1 [24320/51000 (48%)]\tLoss: -68.362823\n",
            "Train Epoch: 1 [25600/51000 (50%)]\tLoss: -67.887077\n",
            "Train Epoch: 1 [26880/51000 (53%)]\tLoss: -69.540512\n",
            "Train Epoch: 1 [28160/51000 (55%)]\tLoss: -71.322647\n",
            "Train Epoch: 1 [29440/51000 (58%)]\tLoss: -70.632645\n",
            "Train Epoch: 1 [30720/51000 (60%)]\tLoss: -71.667114\n",
            "Train Epoch: 1 [32000/51000 (63%)]\tLoss: -72.007645\n",
            "Train Epoch: 1 [33280/51000 (65%)]\tLoss: -73.547928\n",
            "Train Epoch: 1 [34560/51000 (68%)]\tLoss: -74.119682\n",
            "Train Epoch: 1 [35840/51000 (70%)]\tLoss: -74.456017\n",
            "Train Epoch: 1 [37120/51000 (73%)]\tLoss: -77.007660\n",
            "Train Epoch: 1 [38400/51000 (75%)]\tLoss: -76.690033\n",
            "Train Epoch: 1 [39680/51000 (78%)]\tLoss: -77.017227\n",
            "Train Epoch: 1 [40960/51000 (80%)]\tLoss: -78.335587\n",
            "Train Epoch: 1 [42240/51000 (83%)]\tLoss: -78.007393\n",
            "Train Epoch: 1 [43520/51000 (85%)]\tLoss: -78.957855\n",
            "Train Epoch: 1 [44800/51000 (88%)]\tLoss: -80.286476\n",
            "Train Epoch: 1 [46080/51000 (90%)]\tLoss: -81.171875\n",
            "Train Epoch: 1 [47360/51000 (93%)]\tLoss: -82.205055\n",
            "Train Epoch: 1 [48640/51000 (95%)]\tLoss: -82.658417\n",
            "Train Epoch: 1 [49920/51000 (98%)]\tLoss: -83.249641\n",
            "Test set: Average loss: -82.4674, Accuracy: 8223/9000 (91%)\n",
            "\n",
            "Train Epoch: 2 [0/51000 (0%)]\tLoss: -82.937187\n",
            "Train Epoch: 2 [1280/51000 (3%)]\tLoss: -84.510399\n",
            "Train Epoch: 2 [2560/51000 (5%)]\tLoss: -85.508049\n",
            "Train Epoch: 2 [3840/51000 (8%)]\tLoss: -85.131554\n",
            "Train Epoch: 2 [5120/51000 (10%)]\tLoss: -85.969803\n",
            "Train Epoch: 2 [6400/51000 (13%)]\tLoss: -86.900208\n",
            "Train Epoch: 2 [7680/51000 (15%)]\tLoss: -86.433693\n",
            "Train Epoch: 2 [8960/51000 (18%)]\tLoss: -87.442596\n",
            "Train Epoch: 2 [10240/51000 (20%)]\tLoss: -88.652725\n",
            "Train Epoch: 2 [11520/51000 (23%)]\tLoss: -87.999466\n",
            "Train Epoch: 2 [12800/51000 (25%)]\tLoss: -88.925499\n",
            "Train Epoch: 2 [14080/51000 (28%)]\tLoss: -90.126274\n",
            "Train Epoch: 2 [15360/51000 (30%)]\tLoss: -91.202179\n",
            "Train Epoch: 2 [16640/51000 (33%)]\tLoss: -91.563156\n",
            "Train Epoch: 2 [17920/51000 (35%)]\tLoss: -90.554146\n",
            "Train Epoch: 2 [19200/51000 (38%)]\tLoss: -92.014832\n",
            "Train Epoch: 2 [20480/51000 (40%)]\tLoss: -92.951820\n",
            "Train Epoch: 2 [21760/51000 (43%)]\tLoss: -93.454514\n",
            "Train Epoch: 2 [23040/51000 (45%)]\tLoss: -94.099350\n",
            "Train Epoch: 2 [24320/51000 (48%)]\tLoss: -94.389755\n",
            "Train Epoch: 2 [25600/51000 (50%)]\tLoss: -93.746323\n",
            "Train Epoch: 2 [26880/51000 (53%)]\tLoss: -95.358170\n",
            "Train Epoch: 2 [28160/51000 (55%)]\tLoss: -97.016350\n",
            "Train Epoch: 2 [29440/51000 (58%)]\tLoss: -96.055420\n",
            "Train Epoch: 2 [30720/51000 (60%)]\tLoss: -97.014435\n",
            "Train Epoch: 2 [32000/51000 (63%)]\tLoss: -96.985558\n",
            "Train Epoch: 2 [33280/51000 (65%)]\tLoss: -98.381653\n",
            "Train Epoch: 2 [34560/51000 (68%)]\tLoss: -98.861664\n",
            "Train Epoch: 2 [35840/51000 (70%)]\tLoss: -98.887077\n",
            "Train Epoch: 2 [37120/51000 (73%)]\tLoss: -101.368431\n",
            "Train Epoch: 2 [38400/51000 (75%)]\tLoss: -100.785912\n",
            "Train Epoch: 2 [39680/51000 (78%)]\tLoss: -100.865639\n",
            "Train Epoch: 2 [40960/51000 (80%)]\tLoss: -101.922195\n",
            "Train Epoch: 2 [42240/51000 (83%)]\tLoss: -101.435440\n",
            "Train Epoch: 2 [43520/51000 (85%)]\tLoss: -102.144707\n",
            "Train Epoch: 2 [44800/51000 (88%)]\tLoss: -103.461479\n",
            "Train Epoch: 2 [46080/51000 (90%)]\tLoss: -104.102654\n",
            "Train Epoch: 2 [47360/51000 (93%)]\tLoss: -104.855965\n",
            "Train Epoch: 2 [48640/51000 (95%)]\tLoss: -105.063095\n",
            "Train Epoch: 2 [49920/51000 (98%)]\tLoss: -105.713562\n",
            "Test set: Average loss: -106.5590, Accuracy: 8516/9000 (95%)\n",
            "\n",
            "Train Epoch: 3 [0/51000 (0%)]\tLoss: -105.060234\n",
            "Train Epoch: 3 [1280/51000 (3%)]\tLoss: -106.471863\n",
            "Train Epoch: 3 [2560/51000 (5%)]\tLoss: -107.322433\n",
            "Train Epoch: 3 [3840/51000 (8%)]\tLoss: -106.671188\n",
            "Train Epoch: 3 [5120/51000 (10%)]\tLoss: -107.481247\n",
            "Train Epoch: 3 [6400/51000 (13%)]\tLoss: -108.137321\n",
            "Train Epoch: 3 [7680/51000 (15%)]\tLoss: -107.640358\n",
            "Train Epoch: 3 [8960/51000 (18%)]\tLoss: -108.470161\n",
            "Train Epoch: 3 [10240/51000 (20%)]\tLoss: -109.497581\n",
            "Train Epoch: 3 [11520/51000 (23%)]\tLoss: -108.854950\n",
            "Train Epoch: 3 [12800/51000 (25%)]\tLoss: -109.480232\n",
            "Train Epoch: 3 [14080/51000 (28%)]\tLoss: -110.573967\n",
            "Train Epoch: 3 [15360/51000 (30%)]\tLoss: -111.565468\n",
            "Train Epoch: 3 [16640/51000 (33%)]\tLoss: -111.686913\n",
            "Train Epoch: 3 [17920/51000 (35%)]\tLoss: -110.521614\n",
            "Train Epoch: 3 [19200/51000 (38%)]\tLoss: -111.971725\n",
            "Train Epoch: 3 [20480/51000 (40%)]\tLoss: -112.711510\n",
            "Train Epoch: 3 [21760/51000 (43%)]\tLoss: -113.083359\n",
            "Train Epoch: 3 [23040/51000 (45%)]\tLoss: -113.649277\n",
            "Train Epoch: 3 [24320/51000 (48%)]\tLoss: -113.551422\n",
            "Train Epoch: 3 [25600/51000 (50%)]\tLoss: -112.804520\n",
            "Train Epoch: 3 [26880/51000 (53%)]\tLoss: -114.324341\n",
            "Train Epoch: 3 [28160/51000 (55%)]\tLoss: -115.912430\n",
            "Train Epoch: 3 [29440/51000 (58%)]\tLoss: -114.699257\n",
            "Train Epoch: 3 [30720/51000 (60%)]\tLoss: -115.590149\n",
            "Train Epoch: 3 [32000/51000 (63%)]\tLoss: -115.282036\n",
            "Train Epoch: 3 [33280/51000 (65%)]\tLoss: -116.628242\n",
            "Train Epoch: 3 [34560/51000 (68%)]\tLoss: -116.989532\n",
            "Train Epoch: 3 [35840/51000 (70%)]\tLoss: -116.806778\n",
            "Train Epoch: 3 [37120/51000 (73%)]\tLoss: -119.228561\n",
            "Train Epoch: 3 [38400/51000 (75%)]\tLoss: -118.490288\n",
            "Train Epoch: 3 [39680/51000 (78%)]\tLoss: -118.378105\n",
            "Train Epoch: 3 [40960/51000 (80%)]\tLoss: -119.227760\n",
            "Train Epoch: 3 [42240/51000 (83%)]\tLoss: -118.623055\n",
            "Train Epoch: 3 [43520/51000 (85%)]\tLoss: -119.166145\n",
            "Train Epoch: 3 [44800/51000 (88%)]\tLoss: -120.481621\n",
            "Train Epoch: 3 [46080/51000 (90%)]\tLoss: -120.889488\n",
            "Train Epoch: 3 [47360/51000 (93%)]\tLoss: -121.421082\n",
            "Train Epoch: 3 [48640/51000 (95%)]\tLoss: -121.489601\n",
            "Train Epoch: 3 [49920/51000 (98%)]\tLoss: -122.148132\n",
            "Test set: Average loss: -123.5964, Accuracy: 8625/9000 (96%)\n",
            "\n",
            "Train Epoch: 4 [0/51000 (0%)]\tLoss: -121.230499\n",
            "Train Epoch: 4 [1280/51000 (3%)]\tLoss: -122.508835\n",
            "Train Epoch: 4 [2560/51000 (5%)]\tLoss: -123.354874\n",
            "Train Epoch: 4 [3840/51000 (8%)]\tLoss: -122.435081\n",
            "Train Epoch: 4 [5120/51000 (10%)]\tLoss: -123.191307\n",
            "Train Epoch: 4 [6400/51000 (13%)]\tLoss: -123.634903\n",
            "Train Epoch: 4 [7680/51000 (15%)]\tLoss: -123.085083\n",
            "Train Epoch: 4 [8960/51000 (18%)]\tLoss: -123.865631\n",
            "Train Epoch: 4 [10240/51000 (20%)]\tLoss: -124.725067\n",
            "Train Epoch: 4 [11520/51000 (23%)]\tLoss: -124.049980\n",
            "Train Epoch: 4 [12800/51000 (25%)]\tLoss: -124.425690\n",
            "Train Epoch: 4 [14080/51000 (28%)]\tLoss: -125.473763\n",
            "Train Epoch: 4 [15360/51000 (30%)]\tLoss: -126.437340\n",
            "Train Epoch: 4 [16640/51000 (33%)]\tLoss: -126.359962\n",
            "Train Epoch: 4 [17920/51000 (35%)]\tLoss: -125.055519\n",
            "Train Epoch: 4 [19200/51000 (38%)]\tLoss: -126.460503\n",
            "Train Epoch: 4 [20480/51000 (40%)]\tLoss: -127.050308\n",
            "Train Epoch: 4 [21760/51000 (43%)]\tLoss: -127.335007\n",
            "Train Epoch: 4 [23040/51000 (45%)]\tLoss: -127.867447\n",
            "Train Epoch: 4 [24320/51000 (48%)]\tLoss: -127.474991\n",
            "Train Epoch: 4 [25600/51000 (50%)]\tLoss: -126.641037\n",
            "Train Epoch: 4 [26880/51000 (53%)]\tLoss: -128.086914\n",
            "Train Epoch: 4 [28160/51000 (55%)]\tLoss: -129.625443\n",
            "Train Epoch: 4 [29440/51000 (58%)]\tLoss: -128.220169\n",
            "Train Epoch: 4 [30720/51000 (60%)]\tLoss: -129.042267\n",
            "Train Epoch: 4 [32000/51000 (63%)]\tLoss: -128.550797\n",
            "Train Epoch: 4 [33280/51000 (65%)]\tLoss: -129.853241\n",
            "Train Epoch: 4 [34560/51000 (68%)]\tLoss: -130.105621\n",
            "Train Epoch: 4 [35840/51000 (70%)]\tLoss: -129.817413\n",
            "Train Epoch: 4 [37120/51000 (73%)]\tLoss: -132.149170\n",
            "Train Epoch: 4 [38400/51000 (75%)]\tLoss: -131.299225\n",
            "Train Epoch: 4 [39680/51000 (78%)]\tLoss: -131.077271\n",
            "Train Epoch: 4 [40960/51000 (80%)]\tLoss: -131.773224\n",
            "Train Epoch: 4 [42240/51000 (83%)]\tLoss: -131.089310\n",
            "Train Epoch: 4 [43520/51000 (85%)]\tLoss: -131.509140\n",
            "Train Epoch: 4 [44800/51000 (88%)]\tLoss: -132.799454\n",
            "Train Epoch: 4 [46080/51000 (90%)]\tLoss: -133.021393\n",
            "Train Epoch: 4 [47360/51000 (93%)]\tLoss: -133.393814\n",
            "Train Epoch: 4 [48640/51000 (95%)]\tLoss: -133.360794\n",
            "Train Epoch: 4 [49920/51000 (98%)]\tLoss: -133.986298\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-04-10 02:58:08,476]\u001b[0m Trial 3 finished with value: -135.37448882378473 and parameters: {'lr': 0.00010643990215300088, 'optimizer_name': 'RMSprop'}. Best is trial 1 with value: -2367085.104.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test set: Average loss: -135.3745, Accuracy: 8666/9000 (96%)\n",
            "\n",
            "\n",
            "********************************\n",
            "\n",
            "Suggested hyperparameters: \n",
            "{'lr': 0.01861304613287783, 'optimizer_name': 'RMSprop'}\n",
            "Train Epoch: 0 [0/51000 (0%)]\tLoss: 0.045683\n",
            "Train Epoch: 0 [1280/51000 (3%)]\tLoss: -3129.826172\n",
            "Train Epoch: 0 [2560/51000 (5%)]\tLoss: -7757.903809\n",
            "Train Epoch: 0 [3840/51000 (8%)]\tLoss: -12927.195312\n",
            "Train Epoch: 0 [5120/51000 (10%)]\tLoss: -18150.269531\n",
            "Train Epoch: 0 [6400/51000 (13%)]\tLoss: -23631.914062\n",
            "Train Epoch: 0 [7680/51000 (15%)]\tLoss: -29186.041016\n",
            "Train Epoch: 0 [8960/51000 (18%)]\tLoss: -34908.343750\n",
            "Train Epoch: 0 [10240/51000 (20%)]\tLoss: -40810.878906\n",
            "Train Epoch: 0 [11520/51000 (23%)]\tLoss: -46664.046875\n",
            "Train Epoch: 0 [12800/51000 (25%)]\tLoss: -52746.484375\n",
            "Train Epoch: 0 [14080/51000 (28%)]\tLoss: -58874.832031\n",
            "Train Epoch: 0 [15360/51000 (30%)]\tLoss: -65243.402344\n",
            "Train Epoch: 0 [16640/51000 (33%)]\tLoss: -71337.476562\n",
            "Train Epoch: 0 [17920/51000 (35%)]\tLoss: -78309.515625\n",
            "Train Epoch: 0 [19200/51000 (38%)]\tLoss: -85015.265625\n",
            "Train Epoch: 0 [20480/51000 (40%)]\tLoss: -91952.593750\n",
            "Train Epoch: 0 [21760/51000 (43%)]\tLoss: -99091.437500\n",
            "Train Epoch: 0 [23040/51000 (45%)]\tLoss: -106164.687500\n",
            "Train Epoch: 0 [24320/51000 (48%)]\tLoss: -113373.648438\n",
            "Train Epoch: 0 [25600/51000 (50%)]\tLoss: -121009.015625\n",
            "Train Epoch: 0 [26880/51000 (53%)]\tLoss: -128526.437500\n",
            "Train Epoch: 0 [28160/51000 (55%)]\tLoss: -136120.078125\n",
            "Train Epoch: 0 [29440/51000 (58%)]\tLoss: -144069.046875\n",
            "Train Epoch: 0 [30720/51000 (60%)]\tLoss: -151803.468750\n",
            "Train Epoch: 0 [32000/51000 (63%)]\tLoss: -160138.796875\n",
            "Train Epoch: 0 [33280/51000 (65%)]\tLoss: -168323.343750\n",
            "Train Epoch: 0 [34560/51000 (68%)]\tLoss: -176077.406250\n",
            "Train Epoch: 0 [35840/51000 (70%)]\tLoss: -185071.000000\n",
            "Train Epoch: 0 [37120/51000 (73%)]\tLoss: -193341.171875\n",
            "Train Epoch: 0 [38400/51000 (75%)]\tLoss: -201951.453125\n",
            "Train Epoch: 0 [39680/51000 (78%)]\tLoss: -211412.031250\n",
            "Train Epoch: 0 [40960/51000 (80%)]\tLoss: -220192.953125\n",
            "Train Epoch: 0 [42240/51000 (83%)]\tLoss: -229471.625000\n",
            "Train Epoch: 0 [43520/51000 (85%)]\tLoss: -238737.703125\n",
            "Train Epoch: 0 [44800/51000 (88%)]\tLoss: -248444.593750\n",
            "Train Epoch: 0 [46080/51000 (90%)]\tLoss: -257791.109375\n",
            "Train Epoch: 0 [47360/51000 (93%)]\tLoss: -267607.781250\n",
            "Train Epoch: 0 [48640/51000 (95%)]\tLoss: -277246.062500\n",
            "Train Epoch: 0 [49920/51000 (98%)]\tLoss: -287562.000000\n",
            "Test set: Average loss: -291456.6761, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 1 [0/51000 (0%)]\tLoss: -296271.687500\n",
            "Train Epoch: 1 [1280/51000 (3%)]\tLoss: -303944.093750\n",
            "Train Epoch: 1 [2560/51000 (5%)]\tLoss: -310945.156250\n",
            "Train Epoch: 1 [3840/51000 (8%)]\tLoss: -318438.281250\n",
            "Train Epoch: 1 [5120/51000 (10%)]\tLoss: -324972.000000\n",
            "Train Epoch: 1 [6400/51000 (13%)]\tLoss: -332798.750000\n",
            "Train Epoch: 1 [7680/51000 (15%)]\tLoss: -339978.281250\n",
            "Train Epoch: 1 [8960/51000 (18%)]\tLoss: -347525.093750\n",
            "Train Epoch: 1 [10240/51000 (20%)]\tLoss: -354891.281250\n",
            "Train Epoch: 1 [11520/51000 (23%)]\tLoss: -362288.500000\n",
            "Train Epoch: 1 [12800/51000 (25%)]\tLoss: -369899.468750\n",
            "Train Epoch: 1 [14080/51000 (28%)]\tLoss: -377532.281250\n",
            "Train Epoch: 1 [15360/51000 (30%)]\tLoss: -384998.718750\n",
            "Train Epoch: 1 [16640/51000 (33%)]\tLoss: -392442.468750\n",
            "Train Epoch: 1 [17920/51000 (35%)]\tLoss: -400368.031250\n",
            "Train Epoch: 1 [19200/51000 (38%)]\tLoss: -401378.718750\n",
            "Train Epoch: 1 [20480/51000 (40%)]\tLoss: -414618.406250\n",
            "Train Epoch: 1 [21760/51000 (43%)]\tLoss: -422873.156250\n",
            "Train Epoch: 1 [23040/51000 (45%)]\tLoss: -431270.562500\n",
            "Train Epoch: 1 [24320/51000 (48%)]\tLoss: -438980.437500\n",
            "Train Epoch: 1 [25600/51000 (50%)]\tLoss: -447607.187500\n",
            "Train Epoch: 1 [26880/51000 (53%)]\tLoss: -455442.531250\n",
            "Train Epoch: 1 [28160/51000 (55%)]\tLoss: -463959.000000\n",
            "Train Epoch: 1 [29440/51000 (58%)]\tLoss: -472324.687500\n",
            "Train Epoch: 1 [30720/51000 (60%)]\tLoss: -479748.312500\n",
            "Train Epoch: 1 [32000/51000 (63%)]\tLoss: -489201.531250\n",
            "Train Epoch: 1 [33280/51000 (65%)]\tLoss: -497556.531250\n",
            "Train Epoch: 1 [34560/51000 (68%)]\tLoss: -505765.937500\n",
            "Train Epoch: 1 [35840/51000 (70%)]\tLoss: -514873.812500\n",
            "Train Epoch: 1 [37120/51000 (73%)]\tLoss: -523277.125000\n",
            "Train Epoch: 1 [38400/51000 (75%)]\tLoss: -531482.125000\n",
            "Train Epoch: 1 [39680/51000 (78%)]\tLoss: -541428.937500\n",
            "Train Epoch: 1 [40960/51000 (80%)]\tLoss: -549845.000000\n",
            "Train Epoch: 1 [42240/51000 (83%)]\tLoss: -558959.625000\n",
            "Train Epoch: 1 [43520/51000 (85%)]\tLoss: -567377.000000\n",
            "Train Epoch: 1 [44800/51000 (88%)]\tLoss: -576296.812500\n",
            "Train Epoch: 1 [46080/51000 (90%)]\tLoss: -585499.875000\n",
            "Train Epoch: 1 [47360/51000 (93%)]\tLoss: -594561.187500\n",
            "Train Epoch: 1 [48640/51000 (95%)]\tLoss: -603718.812500\n",
            "Train Epoch: 1 [49920/51000 (98%)]\tLoss: -613786.312500\n",
            "Test set: Average loss: -611060.8687, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 2 [0/51000 (0%)]\tLoss: -621205.937500\n",
            "Train Epoch: 2 [1280/51000 (3%)]\tLoss: -628633.625000\n",
            "Train Epoch: 2 [2560/51000 (5%)]\tLoss: -635205.500000\n",
            "Train Epoch: 2 [3840/51000 (8%)]\tLoss: -641977.687500\n",
            "Train Epoch: 2 [5120/51000 (10%)]\tLoss: -647447.250000\n",
            "Train Epoch: 2 [6400/51000 (13%)]\tLoss: -654100.500000\n",
            "Train Epoch: 2 [7680/51000 (15%)]\tLoss: -661141.625000\n",
            "Train Epoch: 2 [8960/51000 (18%)]\tLoss: -668380.750000\n",
            "Train Epoch: 2 [10240/51000 (20%)]\tLoss: -675452.312500\n",
            "Train Epoch: 2 [11520/51000 (23%)]\tLoss: -680874.812500\n",
            "Train Epoch: 2 [12800/51000 (25%)]\tLoss: -688264.562500\n",
            "Train Epoch: 2 [14080/51000 (28%)]\tLoss: -695427.687500\n",
            "Train Epoch: 2 [15360/51000 (30%)]\tLoss: -701532.812500\n",
            "Train Epoch: 2 [16640/51000 (33%)]\tLoss: -708396.000000\n",
            "Train Epoch: 2 [17920/51000 (35%)]\tLoss: -714812.500000\n",
            "Train Epoch: 2 [19200/51000 (38%)]\tLoss: -722250.812500\n",
            "Train Epoch: 2 [20480/51000 (40%)]\tLoss: -722747.812500\n",
            "Train Epoch: 2 [21760/51000 (43%)]\tLoss: -733762.500000\n",
            "Train Epoch: 2 [23040/51000 (45%)]\tLoss: -741048.250000\n",
            "Train Epoch: 2 [24320/51000 (48%)]\tLoss: -748633.875000\n",
            "Train Epoch: 2 [25600/51000 (50%)]\tLoss: -756694.062500\n",
            "Train Epoch: 2 [26880/51000 (53%)]\tLoss: -763090.000000\n",
            "Train Epoch: 2 [28160/51000 (55%)]\tLoss: -769571.312500\n",
            "Train Epoch: 2 [29440/51000 (58%)]\tLoss: -776990.687500\n",
            "Train Epoch: 2 [30720/51000 (60%)]\tLoss: -783566.625000\n",
            "Train Epoch: 2 [32000/51000 (63%)]\tLoss: -792796.500000\n",
            "Train Epoch: 2 [33280/51000 (65%)]\tLoss: -799754.875000\n",
            "Train Epoch: 2 [34560/51000 (68%)]\tLoss: -805689.937500\n",
            "Train Epoch: 2 [35840/51000 (70%)]\tLoss: -813980.250000\n",
            "Train Epoch: 2 [37120/51000 (73%)]\tLoss: -821155.625000\n",
            "Train Epoch: 2 [38400/51000 (75%)]\tLoss: -828112.000000\n",
            "Train Epoch: 2 [39680/51000 (78%)]\tLoss: -836653.375000\n",
            "Train Epoch: 2 [40960/51000 (80%)]\tLoss: -843573.812500\n",
            "Train Epoch: 2 [42240/51000 (83%)]\tLoss: -851517.000000\n",
            "Train Epoch: 2 [43520/51000 (85%)]\tLoss: -858716.312500\n",
            "Train Epoch: 2 [44800/51000 (88%)]\tLoss: -866404.812500\n",
            "Train Epoch: 2 [46080/51000 (90%)]\tLoss: -873925.062500\n",
            "Train Epoch: 2 [47360/51000 (93%)]\tLoss: -881684.500000\n",
            "Train Epoch: 2 [48640/51000 (95%)]\tLoss: -889161.125000\n",
            "Train Epoch: 2 [49920/51000 (98%)]\tLoss: -896945.250000\n",
            "Test set: Average loss: -922070.1907, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 3 [0/51000 (0%)]\tLoss: -902948.562500\n",
            "Train Epoch: 3 [1280/51000 (3%)]\tLoss: -909459.562500\n",
            "Train Epoch: 3 [2560/51000 (5%)]\tLoss: -914779.562500\n",
            "Train Epoch: 3 [3840/51000 (8%)]\tLoss: -919799.187500\n",
            "Train Epoch: 3 [5120/51000 (10%)]\tLoss: -924912.062500\n",
            "Train Epoch: 3 [6400/51000 (13%)]\tLoss: -930668.687500\n",
            "Train Epoch: 3 [7680/51000 (15%)]\tLoss: -935893.250000\n",
            "Train Epoch: 3 [8960/51000 (18%)]\tLoss: -941486.812500\n",
            "Train Epoch: 3 [10240/51000 (20%)]\tLoss: -947644.687500\n",
            "Train Epoch: 3 [11520/51000 (23%)]\tLoss: -952276.187500\n",
            "Train Epoch: 3 [12800/51000 (25%)]\tLoss: -958375.437500\n",
            "Train Epoch: 3 [14080/51000 (28%)]\tLoss: -963551.375000\n",
            "Train Epoch: 3 [15360/51000 (30%)]\tLoss: -968540.937500\n",
            "Train Epoch: 3 [16640/51000 (33%)]\tLoss: -974176.125000\n",
            "Train Epoch: 3 [17920/51000 (35%)]\tLoss: -980029.375000\n",
            "Train Epoch: 3 [19200/51000 (38%)]\tLoss: -985889.375000\n",
            "Train Epoch: 3 [20480/51000 (40%)]\tLoss: -990945.562500\n",
            "Train Epoch: 3 [21760/51000 (43%)]\tLoss: -996612.250000\n",
            "Train Epoch: 3 [23040/51000 (45%)]\tLoss: -1001875.312500\n",
            "Train Epoch: 3 [24320/51000 (48%)]\tLoss: -1007020.937500\n",
            "Train Epoch: 3 [25600/51000 (50%)]\tLoss: -1013366.937500\n",
            "Train Epoch: 3 [26880/51000 (53%)]\tLoss: -1018036.437500\n",
            "Train Epoch: 3 [28160/51000 (55%)]\tLoss: -1024500.250000\n",
            "Train Epoch: 3 [29440/51000 (58%)]\tLoss: -1030198.500000\n",
            "Train Epoch: 3 [30720/51000 (60%)]\tLoss: -1035738.875000\n",
            "Train Epoch: 3 [32000/51000 (63%)]\tLoss: -1041779.312500\n",
            "Train Epoch: 3 [33280/51000 (65%)]\tLoss: -1047429.812500\n",
            "Train Epoch: 3 [34560/51000 (68%)]\tLoss: -1051950.250000\n",
            "Train Epoch: 3 [35840/51000 (70%)]\tLoss: -1058492.250000\n",
            "Train Epoch: 3 [37120/51000 (73%)]\tLoss: -1063395.375000\n",
            "Train Epoch: 3 [38400/51000 (75%)]\tLoss: -1069077.875000\n",
            "Train Epoch: 3 [39680/51000 (78%)]\tLoss: -1076612.875000\n",
            "Train Epoch: 3 [40960/51000 (80%)]\tLoss: -1081364.000000\n",
            "Train Epoch: 3 [42240/51000 (83%)]\tLoss: -1087149.625000\n",
            "Train Epoch: 3 [43520/51000 (85%)]\tLoss: -1092824.125000\n",
            "Train Epoch: 3 [44800/51000 (88%)]\tLoss: -1098971.875000\n",
            "Train Epoch: 3 [46080/51000 (90%)]\tLoss: -1104419.125000\n",
            "Train Epoch: 3 [47360/51000 (93%)]\tLoss: -1110574.875000\n",
            "Train Epoch: 3 [48640/51000 (95%)]\tLoss: -1116139.125000\n",
            "Train Epoch: 3 [49920/51000 (98%)]\tLoss: -1122506.875000\n",
            "Test set: Average loss: -1134721.0316, Accuracy: 895/9000 (10%)\n",
            "\n",
            "Train Epoch: 4 [0/51000 (0%)]\tLoss: -1126247.250000\n",
            "Train Epoch: 4 [1280/51000 (3%)]\tLoss: -1131190.500000\n",
            "Train Epoch: 4 [2560/51000 (5%)]\tLoss: -1135824.750000\n",
            "Train Epoch: 4 [3840/51000 (8%)]\tLoss: -1140159.625000\n",
            "Train Epoch: 4 [5120/51000 (10%)]\tLoss: -1143688.125000\n",
            "Train Epoch: 4 [6400/51000 (13%)]\tLoss: -1147659.000000\n",
            "Train Epoch: 4 [7680/51000 (15%)]\tLoss: -1151796.375000\n",
            "Train Epoch: 4 [8960/51000 (18%)]\tLoss: -1155887.375000\n",
            "Train Epoch: 4 [10240/51000 (20%)]\tLoss: -1161302.125000\n",
            "Train Epoch: 4 [11520/51000 (23%)]\tLoss: -1164311.500000\n",
            "Train Epoch: 4 [12800/51000 (25%)]\tLoss: -1169033.875000\n",
            "Train Epoch: 4 [14080/51000 (28%)]\tLoss: -1173530.875000\n",
            "Train Epoch: 4 [15360/51000 (30%)]\tLoss: -1176298.250000\n",
            "Train Epoch: 4 [16640/51000 (33%)]\tLoss: -1181203.625000\n",
            "Train Epoch: 4 [17920/51000 (35%)]\tLoss: -1185816.875000\n",
            "Train Epoch: 4 [19200/51000 (38%)]\tLoss: -1190788.250000\n",
            "Train Epoch: 4 [20480/51000 (40%)]\tLoss: -1194361.875000\n",
            "Train Epoch: 4 [21760/51000 (43%)]\tLoss: -1199131.250000\n",
            "Train Epoch: 4 [23040/51000 (45%)]\tLoss: -1203050.500000\n",
            "Train Epoch: 4 [24320/51000 (48%)]\tLoss: -1206485.250000\n",
            "Train Epoch: 4 [25600/51000 (50%)]\tLoss: -1211960.250000\n",
            "Train Epoch: 4 [26880/51000 (53%)]\tLoss: -1214861.750000\n",
            "Train Epoch: 4 [28160/51000 (55%)]\tLoss: -1219594.250000\n",
            "Train Epoch: 4 [29440/51000 (58%)]\tLoss: -1223964.875000\n",
            "Train Epoch: 4 [30720/51000 (60%)]\tLoss: -1228058.750000\n",
            "Train Epoch: 4 [32000/51000 (63%)]\tLoss: -1232801.875000\n",
            "Train Epoch: 4 [33280/51000 (65%)]\tLoss: -1236897.875000\n",
            "Train Epoch: 4 [34560/51000 (68%)]\tLoss: -1240153.500000\n",
            "Train Epoch: 4 [35840/51000 (70%)]\tLoss: -1245069.875000\n",
            "Train Epoch: 4 [37120/51000 (73%)]\tLoss: -1248956.125000\n",
            "Train Epoch: 4 [38400/51000 (75%)]\tLoss: -1252442.625000\n",
            "Train Epoch: 4 [39680/51000 (78%)]\tLoss: -1258542.250000\n",
            "Train Epoch: 4 [40960/51000 (80%)]\tLoss: -1262064.125000\n",
            "Train Epoch: 4 [42240/51000 (83%)]\tLoss: -1266431.500000\n",
            "Train Epoch: 4 [43520/51000 (85%)]\tLoss: -1270545.125000\n",
            "Train Epoch: 4 [44800/51000 (88%)]\tLoss: -1275106.375000\n",
            "Train Epoch: 4 [46080/51000 (90%)]\tLoss: -1279370.750000\n",
            "Train Epoch: 4 [47360/51000 (93%)]\tLoss: -1284268.250000\n",
            "Train Epoch: 4 [48640/51000 (95%)]\tLoss: -1288130.750000\n",
            "Train Epoch: 4 [49920/51000 (98%)]\tLoss: -1293179.500000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-04-10 03:12:01,755]\u001b[0m Trial 4 finished with value: -1285948.324888889 and parameters: {'lr': 0.01861304613287783, 'optimizer_name': 'RMSprop'}. Best is trial 1 with value: -2367085.104.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test set: Average loss: -1285948.3249, Accuracy: 895/9000 (10%)\n",
            "\n",
            "\n",
            "++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Study statistics: \n",
            "  Number of finished trials:  5\n",
            "Best trial:\n",
            "  Trial number:  1\n",
            "  Loss (trial value):  -2367085.104\n",
            "  Params: \n",
            "    lr: 0.022381284365837674\n",
            "    optimizer_name: Adam\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaaA04yUtj1T"
      },
      "source": [
        "Train the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCgK_yOvtrjz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "155156b6-b623-4f7c-afdd-851ae86d7f79"
      },
      "source": [
        "for epoch in range(1, 20):\n",
        "  train(epoch, model, train_loader)\n",
        "  val(model, val_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/51000 (0%)]\tLoss: 2.501408\n",
            "Train Epoch: 1 [25600/51000 (50%)]\tLoss: 1.768204\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val set: Epoch: 1, Avg. loss: 0.6560, Accuracy: 7334/9000 (81%)\n",
            "Train Epoch: 2 [0/51000 (0%)]\tLoss: 0.399630\n",
            "Train Epoch: 2 [25600/51000 (50%)]\tLoss: 0.314973\n",
            "Val set: Epoch: 2, Avg. loss: 0.3970, Accuracy: 8065/9000 (90%)\n",
            "Train Epoch: 3 [0/51000 (0%)]\tLoss: 0.172807\n",
            "Train Epoch: 3 [25600/51000 (50%)]\tLoss: 0.172641\n",
            "Val set: Epoch: 3, Avg. loss: 0.2522, Accuracy: 8395/9000 (93%)\n",
            "Train Epoch: 4 [0/51000 (0%)]\tLoss: 0.098940\n",
            "Train Epoch: 4 [25600/51000 (50%)]\tLoss: 0.138717\n",
            "Val set: Epoch: 4, Avg. loss: 0.2303, Accuracy: 8490/9000 (94%)\n",
            "Train Epoch: 5 [0/51000 (0%)]\tLoss: 0.153854\n",
            "Train Epoch: 5 [25600/51000 (50%)]\tLoss: 0.062233\n",
            "Val set: Epoch: 5, Avg. loss: 0.1667, Accuracy: 8587/9000 (95%)\n",
            "Train Epoch: 6 [0/51000 (0%)]\tLoss: 0.059971\n",
            "Train Epoch: 6 [25600/51000 (50%)]\tLoss: 0.069005\n",
            "Val set: Epoch: 6, Avg. loss: 0.1588, Accuracy: 8584/9000 (95%)\n",
            "Train Epoch: 7 [0/51000 (0%)]\tLoss: 0.048258\n",
            "Train Epoch: 7 [25600/51000 (50%)]\tLoss: 0.029516\n",
            "Val set: Epoch: 7, Avg. loss: 0.1121, Accuracy: 8741/9000 (97%)\n",
            "Train Epoch: 8 [0/51000 (0%)]\tLoss: 0.038844\n",
            "Train Epoch: 8 [25600/51000 (50%)]\tLoss: 0.086600\n",
            "Val set: Epoch: 8, Avg. loss: 0.1620, Accuracy: 8682/9000 (96%)\n",
            "Train Epoch: 9 [0/51000 (0%)]\tLoss: 0.059277\n",
            "Train Epoch: 9 [25600/51000 (50%)]\tLoss: 0.019425\n",
            "Val set: Epoch: 9, Avg. loss: 0.1266, Accuracy: 8740/9000 (97%)\n",
            "Train Epoch: 10 [0/51000 (0%)]\tLoss: 0.023946\n",
            "Train Epoch: 10 [25600/51000 (50%)]\tLoss: 0.044719\n",
            "Val set: Epoch: 10, Avg. loss: 0.1345, Accuracy: 8734/9000 (97%)\n",
            "Train Epoch: 11 [0/51000 (0%)]\tLoss: 0.009852\n",
            "Train Epoch: 11 [25600/51000 (50%)]\tLoss: 0.028088\n",
            "Val set: Epoch: 11, Avg. loss: 0.2358, Accuracy: 8581/9000 (95%)\n",
            "Train Epoch: 12 [0/51000 (0%)]\tLoss: 0.061072\n",
            "Train Epoch: 12 [25600/51000 (50%)]\tLoss: 0.006670\n",
            "Val set: Epoch: 12, Avg. loss: 0.1892, Accuracy: 8624/9000 (96%)\n",
            "Train Epoch: 13 [0/51000 (0%)]\tLoss: 0.076230\n",
            "Train Epoch: 13 [25600/51000 (50%)]\tLoss: 0.024419\n",
            "Val set: Epoch: 13, Avg. loss: 0.1195, Accuracy: 8767/9000 (97%)\n",
            "Train Epoch: 14 [0/51000 (0%)]\tLoss: 0.026704\n",
            "Train Epoch: 14 [25600/51000 (50%)]\tLoss: 0.017958\n",
            "Val set: Epoch: 14, Avg. loss: 0.1439, Accuracy: 8760/9000 (97%)\n",
            "Train Epoch: 15 [0/51000 (0%)]\tLoss: 0.003262\n",
            "Train Epoch: 15 [25600/51000 (50%)]\tLoss: 0.060690\n",
            "Val set: Epoch: 15, Avg. loss: 0.4059, Accuracy: 8364/9000 (93%)\n",
            "Train Epoch: 16 [0/51000 (0%)]\tLoss: 0.029106\n",
            "Train Epoch: 16 [25600/51000 (50%)]\tLoss: 0.032114\n",
            "Val set: Epoch: 16, Avg. loss: 0.0974, Accuracy: 8817/9000 (98%)\n",
            "Train Epoch: 17 [0/51000 (0%)]\tLoss: 0.007636\n",
            "Train Epoch: 17 [25600/51000 (50%)]\tLoss: 0.002733\n",
            "Val set: Epoch: 17, Avg. loss: 0.0873, Accuracy: 8812/9000 (98%)\n",
            "Train Epoch: 18 [0/51000 (0%)]\tLoss: 0.000740\n",
            "Train Epoch: 18 [25600/51000 (50%)]\tLoss: 0.018287\n",
            "Val set: Epoch: 18, Avg. loss: 0.0941, Accuracy: 8829/9000 (98%)\n",
            "Train Epoch: 19 [0/51000 (0%)]\tLoss: 0.002805\n",
            "Train Epoch: 19 [25600/51000 (50%)]\tLoss: 0.001969\n",
            "Val set: Epoch: 19, Avg. loss: 0.1019, Accuracy: 8817/9000 (98%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcRRKE7MzSEQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "5f35d38e-73ab-4bfc-ba6c-a2e54ecc9b22"
      },
      "source": [
        "fig = plt.figure()\n",
        "plt.plot(train_counter, train_losses, color='skyblue',marker='.')\n",
        "plt.legend(['Train Loss'], loc='upper right')\n",
        "plt.xlabel('number of training examples seen')\n",
        "plt.ylabel('negative log likelihood loss')\n",
        "plt.grid(linestyle='-.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX0XLJDjkqX_"
      },
      "source": [
        "## Train on ALL data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZmnWuVmmUCU"
      },
      "source": [
        "full_model = models.resnet34(pretrained=False)\n",
        "full_model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "full_model.fc = nn.Linear(in_features=512, out_features=10, bias=True)\n",
        "full_model = full_model.to(DEVICE)\n",
        "optimizer = optim.RMSprop(full_model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56cCdP5Ektpl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c600c76-56c6-4147-8d89-438f912046b3"
      },
      "source": [
        "for epoch in range(1, 19):\n",
        "  train(epoch, full_model, data_loader)\n",
        "  val(full_model, val_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.553793\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.109975\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.452095\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val set: Epoch: 1, Avg. loss: 0.4209, Accuracy: 7849/9000 (87%)\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.426845\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.116302\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.279465\n",
            "Val set: Epoch: 2, Avg. loss: 0.3359, Accuracy: 8053/9000 (89%)\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.157237\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.170074\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.067195\n",
            "Val set: Epoch: 3, Avg. loss: 0.3650, Accuracy: 8101/9000 (90%)\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.119769\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.098883\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.077822\n",
            "Val set: Epoch: 4, Avg. loss: 0.1145, Accuracy: 8666/9000 (96%)\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.094105\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.186172\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.077245\n",
            "Val set: Epoch: 5, Avg. loss: 0.1311, Accuracy: 8638/9000 (96%)\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.107980\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.120999\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.099004\n",
            "Val set: Epoch: 6, Avg. loss: 0.1562, Accuracy: 8631/9000 (96%)\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.141392\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.113626\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.019705\n",
            "Val set: Epoch: 7, Avg. loss: 0.0798, Accuracy: 8770/9000 (97%)\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.036102\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.024585\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.011089\n",
            "Val set: Epoch: 8, Avg. loss: 0.0448, Accuracy: 8855/9000 (98%)\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.026656\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.030966\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.038459\n",
            "Val set: Epoch: 9, Avg. loss: 0.3108, Accuracy: 8424/9000 (94%)\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.073692\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.054521\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.023289\n",
            "Val set: Epoch: 10, Avg. loss: 0.0968, Accuracy: 8741/9000 (97%)\n",
            "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.009297\n",
            "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.018211\n",
            "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.037474\n",
            "Val set: Epoch: 11, Avg. loss: 0.0144, Accuracy: 8953/9000 (99%)\n",
            "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.003352\n",
            "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.065566\n",
            "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.024997\n",
            "Val set: Epoch: 12, Avg. loss: 0.0301, Accuracy: 8900/9000 (99%)\n",
            "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.011919\n",
            "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.025385\n",
            "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.008848\n",
            "Val set: Epoch: 13, Avg. loss: 0.0149, Accuracy: 8962/9000 (100%)\n",
            "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.001895\n",
            "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.001669\n",
            "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.015941\n",
            "Val set: Epoch: 14, Avg. loss: 0.0269, Accuracy: 8931/9000 (99%)\n",
            "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.001084\n",
            "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.021366\n",
            "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.001726\n",
            "Val set: Epoch: 15, Avg. loss: 0.0087, Accuracy: 8976/9000 (100%)\n",
            "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.001789\n",
            "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.013635\n",
            "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.088080\n",
            "Val set: Epoch: 16, Avg. loss: 0.0125, Accuracy: 8961/9000 (100%)\n",
            "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.000426\n",
            "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.017647\n",
            "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.013634\n",
            "Val set: Epoch: 17, Avg. loss: 0.0069, Accuracy: 8983/9000 (100%)\n",
            "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.009577\n",
            "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.020808\n",
            "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.009188\n",
            "Val set: Epoch: 18, Avg. loss: 0.0081, Accuracy: 8980/9000 (100%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yEtGFpwdYx1"
      },
      "source": [
        "## Make predictions on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16kAGX5Hdcur"
      },
      "source": [
        "def predict(model):\n",
        "  pred_test = []\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "      data = data.to(DEVICE)\n",
        "      output = model(data)\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      for x in pred:\n",
        "        pred_test.append(x.item())\n",
        "  return pred_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWdnJKhigtM7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f942297-99f0-4c9f-d2af-e2834a1fbec8"
      },
      "source": [
        "pred_test = predict(full_model)\n",
        "print(len(pred_test))\n",
        "df_pred_test = pd.DataFrame({'id': range(10000), 'class': pred_test})\n",
        "df_pred_test.to_csv(CSV_OUTPUT_PATH, index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7VpSLb22POJ"
      },
      "source": [
        "## Manually check prediction results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Gp4fBi5iipd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "f7786b0c-9db3-4ce9-bcc7-031a9c03fdd5"
      },
      "source": [
        "# Read many TEST data and display them, compare with our prediction in CSV\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "imgs = (next(iter(test_loader)))\n",
        "print(imgs.shape)\n",
        "# imgs: [256, 1, 28, 28]\n",
        "imgs = np.squeeze(imgs)\n",
        "# imgs: [256, 28, 28]\n",
        "print(imgs.shape)\n",
        "\n",
        "for i in range(20):\n",
        "  plt.subplot(4,5,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(imgs[i], cmap='gray', interpolation='bicubic')\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 1, 224, 224])\n",
            "torch.Size([128, 224, 224])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ4AAAELCAYAAADp1+D/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOy9129c2XI9vDqH0zk3MylSI5GjGU3QvXfGF374vRiG7T/WL4YBAw4XMHwnXQUqMafOOefwPehbpc0WKZEURak1ZwHEjCSS3Wf33ruqVq2qMozHY+jQoUOHDh03BePHfgM6dOjQoeP3Bd3w6NChQ4eOG4VueHTo0KFDx41CNzw6dOjQoeNGoRseHTp06NBxo9ANjw4dOnTouFGYL/PNBoPhxrXXBoMBBoMBZrMZVqsVNpsNVqtVvsxmMwwGA4bDIbrdLlqtFur1OrrdLkaj0Ru/bzweG276GS6Cm15brqvFYkEgEEAgEIDZbEa/35c1bDab6PV6Z67jWdDX9sPh9762BoMBdrsdPp8PDocDADAYDNDr9dDr9TAYDDAej2EymWCxWGC1WmGxWGAymTAej9Hr9dDpdNDpdNDr9dDv98FSkmlfW4PBAKPRKM9us9lgMpkwGo3kmXmGP0L5TGE8Hocn//JShuemYDAY4HK5EA6HYbFY0O/34XK5MDMzg+XlZSwtLSESiSAajSIYDMJms6FareLZs2f461//ip9++gnHx8fnGp/PDUajEWazGSaTCcCrA8mDeNb3hkIh3LlzB9FoFKFQCD/++CP+/Oc/w+PxIJ/PY3NzE5ubm3j58iWeP3+O4+NjNJtNDIfDm360zxa8LIxGI8bjMcbjMUaj0ce4GD5p0Dny+Xz4+uuv8U//9E9YW1vDaDRCpVJBoVBALpdDpVJBr9eDw+FAKBRCJBJBKBSCx+PBaDRCKpXC5uYmfvnlF+zt7aFYLGIwGHzsx3svmEwmOBwOzM7OYmFhAdFoFNFoFJFIBIPBAOl0GslkEsViEd1uF9VqFel0GrVa7Sb32dFZf3njhoeeNsFDBwBmsxlmsxl2ux2zs7NYX1+H0+lEpVLBcDiEz+dDJBJBIBCAw+EQT8ZqtcJgMKBWqyGfz6Ner6Pf7/8ujI7BYIDVaoWmabBarRiPx2g2m2i1WmcaCovFgoWFBfzzP/8z7t+/D4/Hg0AgAJfLBbPZDKfTiVAohI2NDYTDYZhMJtRqNTHi+sV4PTAajXA6nbDb7RgMBuh2u+h2u79r4857Qd1jXKe5uTncunUL8XgcPp8P/X4fZrMZDocDfr8frVYLvV5PLmOPxwOv1wun0wkACIVCcDqdEiFN+z4mC+T3+/H111/jz3/+M5aWluD3+2G1WpFKpWAymRAMBmE2mzEajXB4eIiffvoJ4/EYg8EA/X7/XAf1fd8bcd7vvlHDw8VSQ+But4vBYACTyQSfz4dYLIa5uTksLy9jbW0NdrsdtVoNnU4HJpMJ4XAYHo8HdrsdZrNZHm40Gkk4/XsxOsCrNdU0DeFwGHa7HZ1OB8PhUMJr9YM3m83weDyIx+NYWlrC6uoqXC6XfB4mkwmapmFmZgYOhwNmsxlerxcWiwVGo54OvC4YjUbY7XbMzMwgHA6j0Wggn8+jVCqh3W5/7Lf3UcCoHXgVsY9Go1PRzvLyMm7dugWfzycXKR1Oi8UCp9OJfr8vf7bb7UK58ZIFAE3TYLPZTl2O0wiDwQCbzQa/34/5+Xmsrq5ifn4ebrdb/s3lcmE8HssaxONx9Ho9+Hw+tFot5PN55HI5WZvrAIMHotPpnP191/aK74DBYIDJZILT6YSmaUKhVatVjEYjWCwWxGIxfPPNN/jyyy+xvLwMv98Po9EoPOVgMIDFYpEv8rikLZxOJ9xuNywWy0091kcHjUUoFBJjbjQazzxYRqNR+N9Go4FGowFN02AymWAwGORzCAaDGI1GSKfTwquXSiV0u92p9xQ/BfAzm5+fx9LSEjKZDDqdDmq1GgwGw+9qjU0mE8xmM2w2GywWC0ajkeQWeWeQBeG+JXjued65j9XLbzQaYTAYoNPpYDwew263w2azTbUjRWYiFotheXkZs7Oz8Pl8YlCNRqOkIoBXjnm/34fNZkOz2YTH40Emk0Gv10OxWLw2w8P7xeFwyP3zUQwPX5yLwbA4FAoBAOr1OtrtNrrdLqxWK2ZmZrCxsYH19XWheQaDgQgLhsOhJNFMJhNsNhvsdjusVitGoxHm5uawuLiIra0t5HK5zzbq4boajUZYrVbY7XaJdur1+qlkoorhcIh6vY7j42M8evQIXq8Xbrf7FG3Jn2O0FIvFsLGxgWazKRfCNIKX0mVyKWdRP+8LUqNutxvBYBCBQACFQkGi9M/d6EzeCTabDYFAAF6vF0ajEY1G41SOstfroVwu4+nTp2i1WshkMojH4/D7/UIR09Gi0Ii5s9FoJJeqx+OBzWZDu91Gu92e2rvBaDSK03Lv3j08ePAA6+vr8Hq9MJvNYoitVitMJhOGw6EwSuFwGKurq6hWq9jf30ej0bg0tctUCb/U82QymeByuSRgGI1GyOfzZ/6eazc8Kp3GjcC/0zQNkUgEXq8X7XYb9XpdDpqaq3C73dA0TRZFfTD+Xi4wPaXxeAy32y3U0bSH0pPgOqprQE+RxqPX62E4HGI4HJ55gQ2HQzSbTWSzWRwdHSGTyUjuZjQaod1uy0Ht9/viVVWrVXi9XhEvTBvMZrM4O51OB+12W3KLw+FQaJ3xeHzKi6YHfhZteVUYDAZxwDweD0wmk6gxr5Py+JQwSbEz2uCdEA6H4ff75bxXKhWJwPv9PiqVCtrtNsrlMkqlElZWVrC8vAybzQafzye//6zX5Re98FqthlarNZWGh+ff5XJhbm4O6+vr+Oqrr05R46rRUY3CeDyW8+xyuYTxuMg6qPcO1cQ0LL1eD+12WwIEv9+PhYUFGI1GDAYDvHz58szfea2GhxuAlBcpNeD1JiMH2Wg0UK1W5eIbDAYolUo4OTlBLBaD0+k8Zbz4oIxwAJyKfkajEVqtFlqtFoBXSXReKtMOenT0dlwul6yt2WxGMBiEz+fDeDyWTTUZmXADqhcoJejdblck6e12W8LjYDCIRqNxSvzBz2GaYLfbcefOHYxGIxSLRZTLZYxGIwyHQ7RaLVE38bnsdjvcbjfcbje63a7QjPyZ93l+XhyxWAyapqHX66HZbKLdbn+WwgKVYne5XHA4HHJ+LRYLHA4H3G63RCOqsSC4LwuFgjgLFosF8XgcBoMBTqdTlUafem2j0SjGi5fttKpduY4+n0+iZa4dGSAaHeBVrozrPxqN5OdtNttbHVQVag7N6/XC5/OJEKnb7aJSqSCfz6PZbELTNMzOzspZe1u+8oNEPHyjjEBYf0MFT61Wk02getipVAqPHz9Gt9tFJpPBwsKCqFFowRnhEJSj8lKOx+O4e/cuxuMxEonElcLJTxVMwKprS6rC4XCgWq3KszLBSsOtevqdTgfZbBY7OzsIhUIYDAYIBoMYDofo9Xqo1Wqw2+2S/xmPx/D7/YhGo0in02i1WlNFCVksFszPz8NqtSISiaBSqQB4teeKxaLw/xRmeL1eeL1eWK1W9Ho9oTFJZV71+emYeb1eMTzValUUWdN4GV4E3LcOhwNerxcOh+MNmpwMCKm2yfWlEKlUKsFqtcLv96NcLqPf74vzye8jeOn2ej1UKhX5rKfxPjCbzXC5XFhYWMDdu3fx5ZdfYmFhAW63W2hkRuo0NARzPFxjRpBqLdMkuHakhUOhEOLxOGKxGNxutxieYrGIRCKBer0Op9OJ5eVlLC4uotfrncrHvfE8171A9KppTZlIZdK/VCrJ4WXOhh59qVTC48ePcXR0hO3tbfzpT3/CV199hfn5eSkaUxVx9IpozePxuOSSzGazFIvxYplWqN7c5NpSksuLs9frwWw2w+fzQdM02O12AEChUJDCuXq9jp2dHYk0e70e1tfXoWmaSHvH4zHMZjOGwyEcDgfm5+cxGAzQaDTEgE0LSDPE43HMz89LTrHX6yGXy0mEyFqQUCgEl8slghaqgcrlMo6Ojt6QPV90LZiToxG32+0iLPgQstZPBdy33Lu8ROk5VyoVqcmpVCrnnldSO7VaDblcTkonPB6PRFF8LfVeoYNRqVSmsnaHDovH48Hi4iK+/PJL3L9/H4uLi3C5XKfyZur3T+Y1u90ucrkcstksqtXqG7JylVmhoMPr9SIajWJubg7z8/OIx+MSCAwGA1SrVcTjcdTrddhsNiwtLSEUCqHdbr9V5HXthoeceLVaRb/fF7k0AFitVlQqFaEu6AVRFkkelwVOsVhMckLAa6meupgAxAg5HA4EAgEppPJ6vSiVSpL7mFao1Nh5a0tFSyAQQCQSgc/nE+lprVYTKrPf76Pf78vhTSQScDqd6HQ6WFtbEw+KsnSj0Yh4PI5+v49cLifrP02gUVlcXITf74fFYoGmaRiPxyiXy8JRV6tVdDodeL1eoX7oUff7fWSzWTQaDZTLZTH8pCgvQluQbvb5fPB6vWg2mygWixfm2qcVvPR42amdBngn0DColOOkwo80m5rvqVar8Pv9pwqo1bthssyC9Py0rTfv1WKxeOpO6/f7knfhfgReGxHuT+D1+ql/JpgvJjulaRr8fj9mZmYQi8UQDocRCoXg9/slDQIAkUgEsVgMnU4HRqMRfr8fmqYJE3UertXw8AOn5822K7wcnU6nHHKn0wm/3w+XywW73Y7RaIRMJoN2uy1ezcnJCXw+H4xGI6LRqPw/N44qTFBFDE6nE+FwGIuLi5K/OCvvMU1419qSGpqZmYHX60UkEhH6LJFIIJVKnZKQ8kCWSiW8fPkStVoNmqZhbW1NZKudTgculwvxeBylUkkuTk3TpqqTQa/XQzKZxPz8vOTIAoEAnE4notGo5G2Y32Iuq9/vy0GlB3dyciIlAMAruo45mrcdNIpnfD4f/H4/7Ha71FE0m82puwgvisl922g0TiX3uZfoRJG9oFiAqiw16qc8utVqiSqWtNGk2gqARECk9ywWy1RFmHzmer2Ow8NDEWT0ej2hbcPhV11phsOhGGB1LZhLrNVqouqjcaDTHgwG4fF45HywpjIYDAp7Qjm6WsoSiUTkDJEleZcz9UGoNpUSGgwGb4RurC72+/0ivSuVSuh0Osjn8+j1eqhWq9je3kalUsHx8TE2Njbw5ZdfwmaznQojucCE3W5HNBqFwWCAx+NBOBzGkydPcHBwgFwud92Pe6N429oyauQmYh6o3+8LPamu02g0Qq1Ww9HRESqVCoxGo9AcpN+63a50RLBarfB4PFhZWYHFYsHh4eFbOdxPCcPhEKlUCs+ePUO1WsXCwoJQCDTGzINRWMHDzkObyWRgNBoxMzMDt9sNu92OZrOJk5MTpFKpdyas2WssHA7D5/MBAGq1mhSNTssl+DacV7E+uW9tNhsAiCrV5/NJjkvtN2YwGFCv11Gr1U45OXQwbTab1OTw9VTniq9Lmsrv98PtdqNcLk9dlwjux3a7jZOTE/zlL3/B8+fPEQwG8e233+Lbb7+VSF1txQS8co7K5TJ2dnbw17/+FU+fPkWtVgMAUW8Gg0Gsra0hHo9LN5NgMCjBAe9d3je8c8iAkLYnlV+r1VAoFM59nvcyPKr65CxVCfC6StvlcsHn88mF6Ha7pVcY1RcOh0MWrN1uI51Oo1wuo1wuw+12Y2VlRdo8cHHVYkmG7lwsn88Hk8mEZrMp6otpxHn1JOraulwuqbVhpfLbLjO21uF6+nw+HB0dwe12i0R1OByiWq2K07C4uAhN09DpdJBKpT7oM18nxuMxisUitre3kc/nRR5Oo8rktNVqRSAQAAChMZiYbTQa8Hq9WFtbEyPCg1Wv11GtVt+aP2CBcyQSgcfjQbvdRqVSkXZE0254aAyoelSpx8lnM5lMIl6h2ICipMFgIN0FGKmq0bVaB+Xz+SS/oypcAZySyLPCPxwOIxAIIJ/PT53oiEaU+ap6vQ6Hw4FYLAav14uZmRmUy2VYrVZEo1G4XC4x4iwU3d/fx+bmJvb29tBqtUS96fF4RI02Nzcn6jUq5ki/qQXq/N10aAeDAVqtForFIo6Pj7G/v/9WR//KhoeXvmrxJrluVftNg8Mwm5pzWujJGhEussFgEG9STZYxRGRdgCoV5u/zer0Ih8OIx+MIhULY29u76uN+NExKS89aW8pU8/k8EokE+v0+fD4fAoGAJAvPysuotEU2m8Uvv/yCdruNP/3pT/B4PKhUKjg5OcF4PMbCwgLC4TC2trZwdHQkm25aDi+LDyuVClqtFhwOBxqNhiRKNU3DwsIC5ufnYbPZxIPjvp6dnYWmaULr8DKMxWJIpVJIp9PnvjYvZbfbjUgkArvdjnK5jGKx+FnIqHkXUKVGdSTvA/VO4J1hs9lEYt1ut8V5crvd8Pv9GI/HYiCy2ay8FuXToVAI4XBY+rHRC+c9wtdVKc5IJIJwOIxUKoVisfhR1up9wPIH5h0HgwFCoRD6/T5KpZKIJ5aWliS/6/f7YTAYkMlkkEqlUCgU0Gq1JB88Pz+PWCwmwhuuqcfjOVWyMVkbSSPPu5/lLNvb2/jtt9/EYTgPVzI89E4cDocoU8i5qmo1avRdLhcAoFwuSwJxPB4jEAi809NjYrJcLiOfz4togJudhocejio4sNlsiEQiuHPnDnq9Hp4+fXqVx/0omDQYqrFV19btdouip9lsIp/Pi2qLIw/sdrtEk5Pg+nLTBgIBiSi5ocfjMb744gssLCwAeKWQOzk5QbfbRb1en4qLk5Ed5bSPHz9GoVAQiW88HhdajY5SKBQSEQKFMHzmfr8vdDGr588D1Wykfu12O46Pj0VFN835HUZyNN5syklaiHkY9U7g943HY1G0dTodEQWFw2FRV05eeHy9UCgk3acZ8TDiIuiMMu/L+he1DmWaoDrjqkBCZYlKpRKazabsZbbS2d7exvPnz1Eul6X1GDt5My/M6Fttp0VGgOvKXA6dfDoZnU5HVMlPnz6V1zkPlzY8tHQUB4TDYdhsNuH0WJdD7tbv95/692KxKAvG9t1qUnASTOym02kcHR1haWlJko98LzRCvDAJhp0Mtf/1X//1so/7UaBKGlUVH704FiGymAt4Ve3NOoh6vS5919gNgkqTSbA+AoBIjFut1hv1DsPhEHa7HWtra6jX60gkEhI9TIPhAV6rehqNBg4PD5HNZmGz2TA3NycGeHd3FyaTCdFoFN999x1isZg4RzTwLIx7275VwQvX6/UiGAwK/TtNa3ceyCxQKahpGhwOh7RnKhQKp5SXvPztdrv8e6lUEvpybm4OZrP5VLmECq5lIBAQhRVrgSbZD9VxU+tRWIcyjcXQBJ0ZFpNThVatVnFwcIBEIoF0Oi2Cimw2i0wmI/kyNlqlISG70e12RXXJ3A7vDXaSYP6Xylp+tVotZLNZ5HK5d0bylzI8JpNJPjT25KFHTS6RL+jxeBCJRIReY8RSq9Vgs9lE3viuw0tPn9+nDjpSIx7+OxcTwBvGiUnNaQDfr8vlEiqBypZeryeFtOxvVSgUJGnKIl1+r8fjeasMWqVJi8Uinjx5gtu3byMQCCAej6NcLiOZTMJqtWJubk48oWmUVgOQdWw2m3IAR6MRTk5OhBJjJEd5eSwWQywWk706Go1QKpWwtbUl9OZ5YNEoPfRmsym1bNN68RFGo1HEFlarVUYS0HHqdrtot9uyl3knNJtNVKtVqcWx2+2iaFXzOSq49mxcy+J0Uj5n0e6q08D+eD6fD3a7fWrXn9Hl0tISvv/+e9y6dQtOpxONRkM6tpTLZRwfH8v4CEaefF7mb4+Pj+XOTqfTaLfbIk5iPeSkGlYdqUBpPMs0yHqRpj4PlzI8RqNRQjcaHiopAIhk0mg0wuPxSBuXbDaLQqGAWq0mMzPU5PdFvEbgdCGaKiw4i5ZS5a6UXH7qUAUT9CADgQDC4bDMEikWi6hWq6eKZFutFlKplIS39ExYcwLgnZJd/kwmk8FoNJJiMNJLJycn6HQ6cDqdctmoXcan7QCr+YdCoSByaYfDgXg8jm63i+PjYwCvlJL37t3DxsYGLBaLiDjS6TSePn2KRCLxVqk+jZnH4wEAEbtMc7NKFXQAaXi4R9T+f8PhUIqdmSBnzR4vR3rTqqesnmuec+C1+IPeO40Pv5gIp0NKwxONRjE/P49EIiF5pGmCSrUvLCzg+++/x9LSklC2XENGI8yz8N4kaJw6nY6IAEgh02lQ2aTJPLGqVFTvZXXe0bXV8RiNRvHAWQ9BXTffFCvl2fyQFcnsy6ZGMOcVM02CxYvb29tYWFhAIBDA3NzcqQhG1e5zMXiRJhIJ0a9/yuDFR4qMyp1gMAin0yl0FyXpBoMB1WoVpVIJhUJBjAv53lQqJbkINgQ9D0wOMp9BnpgbkKqVXq+HYDCIe/fuoVQqSVv1aTDsZ4E1JlRWso9VqVQC8HrP93o9nJycAID0xWIE/zZVGi9B1qpls1mcnJwgn89PrcetYjAYoFAowOl0YnZ29lS3c1LcFotF6sJyuZzkERjhqPeBWp+nXmyqUSmXy3j58qW0cOGk4kkBA/A6J2GxWOB2uzE7O4sHDx7AZrPh559/FsdsWqCuFfD6zgBe3ZP5fB67u7tIJBKyr8/7PTTcLCEYDAYiyFCDAfXPkwGC+lmp9++7cCnDwzfLqIZeBTlWtt8mX14sFsVDV+scSHWQYzcajchkMudKHGmdT05OhD9UE2x8X+VyWSgS1mVwfHOlUnmryuJTAKMctQ6HEnSj0SgeMjnwVqsleTP2veNG6Ha7Ih2mE/A2OoifmcFgkMLSw8NDKbQkfVKpVMTbisViUj80zaCnRlkom1CS1ul0Omg0Gtjd3cVgMIDdbkcgEBCP8SICmWaziUwmg1arhXQ6LX3Dpt3w8Gx2u124XC54PB5ZQ9K+jH5Y38H8IS8rGgqqM9maiao21Rix/iyRSCCXy0lT4EkWhV46RUf8LP1+v8wB29vbe6so5FMF17ZSqSCdTsPv9wOAdHRIpVJSD/k2qMaef+Z/P7S8/1KGZzAYIJPJiLyOnrnK9ZPqYZL6rAaIvV4P2WwWvV4Px8fHkqc4byjRpFSauQ21yK/ZbGJnZ0cSZEajEb1eD5ubm3j06JE0Jv2UQU+ayUJWCzOyOTk5OTWvRO1sPNnwj5uTF+pFPBGuc7fbxd7eHkajkXDIRqNRvKlarQa32y2NRz+Xdv48iJ1O55QRJhWkeposGn1XFD0ev+qhl06n0Wg0JJqc1r5hZ4HRMBvI8j7gpUa6u9VqnVm4SSk2naxarYbDw0M8e/YM6XRa9hd/F+8YfjHHQGOm/pf/z/uJSs1kMolKpTKV9VNUrGWzWTx79gwApDCW6Y7L7K2PsQaXMjz0NsgzTn7xw+Vm4AaZfDAmv9vt9qkD/baeatzIuVwOx8fH8Pv90pGZofvh4SH29vaQTqfFwz86OsLBwcFUdKXlAeWz0njbbDZUKhUkk0mhw9i54G1JvKt40+Tl0+k0BoMBFhcXsba2Jrmevb09bG1twev1YmdnZ+oK8S6CyQaglAJPqgsv0p8NeLXfGY3z97+tM/C0gQ4gO23zPgBeC1cYBZ33zMxf2mw2NBoNHB0dIZVKnZrZRceI61epVJBIJDAYDKR+SI2kgNM0ER3UZ8+e4fHjx0gmk1O7d4fDoTAeR0dHsFqtOD4+RjKZ/ORTCsAV5NQqlzfZsmay5uS8TcaNSkpM/ZmzwGSayWTCyckJNjc3pXU9w3FSGWyNUigURIo9LUlctn1ntwC1KI6JQnoy71rji4AOhFoMpkrVSVcyAVyv17G3tycFfZQEf85QDbuatD7LoZoEVZhsqMjfw+JfNYnLi7Hf74vic5ouxckzfdG7AHh1idZqNWxtbeHk5ER6EZ43foKO6/7+vnSboDqRzhhBRoZoNpvY3NzE5uYmSqXSVNwLk+C+4jOT6jw8PESpVILNZpPx9WwD9antpfdqmaN6Ilf52Yv+nMViQSwWw9raGnw+HywWC6rVqiTgKeUulUpIJBLIZrNS+DhNXqVabT35pVIIwGljf9VnZNPPQCAghXgWi0X0+8ztMIHOZo7ValUkmNO0vleFejmRtnnbc/OzsdlsmJmZQTgcljoH9ipj7zA6UDabDd1uF+l0Gru7u0in02/0KJsGXOXMkTqaHNJ31rPzeyuVCnZ2dlAul2XulJqzYF2PWvwIvIqsmAOZNkGMmqti7pV0t81mw9ramjT5BSDjT7a3tz+5vXTtTUKvC/Qu2Xttfn4eGxsb0oeInVLD4fCp5Bhbr/PP04jJw6tKRNWx4jxQqoTyopuLFeCxWAyrq6tYXFwUUQO7J6sKGg5EY48zdSTD7wnneciqnJjKRE3TcPv2bSwtLaHZbKJcLqPX60k9SzweF2WW3W5HpVLBo0eP0Gq1ZGLmp3RZfEgwL0y8iy2hE5TL5d4QCEw6bJNJc9acTAtYMK5pGoBXyuFQKCRtcWw2G2KxGObn5xGNRmWuFvspfooDMa/N8Jwns7sqqHIJBoOyoE6nE4uLi5idnZWDzguSvZv8fr/M+5nG+pLzoLb9YPU1JbqVSkWivIuOK2Ct1fLyMr7//ntsbGyIQXe5XFKIxqLgarUql2O1WpXiVB2vk+PxeBxLS0sy94ed0n0+H9rtttSxWa1W6WIQDodlfEW5XEaz2Zzq8czvg8vcGWrNiAo6ZlRbqrVB0+iI0uh8+eWXuH37tiiCR6MRPB4P4vG4jMFm02VN02RkvSrGuCzeh015F97b8LA4iwPdWEPyPjUKrKeIRCL44osvcOfOHTmkbGanKo4MBgN8Ph9u376NWq0Gp9OJ/f19ZLPZqW9LwgaTPp8Pc3NzWF5exvz8vNAz4/GrAt3t7W1sb29fuN27yWSC3+/H2toa7t69izt37pwaU84WRDy49XodJpMJpVIJuVxOqpx/L5fjZJcMlRZlZ4LV1VV899132NjYkNY4VHh1u11RHKkFjexzxoLUz6nG56bBSzoUCiEQCMBoNKJQKCCfz5/Kj04LzGYzZmdnEY/H8eDBA3z11VeiZiuXy9A0TbpJOxwOUcCqClbezRaL5ZS7IBMAACAASURBVEJ5SeB1Ox52S6EDz59V6Uu1bvJSz3bJtTgFtevuzMyM6MmTySSSyeSpwrrLvDGDwQBN07C4uIj79+/ju+++QyAQkAFIXBC1SZ7P58NXX32FQCCA5eVl/PLLL/i///s/pNPpqR59TWHF/Pw8Hjx4gI2NDczNzUlC2mKxoFgsQtM0tFqtC3PXNDwLCwuIRqPSCWFyfj27RLhcLsRiMSwtLeHk5ASHh4cij/3cL0i2L+IBJtXDPcXux8vLy1hfX8etW7fgcrnE22QtC6N0VXTA2qFarYZkMolUKnXmWGIdZ4MCGRZSzs/P4+7du1heXobJZMLm5iaePHkin8U0weFw4Mcff5TcTSwWw2AwgN/vR71eh9FolJIL1VD0ej1YLBZEo1EsLCxgdnYWzWbzQnuKd7rH45FuKRQRcf1I7xmNRmFFKCy5KN474iElNj8/j9nZWbGW3W5XRi6Tl72ohNRgMMDlcomUVx1ZrM5Wp9Vl3zI2xDMYDEgmk1J4+SFDxg8N1bDfunULoVDoVAQSi8Wkk0MgEBC11NuMwWS3ZLfbfUoSP6lEovrK7/djZWUFtVpNZN4nJydTH1W+DVwrzigxm80yRZf7eVLlxqgRON0hGYD8Vy3AZmffZDJ5oQaLOl6BDoHf70ckEpH7goa/WCxKE1E15zMtcDqdePDgAVZXVxEMBsXpoTiFBfuMrCdLW9QpoZcFnU06UGzJxdwY/91ut4sjpubX34VrGQTHDsjs48bCxUqlIkloJlfVwwqcHQlxQ3EYkeqB8/u50KTl+Gc18X5R2eunDLbjZ7v5ZDIpobbL5ZLx1IxyJnsqnQW187X6/bwM+WdVwKCuLY38eDyWvMXncFFO9v+joEPTtFOjxDkGnBcZJ+YeHR1hdnZWOjqw5IDUhCrrBSBrzXqtWq02VSPFPyYYNfr9fty9exfffPMNvvvuO6ysrMDpdMqMJPZy416eprVld30qThmNsEky95d6hjm0kY4/+2RedMot70zWRt26dUvGWasOF/t1xuNxWK1WaQhw0eDiveXUpAmOjo7QarXgcrng9Xrx7bffnqoqPj4+xvPnz2Ve/dsKINlihK29Y7GYzJPhQVblxOxQC+CUxz9ttRBngWucTCYBvDYGHGrFqvi9vT0plH3XB8/EbD6fx8HBAYLBoESNqgdFqJvbZrOh3+8jFotJp/LPAeyn5nA4xJEBXkf0pHpLpRJardYpB4ojAA4ODuDz+cTwqBfGWVDHeXDGfTAYFHHMNDtMHxocx7C0tIQHDx7gxx9/xN27dxEMBuVeWVhYkNwOL8VpalPUarXw8OFDVCoV6WxO4Q/w2vjSOeSfrVYr6vU6crmc9BO8KHWrUm10tjh4bpIJcblcCIfD0DQNw+EQiUTiwlHlexse9kjrdDo4PDwUo7O8vCzyv+FwKG+OHZRZmFgul9+otGWHhP39fczPz2N1dRU+n08OMb1HXoZqR2c2MKW2f9r7iKnzY1KpFGKxGDY2NrCxsYFAICADnh4+fIhMJnOhhD8LywqFAvb39zE3N4dYLCYbTDVA9PzVtebkUToP035B0plxOBwyTtlut7+R2+Ge5eAy9SB2Oh1kMhlsbW0hGAzCYDBgeXkZkUhEBo8Bpwf6qUnaWCyGxcVF7O/v4/j4WFRJOt4EVYSxWAzr6+v49ttvsb6+Dp/PJ9Fnv99HKBTCvXv3YLVapbXURXqYfSqo1Wr4z//8T9y+fRvLy8tYXFzE8vKyTBZmdKPKydV2RaTFSLmdVxulghH+zMyM5JKplqXxorOvzlYqFAqX6nv33u4qqQbSae12G7u7u1LoxDDX6XTij3/8o7Q/LxQK2N3dxbNnz85MUKu/b/Jym6RE+D4ASG1PPB5HIBCQ1i/TGvlwHfr9vsx0WVlZQSgUQqfTwfHxMQ4ODkQJddHLajKHo6q1gDc70qqbqtPpoFwuo1qtfpJV0VcFD7OqOKPSj1QxuyyrwhauW6PRkFEK5XIZpVIJ9+/fx61bt4RrV6lfrjvVcbwgprFx5U2CHvny8jLu3buHpf9/fEepVMLx8TH29/fRaDTEOK2srKBQKMhkzWkxPIPBAEdHRzLh02QyIRKJIBQKiZhA7VsJvJZAU5y1sbGBZrMJk8mETCbz1vPKOzsUCslI7EKhIEMfGeWzRi0cDiMajb7hhF0E18KTqPJSADg6OkKtVhPlTjAYxPr6OjY2NuDz+VCv12EwGJBIJM48ZPx97XZbxiqEQiHYbDah7ugxklKj5I9h9Xg8fqNqeRpBz0XTNOnesLS0hH6/j8PDQ6lyv6xyj5uMeTTSlaQs1e9To0a2N+H89s9F1UYHik0umVujg8O9yOicqh7maJjLZATPufbBYFB4cFIk6mtyzwKvRQnTPh3zQ4J0ksfjwfz8PJaWlmCz2ZDJZLC7u4unT59ie3sbrVYLoVAIf/zjHzE7O4s7d+5IXzPWwXzqGI/Hp3rVsasIJ+SSUVLPPfeUw+HA3Nwcms0marWaDIZ8W/qBggGOIHc4HCiXy0gkEqe6/rPLdywWg8/nw9HRkVCaN2p4VKjNAqnI4nx14JW3nM1m8fLlS2xtbaFarZ75ZjlW4eDgADs7O3JBsjUGADE89C7ZLTiTyeDFixfIZDLvHID2qYMUYigUwt27d3H79m14PB7s7e3h4cOHODg4uFKnY9Y8hMNheL1euRhpeNQ5SarSrdvtolAo4ODgQBKKnwMlpCZP1fb93W4XdrsdxWIRlUoFo9FI2vdzDkq5XEaxWDw1DoAHNpPJoFKpyJyfScNOsEMHc0lM6H4Oa3udoOFhcbnL5ZKu6b/++itevHghooJ4PC5FlXNzc1hZWcFvv/2GQqEwNfuWIyISiYSMPcjn8/jDH/6ApaUlAK/pNdVBVLu+xONxhEIhoeLPo9xUw6Np2ilmg1Eia9a4tlarVd7TZSLJazc8KvVGDpIUhsHwaojT/v6+FHie56mzPT2L7oDX3REY3ZhMJhmtsL+/LxfG8fExnj17JkOnpmGDnQVuHiZR19fXEYlE0Gw2sbu7i+3tbXnGyxhX5shobNiRml42L0j1z8znlUol7O3tYX9/X/J1047JVirck1QNktbgVF1O37VardIktVqtylpwPVm8SAWimiebhM1mQyAQQDAYhNvths1mm5rL8aahzq0aDAY4PDzETz/9hIcPH+Lk5ESoJbvdLp8LO354PB4RyEzD2pLRqdfr6Ha76Ha7sFgsmJubE4k1864E70nVuYxGoygUCjCbzSiVSkKdqWA0w6iKY23IMLFWKhwOY3Z2Fj6fT+6Es37f2/BBJElURTExq1bVFotFHB4eyuydt3346pwOTdNOyU/ZMbnf7+Pg4ABbW1uoVCoywCyRSFwq5/GpgZEOa3hYn2AymbC7u4udnR2kUqkryW/5uwFIBMPRxIx6bDYbbDYb2u22hOjNZhOpVAqPHj06FX5PKyYVOPx/ylStVqsMN2Pehx5hMBjEeDxGMplEsVg8ZUxoxFqtFqrV6qkhfZP1UoTaRsfn88Fut099tP4hwM+F9TnNZhN7e3t4/vw5ksmkUFOs6VOLKx0Oh1T5T1NnCKYeeAZJmzF6UcU/6r6iUIA5m263i2AwiOPjY+mAfh4YFbGsxeFwwGAwwOPxYHFxEYuLi3A6ncjlctJJ/DLree2Gh/Sa1+s9lahyu92o1Wo4OTlBIpEQT+Q8w0CvnBckp0AOBgPkcjk8fvwYpVJJLkq32w0AkoQvl8tTN9ZWBaXLoVAIt2/fxurqKrxeL1KpFLa3t3FyciIFulcBL8pWq4WtrS00m00plnQ4HJiZmcHs7Cyq1arMAapUKjg6OsLW1ta5Q/s+dUyq9NSZ8er30GNmdMMuwJRWRyIR9Ho9lMvlc9WTqrx1cswF8LrdCPC6Xsvv90svvs9pWNx1Qc1NsratXq+fUsdqmiZ5ndXVVQQCAcnDqX3cphXcn3TEeVeqz0SK3OFwIBKJ4O7du9A0DScnJ2i320gkEmc6Np1OR3JgpPcDgYAoXj0eD7744gvcunULBoPh3GGf78K1NgllexdK8VQulp1kd3d3kc1mxVM/z/Dwgmi1WkilUnA4HKhUKrDb7Tg5OcF///d/I51OIxQK4ZtvvsGdO3fQ7/eliJXz3ad1XgwvrUAggNu3b2NhYQEGgwHZbBbHx8eo1WpvCCguOu8ceJ3TYA7i4OBACsMCgQDu378Pq9UqhZGVSgW5XA47Ozsywnkao0nKxSlDHQwGUvDMSIf/zv6DbDbJfI9auHcW1GiJYw80TRMDpNZdqB4qaYyVlRWUSiV0Oh0Ui8WpGOx1U+Dn43a7oWmaCJgohTcajXJhfvfdd1hfX4fH48Hx8TGy2exUU+9q0b3aAkgtcyDUkd8sRSGFlkqlRByj3hfqiHbSkvF4HD6fT5gNj8eD1dVVzMzMIJ/Po91uo9PpXLqs4loMj1pRGwgEsLa2hjt37kj1LMfM5vN5ZDIZ1Ov1d75R1pocHx9jMBhga2tLZu9wFgdHXAMQvpO8OzfltKqDuJncbjcWFhYQDAZRrValGIzesUpX0is/a4a6Cq4tpeYsNhsMBnA4HBgOh8KT8/Jknm3yNaYFXE+73Q6Px4NAIHBKJs3aD+ZymOTvdDrY399HsVhEr9fD/Pw8AoHAqVn1Z70WZ6YEg0F4vV7pJk5PlBcFc2j0ThcWFvDjjz9Ku5LHjx/j4ODghlfr0wbpSpZphMNhGe3h8XgwMzODO3fuYG1tDeFwGKVSCTs7O3j58iVyudxUFZGqUOvvDg4OEIvFEA6HTzlAk+IVtbltv99HJBJBIBCA2+1GqVQ65fyzfvL4+Bjj8RiFQkGUncyDapomIhjm4NWx4xfFtRgeeiB+vx9zc3NYXV3FwsIC0uk0kskkDg8PkU6npdD0Im+SIoV8Po9qtQqbzSYeI/lzu90uLTE0TYPH45FeWpxGysTctOUi1HYtnL3R6/UQDodx584dRCKRU8KL8XiMfD6PXC4nSWl6RpMTLdXOBbxw2XLHYDBIU0un04lAIADglacDQDzwyyYTPyZoCEjJBgIBxONx2O12lMtlEbKYTCa5yNjwNpfLIZvNolgsys+rs4jOa1FEXp6HE4D02mKkyp/nWWBHdta/sRPx0dHRTSzT1ICyd86IWl1dleiUfdui0SjcbjeazSYODw/x7NkzEeNMa7TO89xoNKSoU21zdVY5hPqzmqYhGo1ibW0NmUwGvV4PiURC7hCKYtgvMJPJSCcP/g6Px4NQKCT1O1cxOsB7Gh4mqX0+H2ZnZyUvEA6HZRDR3t4e0um0TK28jKehHt5er/dGE8vJOeuUHVOpxYXZ39+fKsNDj1gtKGS+5/79+1heXpaaEobUfM79/X2hMdlVgglAtXhRXVfmOLi29OYNBgP8fr90pvZ4PELnvXz5cmoMj8lkOkU3BAIBSeAbDAaJ4gDA5/MhEonAZrPJCAi2ydE07dSMp8m+bsR4PBYJ7M7ODpaWlhCJRKQZK3+WVN/k3zmdTgSDQcRiMfj9/qnOR1w3eK45It5isUiZgVoH1e/3kc/nsbe3h19//RUvX75EKpWSOVLTGPEQTGF4vV44nU4xDNw/wOu7k6BgiN0cqAhmJE/QKa1UKmg2m6c6xJhMJoRCIRQKhVMd1GnwLmOArmx4qPmemZnB6uoqlpaW4HQ6pZNprVbDixcvkEgkhB66yoetXpTqQlLdRtrN6/VidnYWfr8fTqcTXq8X4XAYHo/nXC/gU4YaVm9vb0tkxwadLB5TB97FYjF4vV7pFMt5ROf1TzorJ0SP6vDwEJubmwCAubk5EW9ks1ns7++/UQz5KYN9vUizURLNPE+/35fDRbVUo9FALpdDoVAQQ06qjGs2aXAIRuuslKf66CxK5CzjpfaHm9b9+yFBp6nRaKDVagkF3+/3peA8k8kgkUhgf38fOzs7ODo6kuh+Wo2O2iaHzhMNz6Q4Ru1+DkAoXXZBUZXGk+C9QBk1fycjq729PXi9XkQiEXg8HkSjUall++DdqXmY7927h++++w6zs7MoFot49OgRdnZ2JK/DOQ7v82GfdWmOx2MJB3/66Sckk0nMzs5iZWVFKvunRas/CYbUNKp2ux3b29vSpdrhcMjFVq/XJZrhs5MKouxcbWr5rtelF7S5uSkREZUxk8nxaQEVgjykardzRuwWi0WMCqNEGh21Pqfb7aLZbMpYivPWlhErPdPJnlrqgQYgc03YyYDv7bzL4fcMno98Po/Dw0M0m81TpRp7e3s4OjpCOp1GvV6XeTLTeh8QqoKN6kiVYuO+YV6bQyG5rwuFgkR+hUIB2Wz2napJNU88Ho8loBgMBnjw4IE4/MVi8VJTia9keCY9NOYLMpmM5HXS6fQHL4AbDodotVri2U/SIpRfT+NmU3NcT548wcHBAaxWq9AJvKharRba7TYMBgOq1SpKpRL6/T5yuRxOTk5kNMVFDT8jLdamML9069YtUYBN23oOBgMUi0U0m02p78jn8xK1sdkpuw5UKhWhGlQ6sdfroVAoYGdnB7VaTS67bDb7Bu1IObbf70cwGJQ6CIIHmRdDs9kUtedgMBDJa71ev5lFmhKMRiO0Wi0kk0kYjUbk83mJxsvlMrLZrFysFDFNa4QzCe6XWq2Gg4MDLCwsYH5+HpqmwWazSXnAYDBAqVRCIpFAp9ORv08mk/jll1/EWKfT6QsNjVRfnwWjx8fH8Pl88Pl8p1TKF8WVDA8taLvdRjqdFgqoXC5Lm5qbavfBC5qzTJgL8nq94u1M68ZjVJdOp5HNZk/VnwCnx84aDAbUajXs7e0JFcEJoZetBeEGq9fr2Nvbg8PhQKfTgaZpN/rZXheGwyFyuZx4iyxwptpHzW2x3uMsSoaOAKlIABIBqWusUiJutxtutxuDweBUo0VeBqRA0uk0nj59ikwmIwaQDRqnKT/5oUEjnUgkRK3G4ZDM1VEeP2379F3guSyXy9jd3cX8/Dzu3buHSCQi0Xe73Uav10MymcSLFy9QKpVkTEQqlcJvv/2GdDotjNBV7gbu5e3tbZmAXC6Xb6Zlzmg0QqPRwP7+vjTka7VaKJfLN1oVrCbDAYgs2G63n+pwMK1gsu9dYDsi1vfwUrvqIDxGkycnJwBeUUexWEzWc5oONJ0T5gUnO2/TkKt76bycGA82IxEajkmOnYaHfQqZ66QjwM+Gr59MJvG///u/OD4+lgis0+mgUqnohmcCXG/mI0lh8u+nffjj28CzPdlxv91uC71I1Wk6ncb+/r5EPo1GA9ls9r1GbtDwNBoNJJNJEedcdhjkexmeTqeDfD4PACIs4MG8yShDfa1msynhpXoBf+6YLCp734PHy7pSqcBsNiMajcp02WmmL89636rheRdUg/G24mfmeNjsk6OtqbQkDU1laCKRwIsXL0SByff6e9m/lwWdBNZA8e9+D6Bjw64tVLVyvhYj8GaziVwuh+3tbWF+ruPs0hmmQ3/euXrrM1zmBwwGQx7ANBcVLI7H4/DHfhNnQV/bDwd9bT8c9LX9cPgM1hY4Z30vZXh06NChQ4eO98V06WJ16NChQ8fUQzc8OnTo0KHjRqEbHh06dOjQcaPQDY8OHTp06LhR6IZHhw4dOnTcKHTDo0OHDh06bhS64dGhQ4cOHTcK3fDo0KFDh44bhW54dOjQoUPHjUI3PDp06NCh40ahGx4dOnTo0HGj0A2PDh06dOi4UeiGR4cOHTp03Ch0w6NDhw4dOm4UuuHRoUOHDh03Ct3w6NChQ4eOG8WlRl8bDIa3To2zWCxwOBzw+XzweDywWCwylhYA+v0+qtUqSqUSOp2OjGm+SYzHY8O7v+vm8a61nQZM29qaTCbY7XZ4PB54vV5YrVYYja99MY7/bjabqFarMlL4Y2Da1vamYTAYYDAYYLVa4XQ6YbPZYDQaMRwO0e/30e/3ZaQ4R8RzCKa+tm+8LsxmM+x2OxwOh5wLo9Eoo7aB1yOvB4MBer0eOp0OOp0Oer2eOq69cNYE0ksZnrfBbDZjZmYGDx48wL/8y7/g//2//4dQKASz2SyHuVar4eeff8a///u/47/+67+wvb2Ndrv9u5mVruPTgcFgQCAQwJdffol/+Id/wD/+4z9ibm5OnCWj0Yh+v49UKoXffvsN//Zv/4b/+Z//QT6fl1nz7/v6APS9/54wmUxwOBxwOp1wu91YWFjAV199hW+++QaRSATHx8fIZDIwGo0Yj8eoVCp48uQJnjx5gkqlci2f5ecEg8EATdPwxRdf4MGDB/jTn/6EW7duweFwwGazwW63i0EfDAZot9soFAr47bff8B//8R94+fIlCoWCanjOHN19bYbHYDDAZrPB7XZD0zRYLBYYjUbxRIBXxikYDGJubg5+vx8WiwWdTkc/fDpuHPTqztqv6n+tVitcLhecTqd8z1VAT3E8Hov3OBqNMBwO1UOq45LgZ7iwsIC7d+/i1q1bWFxcxNLSEvx+P9xuN5aWlmCxWNBoNHB0dIR2u41ut4vd3V1kMpmP/QifFEwmE5xOJ2KxGFZWVjA7O4tQKASbzQaLxQKr1XoqajSbzRiPxwgEAnC73bBYLBd6nWszPAAkPGNoNh6P3zhYNpsNHo8HTqdToqHrOni8TCwWC8bjMbrdrn6odZwLUm12ux0mkwkAZL8aDAaMRiMYjUaYzWZYrVaYzWZxpC7jLNEr57mw2+2w2WxotVqoVqvodrsYDocf6jE/S3Ad/X4/5ufncf/+ffzwww9YWVmBz+cTDz0UCslnmc1m0ev1MBwOYTab0Wg0kM1mP/ajvBfoIJlMJpjNZphMJphMJqHABoMBhsOh0Izv+l0WiwUulwuRSATxeBxut1ucrfF4jNFohNFoJL/XYDDA6XQiFAphZmYGR0dHKBQK74wkr83w0Ivj4RwOh8L18QCPx2OYTCbYbLZTB/l9oS6+x+NBKBRCv99HOp3Wqbz3AD8bNWolr/s5rKn6TKPRCP1+X5wl7ufBYCB7+KrPbLFYEAgEEAgE4PP5EA6H4fF4kEwm8fLlS+TzebRarc9iTW8KVqsV8Xgcd+/exb1797C+vo6lpSWEQiEx8haLRajTXq8Hr9eL5eVleL1emM1mbG1tXcv987HA+9bhcEDTNLjdbrjdbtjtdgwGAzQaDdRqNTQaDbTbbTG654E5Mo/HI3uV1JoasY9GI5hMJjFIRqMRPp8PMzMzCAQCsNls6HQ6b3X6r8XwWK1W+Hw+xGIxRCIROJ1O9Pt9NBoNOcSj0QjtdhuJRALJZBLNZlMe9n1gNpvh9XrhcrkwGo0QDAZx9+5dNBoN9Pt98XL0Q30xGAwGmEwmWK1WObx0GphA7PV68r3cfB8r6f4+oLHp9/vo9XoSeXS7XRgMBjgcDvT7fXS7XTm0l91H9CI9Hg+WlpawuroqXvnOzo7QFp1OR496Lgij0SjRzK1bt/D111/j1q1bQody/6pO8Gg0gs1mQzgchtPpRKPRgN/vvzJ1+jFhMplgsVigaRq8Xi9CoRCCwSACgQA8Hg/cbjeGwyGq1SqKxSJyuRzS6TQKhQJarda5Z5XpEo/HA4/HA03TxMAwmgIga8a7u9PpQNM0hEIh+Hw+EXZ8UMPDZNT8/Dzu3buHr7/+GpFIBN1uF9VqFUajES6XC61WC7u7u/j555/x888/4/DwUDzMq8JoNMLpdAqv22q14Pf78d1336FUKiGfz6Ner0tYqOMVVGNPL2Y8HsslyQ0dDAbhcrlgNpvR7XZRqVSQzWZRrVYxHA6F8zUYDGg0Gh/xia6G4XCITqeDVqslxqXb7aLZbEqep9PpoNlsot1uo9/vXyryYY7I6XRC0zTEYjF88803WF9fh6ZpCAQC6HQ6KBQKKBQK+h69AOiVqx6+pmnQNE2cJafTCavVCgDyeTFq5+XKvN20GR7StoFAADMzM1haWsLCwgKi0ajQxsxZDgYD1Ot1JBIJPHz4EM+ePUM6nT7XgVINj6Zpkgqhg6n+mV+k3ZgL5edgMpne6oxeS8RjMpngdrsRCAQQDofhcDjQbrdRq9XE+2g0GkilUtje3sbOzg7K5fJ752D4IYTDYdy+fVsW3efzIZfLCU2iRzun6Uj+l+EzE4X0zCORCGZmZjAzMwOPxyOGJ5/Pw+Px4OTkBOVyWcQkANButz/yE14O5MDb7bYkm7vdLtrtNhqNhhiNTqeDer2OZrMphuciMBqNcDgciMfjWF5exszMDGZnZ+F2u+F0OuHz+RCNRjEzMwOXyzV1F+DHAD+TcDiM1dVVbGxs4NatWwiFQnLZqbkOg8Eg+5qX5mAwQKvVQq1WmzoankKKeDyO27dvY2lpCbFYDJqmwWg0otVqyTP5/X74/X5hoeg4VqvVc5+bRl0V09AxJRNCChqARDU0+JPpk7flQq/F8IxGI3S7XbnASE80m02YTCZomoZ2u41qtYparYZWqyUh31UMj9FolEsyHo8jFAohEolgZWUFRqMRL1++xPb2NtLpNFqt1u9eYMANxaS2zWaTAwpAvHm3241YLIaFhQUsLS1hZmZGkouDwQCVSgWRSAQulwu7u7uwWq0IBoMwGAzI5/Mf+SkvD0Y8fH5SiY1GA2azGS6XC+12G/V6Ha1W61IRuslkgsvlwhdffIHvv/9eVJyFQgEul0siRbPZfMqTnKaL8KZBjzwSieDOnTu4f/8+1tfXEQwGRW2leuQ0VPxZo9GIbreLer2OZDKJXC43NVGm+uwbGxv44YcfMDs7i9FohFwuh/39fWQyGYxGI8zMzGB+fh5zc3NYWFhALBbD/Pw8IpEI9vf3z01vULAxaXgm34f6/6T9qG5ThQxv28vXYnjUA9ztduX/O50OzGazHOhWqyU5gqvSX/QkQ6EQ7t69i/v372N2dhaRSASapqFareLk5AQnJyeo1+u6Th+vLkHm4AKBgHhIdAYGg4FIKGdmZhCNRhEOhxEMBk9tkqvI7QAAIABJREFUwGg0ilAoBKfTKXSG3+9Hr9fD8+fPP/ZjXhrct/zi3mWup9frnYqILiN9ppfIIrzBYIBSqSRGLhAICA2tSrWn5SL8WBgOh6jX68jlcqjX6yJJHw6HIlpSqSD1i45Wr9cTR2JawHtvZmYGd+7cwdraGjRNQzKZRKPRkLIUUo1erxd+vx9OpxO9Xk+CgfP2L4202+1GOByWXA2NyiQlP4lerydChneJGIBrNjzkyvlFVRu5c/7dVSIdlWekNv8Pf/gD/v7v/x5msxnFYhGJRAJHR0fY3d1FLpeTJPjvHWazGX6/H0tLS5ifn4ff75coJZPJwGw2IxqNYmFhAeFwGJqmweFwCGfMDWgwGBAKhWC32+F2u0XzX6/XxbOcJoxGozccpsl9S6fpMjQb8LrrQbVaRT6fF7q50WggEAigXq+LKEbTNNhsNjSbTd3wvAX8vDKZDAwGA/x+P1wuF8LhMAKBAOLxOGw227neNqnVarWKSqWCwWAwVao2i8UCt9sNj8cDu90uTBMA+Hw+BINBeL1ezM3NYWZmBn6/H6PRCOl0GsfHx8hms+fSbGSRmN+12+0AXkc47PRAgQExGo3QarWQzWaRSqWkK827zsp7Gx7K62hcVCPT7XZFtaMe4Muqg5j0pkcTDAYxPz+PmZkZaJqGUqmEra0tbG1tYW9vD4lEAsVicao8mg8Nyi69Xi/C4bDUQESjUYkgWQRGQ0NDTxEB6ScqvuhAlMtl8SanBaQFVNWauoe5byfbrVwUvCRTqRQCgQDW19cRj8dF1tvpdET9oxp2HeeDnxkvur/97W+oVqvwer1YXFzE3/3d32FpaQk2m02iclJvvCDT6TSeP3+Op0+folQqTc2asy6xWCzi+PgYfr9fopJ4PC6SZhph1int7e3h4cOHePLkCZLJ5FtTD1ynXq+HRqMhnR3U+iCTySRnZjgcot1uo1wuy/17URXxteV41EKlfr8veRy2ViBNcZVkPykLFiktLi5iZWUFAPDixQscHR3h8ePHeP78OVKplEhTdb78FXjoSqUSPB4PjEYjPB6PFIqxt56aA1ITiORxTSaTbFqLxYJutysbedITmgaonDSl1e12G61WCzabTQzOVUQqjHhyuRwSiQTW19cRi8Xk4jw4OIDP50O32xVu3Ww2v7fS83MHP7NGo4GDgwMUi0W4XC7U63UsLi5K5bzP5xPRAfNojUYD6XQau7u72NvbQ6lUmpq1Ho/HaLfbSCaTsFqt6Ha7iMVicLlconKLxWKYnZ2F1WpFLpcTFfGvv/6Kvb09FIvFc1kgim1qtRoODg4wGo2QTCaFUrdYLPB6vfB4PGg2mygWiyK8SaVS2N/fx+7uLorF4oVKK67F8EwqpljDUy6XoWma1PLwAS8DeoR+vx9ra2v485//jNXVVfT7fezv72NzcxMHBwfI5XIol8t6Id4ZGAwGItfN5XLw+Xzw+/24c+cOHjx4gHA4LAlvRjis3WECkYaF0knKLQGI/HhaQaUO922lUoHT6ZQOBlf1itV8RD6fR7FYRLfbRTKZRD6fRzweh9/vBwBZ/2lTB34MqMaH1fNs5np4eIh8Pi+siM1mE6VroVCQepZGozFVwiPVkWm320in02IIIpEIbt++LXnZcrmMZ8+e4S9/+QuePHmCg4MDVCqVt+Z4GKEnEgl0u11sb29LFxoWla6trWFtbQ3VahUHBwdoNBoolUo4ODhAJpNBtVqVYONduNZebTQ+rJqtVquyaGqS6jK/U9M0xONxUQf94Q9/gN/vx9bWFpLJJJ49e4ZEIoF2u61Lp8/BaDRCs9lEr9dDpVKRHA0AhMOvGse63W4xPJPtjrhZNU1DJBKBzWaTtVYT79OGSYdJ3bekkFUp7mUNEOm6SqWCly9fotlsotvtCg9OSog03zSu4ccEo1HgdVFjq9XCyckJMpkM0uk0+v0+LBYLQqEQKpUKHj16hKOjI0mCT9N9QWqLNZJsVaMWztbrdezv7+PXX3/F3/72NzE674qk1b3aarVO0Wt2u11yvwsLC5I6qdfryOfzohC8TB70vQ0PW+CwPoGXkVr3wLbZaivyi/5uv9+P27dv44cffsA333wDv98vVjaRSIhyTTc6bweNCBOszWZTcmbHx8fweDynKDbSo0y6U6b5ww8/IBaLSc1LqVRCOp2eaiEHDRC9vkajIXUhpBmuSiWORiPUajVsbm5id3cXw+EQJpNJ8mzs8sF9rO/hi4NKNZfLJUWP7E/GXEgqlUKr1YLX6xWqiurCaTT0dAj5/06nE4uLi7h9+zbcbjeKxSI2Nzfx+PFjHB8fX8joEMzvUHTBc+F0OuFyuTAej0/1GRyPxygWi3KvXCZ6fC/DQ4nf3NwcHjx4gOXlZdTrdZTLZbTbbcTjcQSDQRQKBezt7eHFixdiGS8C1p94vV54vV6Mx2M8f/4cz58/x5MnT7C7uyudCfQD+26QxwVeGaJMJoPxeIyjoyM4HA65YCebATLXEY1G0Wq1EAqFxEtnYXCn0/nIT3d58DmBN/u2Aa8pOP79VdHv91EqleTPbC/f6/VgtVrhcDjgcDhO9cPS8XbwbgiFQrh37x6+/fZbhEIhFItFkVS3223kcjnkcjnJn7GV1rSDfSkXFhawurqKaDSKRqOBFy9e4MmTJ9jf37+U0SFUw0bjQyeU9z0NfLvdlrW+7J59b8Njt9sxNzeH77//HvF4HKlUCul0GuPxGPPz8/D5fHj48CEePXqE58+fS0eBy6Df70sjxWfPnuHJkydIJBKo1WqTQ4d0XBDD4VDWTw2rz8rVcFPl83nk83k4HA6JoFjrMo2Gh+Dz0dCoRYg01u/jHasGn3+uVqsS5VAKzE7V09j37iZBT9xiscDv9+PevXu4f/8+7HY7ms2mXIb03lU5/OdwV7BjSzwex507d7CwsACz2Yzt7W08fPgQW1tbyOVyoiq+KpjjVX8He2MOBgMx6FfJgb431abOLLHZbFJ4p3aqrtVqKBaLwqteFJQQ5vN5/Pbbb+h0Osjlcshms6jValN92X0KYPsQ4LV3o7a7UGEwGKQlB8dOqDTqtFJtaoSjHjB6fjSs1zligw1z8/k8SqWSdE1mb8HP5YL8UKDgBXj1OXEcC2kg4JWTRNXbVRu8fqpg4fby8jLu3r0Lj8eDXC6Hp0+f4tmzZ1Kvcx17SC2XYfG/2WyWnmxXFd68t+FR5aiTHywPLitrL+s1UgacTCYlmcXLTvcK3x9qWH0WJjcV152UkPpZTONFqRYaTr5/VsSr+crrurioUMpms0gkElhZWZGeWqw/m8b1vCmokSibuLIVFwsZj46OkEgkpkq5dhGwtCQajWJxcREzMzPo9XrY3t7Gy5cvpWPLdeaveA5KpRJSqZQIk95nXd/L8Kger3pYeGExOU0q5rJvdDQaoV6vS8iscrPTmBicNpzlSHBkAP9djRimEWotz2S0NxgMpFbpfWmLSZCqODo6EmWS3++H1+vVWz29A3RyyYiwTVa9Xsfm5iYePnx4Sun6OYG5LRaQjsdjnJyciGyaLbCuCzwfFGY8efIEg8EAPp8PtVrtypHkexkeSvDoFdK4ME/AN0zN/FWMBY3a5xImTzPeFSFNG+g107iwLok5HlUBeN3SW/VAUyDTarX0nOUFoPYMozw9mUyKAcpkMqjVap/tOqpdDPb29rC7u4vt7W0UCoUPMnWZas90Oo2nT59iNBphbm5OgoKPZnhqtRpSqRSMRiMqlYr0AyoUCiJtvKrh4evo0HHdYLTWbDZRKBSgadqpNu+84CjZZUeOi5YGUO5LtaBKXVosFoTDYYRCIblEjo6Ofretnq6ijKLjUK/XUa/Xpe7P4/GcanVEOvhzuEfUvPfu7i5arRZyuZzUhXm9XnlutWvM+4A5HnYl4JwqMlJX2a/XkuPJ5XL46//H3nc+N5ml2R/lnKMlW3LCgIEOM/T0zGzVbNVu7dZ+37919/PUht7dGXq6qQaMcZBsBSvnHH8f+J2HKyEbDIZGoFPlItiW9N73vvdJ5znP//6vaFBxKFgmk8F4PMbJycmN5x1XWOFdoKbT2H82GAxwenoKt9stIrR+vx8OhwN6vR5/93d/h52dHRQKBSQSCZyfn0sT6FWgcVlbW4PL5RIhVVJTo9EovF6v1HqWub/kXUCSgDpU7DrECrWPJxwOw+l0IhQKCbu2VCqhXC6jWq3K/Vp240O2XrVaRavVgsViwZ07dxCNRmXsDEVQa7UayuWyRNTvAwqS1mo1JJNJMfSsrX1UOjU/UKlUwqNHj0TLixuHMxreRd13hRVuClqtVkZBsGlWr9fLPCeXyyXD3zhrntNXA4EAQqEQ6vU6zs7OoNfr0Wg00Gw2F9Z91NHhLpcLd+7cwYMHD7C2tgar1QoAkk4LBALQ6XTSTU9W1pfAamNjotPpFIOsTnt9m4OS7Rx+vx9bW1sIhUKYTqe4ffs2MpkMEokETk5OcHp6KkQlAMLQWkYjxFHe3W4XOp0OsVgMW1tbMBgMaDabKJfLyOfzSKVSSCQSAGZrsTyL3wUqKaZcLgN4pR7x0Q0PAAl3V/hwUNM0y/jA/JowmUzY2dmB2WxGOBxGOByWOTkul0sUdykPwsOQ9UoOhOv3+1KvvOxh4+BDHoa///3v8fDhQ4RCIZHsZxqI6TxqiT169AhPnz5FJpP5LDXb2H/DAvn6+jru3bsHn8+H8XiMbDaLRCIhM2au2ufqoDfOnrHZbABeCdsCkIZ26u6ZTCYUCgVkMpmlND68DgCiYel2u0XIlzJM0+lUoiKOORkMBkilUqjVakLQuK6Do+7f98FH17JXtbGYo/3cvbv3ARvlKN0yHA7R6/UWHnzqOOvV2r6CTqfD+vq6jAyOx+MiD6TT6YStx34PyuWoytTVahWZTAYXFxeo1+uXPnhM/zgcDmxsbGB3dxe3bt2Cx+ORvjYquLMNwev1wul0ijrwTYyF/1TACFCv18uhaLFY4HA4RKQ2FAqh2+3i4OAA3W5Xev7ehHmFDTVFScXmQCCAdrsNn88nKtVMe2azWdRqtQ95+TcORi/8Uq+bhpURezgclmjS4XCg3+/DbrejXq9jPB6jXC6jVCr9KjXFj2p4mOowm80wGAwYj8fCGPocHrKbhrqRvF4vLBYLarUaisUi2u32TDqC3cxWq1U8+FardeM04GXEaDSCXq/H1tYWbt26hVAoBI1GM1N8psc4Ho+FEMC6w3A4RKlUeqs6DHvXKIxLI8aDQm18pANG47O+vo5wOIzj42PU6/WlfyY4R4sTMTmiPhgMwuv1Ih6PIx6Pw2q1olarwWazwWw2v5V0ENeTwpaVSkVkn+gsmEwmrK+vw2azyQyZXq+HUCiEjY0NPHr0CD///PNHXJGbAQ1up9NBtVpFMBiUEfU09G63G7u7uzPN/KPRCFtbW5Iye/ToEf73f//38zY8aljM0JC53PkLV/tEvlQwHeHz+XDr1i2Ew2HodDrh6jNfDbzKdUciEQSDQQBApVKRHqubXsf5iPVTB8cT0MCwu53eMqNKXhMjIRonMoey2axEO29aU3rcTP+oqQ1Sgal8PRwOZTAfh3vp9fql7rbn9Xk8Hqyvr2NnZwdbW1uIRqMyUp2D8trtNur1OpLJpFCC33TdvGeclGsymWYam4FXHf5MPXE4pcViQb/fx3A4xOHh4cdYjhsDG0h9Pp+so5oNYQqSNU0Asi7qbK1Go4F0Oi3ziz42Pojhmde7ohVm7tvv98uBpbJamCpSQ2hC7TK/7L2YuvgcQLZOLBbDb37zGwQCAZknooIGyuv14tatW9jZ2ZFxFKVS6Z3fnxtV7cniIU1hy6vSfp8SxuMx8vk8Tk9PEQ6H5XAHZq+TNRdgtpeHHjW11K66VkaePp9PWGtM8aiUahIQSMDhIXodr/9TBY2Ow+GQ9OZvf/tb7O/viyr3YDDATz/9hP/4j/9AKpVCp9NBs9lEvV5Hu91+K8NjtVplABpnSjG6JRjBajQauN1u+T3WKf793//9Qy/HjUKj0cBqtSISiWBzcxORSESuiwMZVeMLQBwco9EIs9ksZ+W7jKlRx4PwjFaNmnp2X3UPb9zwsEBrsVjECwEg87x9Ph/MZjOq1eqMFAlzrzabDe12Wyh6quc5GAxmoiGm7qxWK/R6vfye2tm8jGCKLRQKYWtrC7FYTNiDHCbGG67X6+HxeBCPx7Gzs4NwOIxmsyn9Ju/y3jyErVarHNDNZhPdbhd6vV4GyfX7fbmPnP3+KWI6naJYLOLo6AiBQAButxuhUEjSaQaDQSIegod+tVqVOoDf75chYpc5OHQYfD6fzDChQ8UHnV+qGrjKtuP7Lyt0Oh1cLhdisRgCgQAmkwmKxSJSqRT6/T6MRiPq9TqOj4+RyWRwfn4ugrXqjJ2rwL3PVCbTSWokzp4Tppt4DxiFbm9vC9NwmbCotsNInrVJ1cnhuhiNxhmH6zrno+pwkiRD0QCdTge73S6GnxJpVzGZb9TwcAHMZjPcbjfsdrt4bzzIDAaDjExVPUjmJUldJb2VhwNHEtND7HQ6ssH9fj/MZjOKxaIUy5jTXcYIiDcyFothc3MTdrsd6XQayWQSuVxODnka82g0ijt37iAUCmE8HqNUKklT2XWvnxvMZrPB5XJJ7pheO9N/oVAIvV5PvKtPWSSUatDJZBJut1ty/oFAABaLRTxBdeYOH8xGo4HpdCrU62w2i2KxuPBwVD19vo9aKwJeEUBUJWxGO5wr43a7YbFY3un+/dpQU2zb29vQ6/XI5XIolUpIJBLw+XwwGo1oNBooFArS/c6D6m0OQ66ZzWaTCcedTkfYiFqtVgyYKihKx6Lb7cp7LhvULAR1MPv9vjDaDAaDRD7qFGHglSJ9u90Wkszb9IypzqjD4YDD4RDDz7Pd6/XC5/NJM3SlUrkyA/VBUm2qAeKAJuYi2+02KpWKLJgaqnHEajAYFA+F7CPWNRwOB0ajEc7OzjCdTrG2tob19XVYLBZks1lkMhl0Oh1UKpW3zhd/SuANdTqd2NraQjwel0Ytjvpl0dpqtSIcDuPu3bu4c+cOJpOJeJFXMa/e9N5Wq1WcBJPJBJPJBI1GI8Pi3G43nE4nDAaDSCIxvfepgk13T58+RafTQalUwr1797C3twe32y0pCFWOBQDcbjc2NjYwHA7fSgbeYDDAbreLei9TIDTMatMk8KqOaTabEQwGsbu7i7t376JWq6HVai1FDU2F+ty7XC6JiqvVKpLJpBh4qn53Oh0hyrzNc0rD5nK5sLm5id3dXakXsVamptpYAwIgDuv5+TlevHghNbtlAdO4gUAAW1tbCAQCUselc0hxW9YJuc84Wubk5ARnZ2c4Pz/H4eHhtYwvz2lG51qtViYXcxCoXq+f0Te8bP/euOEh06TT6Uhu0Ww2z3gb9Xpd+hRoWNg4Nx6PhfnCoplGo5G0HFM8brcbo9EIa2tr0iPhdDrh9XrRaDSQTCZlDANTG8vSEU4jTFrkeDyG1+vF5uYmzGYzer3eTIptb28PoVAIL168QCKRQD6ffyc1cIJr1ev1JG3q9/vFgwIgIxKWhZHI5jlOpOQhFY1GF/48jY/NZpP0JY0x89yLDkquG7vGPR6P7GF6gBQkpWdOIV2OaWamYNnSbRwUFggE4PP5JGKs1Woin0WoRIvrTA/WaDRSQ9vc3MT29jY8Ho/sTb7WfGGd6fpGo4FcLoenT5/i+fPnS9V/qDY97+zsIBKJvLZPeK2kr3OfMtrhPLNMJoNqtfrWmQqm90hgcDgcIk1EZ009D950P2/c8LCrlrm+RqMhzXcWi0WKtGycs1qtM15QoVBANBqF3W6Hy+WSDeTz+WAymaSo7Xa7MR6PZQ4QAJlI2Gq1oNFoZCQzH3KVCfYpQx1RTUNMYkapVJJaCx8+m80mkxZTqdQbQ2huTmCWtMHN1Wq15D0AwGq1IhgMwmQySZ2JRWB6rcsQVZIa3Wq1UCwWcXFxgUqlgkAgIHVGdW2AVx620+mE3++XqK/f77+2xvSoM5kMXrx4ITRXGhIejlxnnU6HXq+HfD6PXC6HXC6HFy9eyPTIZXGUgFd1SZ/Ph9u3byMcDqPVaiGfz0vD7TxLT+1HeZvXVx2y9fV1RKNRhEIhYQ+SDAK86hdUe9qYbalWqyIrsywRJTMRgUBArtvv98ve4pkBzKZzqRxDg5tKpWT8N1O5at1x0T2hQe/3+2i323Lu2mw2DAYDFItF1Go1SZsymr1q/96o4VG59WQ8TSaTGWkMFl9ZA3K73ZKySaVSqFaryGazcDqdGAwG4j05nU5YLBbhozPiYXGWi8GF5gPNtF273cbR0dFNXu4HgeqdHB8fS8qNxTsW9YxGIyKRCDY2NiRnm0qlZjbUZWBeFsDMgUBvnL/L2gO9GDY4FgoF1Ov1pZRC4iHUaDTEiDabTYlKWA9QIxrW3Hw+n9RfLhO95QGn9qep/UBkXTEFV61WcXh4iMPDQySTSfmiIOOyQKfTweFwIBKJiOHJZDJC+CGTTJ3fxTqsuv8WGSFVky0UCuHu3bv4+uuvsbe3J2w2lSHIe8c/O50OisUizs7OZK2z2SyazeZSOEwGgwE+nw/xeBz7+/vY399HNBqFx+ORnsh5qPucjtDjx4+RTCZRq9WkzMF6mdrczD4odUAizyV1zI3ZbMZwOESz2USlUkGj0ZDz4E3N6x+kxsObqcpU0PhQmNHhcIhRmUwmyGazqFQqMjmw1+shHA7j3r17iEajcLlc0nwKAHa7XXpUeHjSKxoMBpJjpCDj2dkZksnkh7jcG8d4PEatVsOTJ09Qq9WwsbGBYDAoD+9gMIDD4RCdqnw+j2fPniGZTIpHMg8eqkxr2u12AJBoVA2Z1QgIgCg4A5BIR924jFaXBeo4gvkhhYvqL6Tgss/G6XSKAvt8QVwtfJNgQAPDfDr7ThgdHRwc4OnTpzg+PsbFxYUovC9TxENyENmVHFJGrbtarYZeryd7jQcWVQpU5Yh5MOr0+Xz4+uuv8Yc//AHffvstvF6vvLeaUgNeOXBM9x0dHeGvf/0rHj16hFwuJ4ysTx3s0YvFYvjuu+/w7bffiiPKtO/8uHoa9+FwiHQ6jZ9++gl/+ctf8Pz5c5TL5ZnxNUztWiwWaLVaKZOwPqPeD6aYOT2XtctmsymOFn+eP3vZufDOhoc3mW+wSPtHpfnZ7XZ4PB70ej10u104HA6RtBgMBqjVamI08vk82u02yuUytFqtiDja7Xasr6/D4/HAaDSK58SRrPwser0eoVBIoiEOL2NK7lMHw9pCoYB2u41cLgeXywWLxQLgZepre3sbwWBQ+lOOj49RLBYXkilobNgjolIfmYNXvWs+vExtmM1mGXPrdDpntMo4QqBQKHy8BXpPqHtXZeyR3ryoXsDx7n6/H9FoFGazWSRH5jW/+LrMe+dyOZycnCCbzUKn0yESiYiaMAeYmc1muFwuVKvVhU3VnzoY8YTDYayvr2NzcxO9Xg/BYBC3bt0S4875Wv1+fya1mM1m5cCcB88Ru92OSCQiRo0eN4CZNOa8pz4cDlGv15HP53FxcYFSqbQ0Rp2GhxHP5uYmwuHwzDMKvF4zA15F3+VyGdlsFoVCQc4Hnocqe9VqtQorrVaribMJvNIg9Hq9sNvtkoZnIMH7y540GsVMJrPwut7Z8KiidJRoYX6PkQ5TNaSsMgfb7/clL8kDgGBto9vtotFoyPf8fj8CgYBENqoMCQuO/D8AkhLp9/toNBoIBAK/Wpfuu4Be+WAwkHlHXPO1tTVsbm4CgBieTCYjEwHnwU5mv98Pt9stm2wwGMBoNEqEyOiQhUn+js/nm6FVc7ONx2OZdb9M7CA+zHRE2EzHLzVVo6Ydqb/W6XRQLpeRSCQWyhJRKSGTyQhp5ueff8bR0RGMRiP29vZw7949mWfidDolXdTr9VAoFJamHqlCPRO4jhwxQa+aqZzhcIhUKiUNpUzzzhsf1jaCwSDi8Tg2Njbg8/nkYKOjwP4VNV3HQ5g1Ds6OWZbUsEajgdfrxcbGBjb/f7Oox+ORdWQbCoCF6S0aHkab6tqQCOJ2u6Uvz+l0ShTIOg1/lioQ0WhUetOsVusM+9hut888SwBuzvColDqqwhoMBnQ6HdRqNaHxkoHBWk6v10MikZA+m/X1dXi93tdolAyR+ZVOpzEYDGC1WuH3+2WuOhlBer0esVgM8XhcPCDVU+XNmO/TWAaojXAARCmZRVybzSbFwnq9fqk8jrrJuMFolFlPYlMkG3nZAMnOaH4O9vBoNBqRIAGAg4ODj7o27wpGxJQUyeVyKBaLiMViMBgMM4woNfLRaDTweDzY39+H3W7H6ekp6vW6OFQEHYbz83ORZOH7lMtlSVv2ej3s7e1hc3MTTqdTiBrLuE+BlwcfHaSzszO0Wi0kk0mMRiPEYjHcuXMHbrdbflZN2/AMYF2YESQdpvX1dXz33Xf47rvvcP/+fZGPUlO9quev1udIo08mk0in00ulSG0wGPDgwQN89913+Prrr2fEZgHM7FXg1X5VU2YXFxc4Ozt7jaxC58vlcknjPp13h8MhvZTzZwKdV71eLwMRvV6vSHWxt0hlLi/CtQyPRqORB8Nut8Pr9SIUConHS++CG8bn88Hr9UrXPVlEvDi1wXM+8gFeNTyxOzafz0vekhvX4XC8Rs2mUCaNExVvly19MQ+uezAYRCgUEtHQN10bD8/RaCRFQZfLBbPZLGQMjrbQ6/UyGZPpvclkIrPcA4EAwuEw/H6/pAF5oC4TmIJlSoHpMXWWFIAZo8+eBZPJhMFggEQiMdOwqL5uoVBAtVqVngeVdDMajWA2m7G7u4t4PA6Xy4VCoYCzs7PXnoFlwWg0QqVSwdHRkVDwj4+PMR6PEY/HUS6Xsb+/L8xANstGo1Hk83npAVnEKKTBf/jw4UyKjYcs10yt8ahstlarhXK5LMXvZYFer8dv8EQYAAAgAElEQVTm5iZ+85vf4O7du/B6vWJw1WzPIkPK/dZoNETYdv4Z5ZlANQk2PDOlxohJp9PJYEQGBgBkvLjf7xcpqlarhXQ6LY2ql17bdRaCBUS1O9vj8QhfnCkyAHC5XELBrVarwiDqdruwWq0SiXDzzNNYCTV8HI1GODo6wsXFhVjhcDiMwWCAk5MTTKdT+P1+fPPNN1I7GgwGqNfrePr06VJx9heB67+2tiYaYCzWMq+6iBk0Ho+FYm02mxEKhYRR6HQ6MZlMYDAYUK1WAbykrmu1WuTzeeTzeQBArVaTvqpIJAKTySTp1WQyuTSpoel0KmN8LRYLtra25IEjY5Id2UxlqOkJ1np8Pp8IUPL35mnprGmoNR/VANFJstvtwqxbVnCPHR8fo1QqCVN1PB4jmUzi+PgY9+/fxx/+8Ac8ePBApG4MBoMYrEKhMBO1c63IXlWVvlXlB76/qh1GqAXyZTPqdPSpuq0yzdQUm6qNNk+wmG9YJpgO5plAKTM2+jOTxRS61+uFRqMR2j/w0vCwZsx+Td6/arX6mq6kimsbHnrKJAsw9cO0FiMXVamgWCyiXC4L40rN6XKBrwIXu9/vo1QqoV6vQ6fTodVqYTgcSiQ0Ho8RCoXQ7/cRCoXkPThaeJmYV4vAdfV6vXA4HDCZTFL7ajabQm+er/NMJhOpF1H12GKxSChNthG9J75WtVoVz7Lb7cLlcgmxgOAhvkxrS/LGZDKRfjFGLurXoihOTTOrsjiLSB30DtXX4esydcmUCBtIietqaf3aYKRHopAq+DudTlEqldBoNIRKHovFZCbOyckJzs/PJaqef01S39PptBCTWMNVZWHU7AkA6THJZrOiBbdMa8paSbVaRSqVkqZN1ljUawZmVf1ZYywWiwsZkiopi44T2cesw9FpUtPDTNnxGfJ4PK8NRuRrX+XoXzvVRpYTpSm4sRiB8P9IIWWko86PIQ2PlFY20r1Jq4m/x+5ZdolzcWhp6/W6WGEWwdm0uuxQc9kGgwHBYBDhcBilUgnVavVSI87NwpQIjQ8AMVb0uBm58kFlHp1RAfCq+MifWybQCyRF2uFwSJpHjb7nqbnqGvL3SWunOO0izNcwh8PhDNvIbDajVCqh0+kI24jjEpYN6lqpSheTyQTpdBp//etf0ev1cOvWLej1eqRSKTx//hy1Wu216+Va1Wo1nJyciNO1vr6O7e3tGUow7x2NEHtO0uk0jo6OhEK9LGw24KWj3+/3JRrkDKNgMCjn73xEzrOXUWQymUSlUllodFUlDU4lZVpYZaipvZE8o1krVSNO6t+9jWL9tQwPD612uy1hFsNAfp+psX6/L93BlLsh6Im8ePFCQrlqtYpCofDGpjn14ab0ieodsrmJn4kdzeVyeekNDw3o2dmZeD7n5+fS+/EmUVQ6CfSAVCo8AFmz+Q2m0jljsRjsdjsajQYSiQSOjo5QqVSWam1ZO2Au2+VyyeFFY6OmL0ajEcrlstTSmDvv9/vwer1wu92veeuXgam+SqWCx48fYzgcSmqTWoSRSATj8XjpmkjnMc+wqtfrOD09RafTQTKZhE6nQ6lUklHfi/YQ9/zTp08xGAyws7MjzFcOPmR9QZUyUp0GNqvOKyd86uj3+/jll19ECDgej4tmoMlkkho7m8FVp5MGhanKq5witd6pOrbA7JmgGhOO3Y5EIjJKvtFo4Pz8/K2UN65teAqFgnjAtLgs5Ku6QOTqL6Iv0vC0222cn5/Lhc2ncd6EReqn3Fws+LLRaRm9x3mMRiOUSiUcHByg0WjAYDCgWCwin89fWkBcBDoG842SZKhxM/Lvao9GJBKB3W5HJpPB6ekpjo6OpDa0LGDkzsmYVqtV0pHqAUjPvd1u4/j4GMfHx/IAsoMbwGvMtqtAQ8YG4VwuJ3N7jEYjhsMhHA4HbDbb0q3rVeDhViqV0Gw25bmnl3zVc9/tdpFIJGTPdjodiYCCwaDUy6iaDLw6eK9SSP7U0e128fjxY1itVhEIzuVyIqa8ubkJjUYDu90u4qzMSKiljKu0BQlmk+bB/1PPBOAV0YkNwlarFblcDufn5zKs8kaVC1TRzUVfvAjVY1x0kfSqmQecl2t5W8y/Pr0hevKXfYZlxGQyQavVEoE/rVYrh+V1R0DMbyT+3yKwAa9Wq80YdR6+y+SVq5pf1AkEgFarJeMk2IFNp6nVauHJkyeibM10G/BybTjt9W2hMo76/b40RzNtQkLMMqWF3ga8ttFoJGSUt3k+6RTk83lMp1Nhzm1sbGA6nUo/STweh91ul1Rbo9HA2dkZEokEms3m0hkg7r1+v4+TkxNUKhXRqtvZ2ZE+NPbcRSIRuFwudLtdFAoFJBIJlMtl2fNvuv7L2HGLwOZzrqt6JrzJkQDesYF00aEFXG9ktcrQuEnQo/wcQa9RNdi8Fx/SuDLPXq1WZyKlZcV8zWo6ncqohPPzc0nhMk3RbrdxeHiIZ8+evTYdk6mcdzESdMCYwubr8TWX7aB8W7zLdU2nUxGvZbuG1WqVVgmbzSZsTeBltNrpdJDL5XBxcYF2u7206zkcDlEqlaQVJRqNiggrGb4cQ2Gz2WQUBfv75vt9bgKq6sm7ZJM01zmwNBpNEcDZtd/l00F8Op0Gfu0PsQirtf1wWK3th8NqbT8cPoO1BS5Z32sZnhVWWGGFFVZ4XyxXR9UKK6ywwgpLj5XhWWGFFVZY4aNiZXhWWGGFFVb4qFgZnhVWWGGFFT4qVoZnhRVWWGGFj4qV4VlhhRVWWOGjYmV4VlhhhRVW+KhYGZ4VVlhhhRU+KlaGZ4UVVlhhhY+KleFZYYUVVljho2JleFZYYYUVVvioWBmeFVZYYYUVPipWhmeFFVZYYYWPipXhWWGFFVZY4aNiZXhWWGGFFVb4qFgZnhVWWGGFFT4qVoZnhRVWWGGFjwr9dX5Yo9F8tHGlOp0OJpMJWq0Wk8kEWq0WWq0Wer0eOp0Oer0eRqMRRqMRer0eGo0Go9EI7XYbtVoN3W534Yz16XSq+VjXcB18zLWdh1arhU6ng0ajwXQ6xXg8fqf59Ku1/XBYre2Hw5e8tlqtFkajEQaDAQDAidR6vR4GgwEajUb+rdPpYDAYoNfrode/NB3D4RDNZhOtVgu9Xg+j0Wj+LUqLRl9fy/B8aPCCLBYLdnZ28Pd///fw+XyoVCool8uYTqdYW1tDJBKBw+GA0+mE1+uFy+UCAKRSKfzP//wP/u3f/g2Hh4cYDAb4kkZ7azQa2SjAy0101fVrNBoYjUaEQiGsr6/D5/NhMBggkUggl8uh0+ks2kgr3BDoTAHAZDJ54/363KHuXQBf9Fq8K+icA8BoNMJ0OoVGo1noSGo0GoTDYfzjP/4jfvvb38JqtaLdbqNer8Nut8Pr9UKn08l9MBgMsNlscDqdcDqdsFqtKJfL+Nvf/ob/+q//wl/+8hfk83n0+3313p0t+pyfjOHR6/Vwu90Ih8PY2NjAnTt38PXXX8PpdKJWq6FcLqPf78Pj8cDj8cBsNsNsNktEpNPpZFEsFgv0ej2Gw+EXs3k1Go14KXq9HpPJBP1+XzbfIvD/tVot7HY7tre3YTKZYLVaYTAYkEql0Gq13in6WWExeLhqtVpYrVaYzWZMp1MMBgN0u90r79fnDDWbwaj7S12Ld4VGo4HVaoXf78d0OkW1WhXHcTAYvOZE0vH0+/3Y2tqC3+/HYDBAo9GARqOByWQC8NKADYdD+XmtViv3aDweo9vtotPpYDgcYjwev9U9+2QMj8FgQDgcxrfffov79+9je3sbPp8Per0eNpsNHo8Hg8FAwj1+8UHW6XRwOBzw+XxwOBwwmUzo9Xq/8lV9HGg0Guh0OlitVtjtdhgMBgwGA9Tr9TduBNWb8fv9cLvd6HQ6KBaLyOfzYthXeHdotVqYTCaYTCbZszqdDl6vF263G6PRCLVaDblcDq1WC+Px+Nf+yB8N6vNrsVhgMpkwnU7R6/XQ6XS+qLV4X2g0GtjtdmxtbcFoNCKfz6PRaKDdbqPRaLx2Fuh0Ouh0OoxGI4zHY1itVrjdbtjtdjEik8kEo9FIyhkscej1eozHYzQaDWSzWeTzebRarbfOkHxQw6OGzvz7ZekEg8GAtbU13L17F7dv30YgEBDvh8ZmOBzKa7Emob72eDyWKGc+7fS5Qb12nU4Hs9kMn88Hr9cLrVaLRqOBbreLfr9/peGg5+10OqHT6dBsNpHJZJDL5aROtkqBvB9MJpOkiJm+AIBQKASfz4dWq4XT01P0ej1Z889tjdVIb9G5YDKZ4HK5YLfbMZlM0Gw2MRqN0Ov1Pru1+FDQarWw2WyIRqOIx+MYj8c4OjrC06dP0e12ZxxxjUYjTlCpVEImk0EgEIDP54PBYJhJ0Wm1WqkB0UEwGo1ot9solUrIZrMolUrXKm18EMPDsJnFfz5oDMt4GGo0GtmItLzdbhfNZhMmk0leg9ENf5bWl+/FRWk2m6hUKmi3259lmM7NwloY6wN6vR52ux2BQAButxvdbheNRmPm9xatBUNnj8cDv9+P8XiMUqmEs7MzydXyXqppEIbe70pC+JLANObt27dx7949eL1euR+BQABOpxPZbBbValX28ecEEld4FvA5nk6nMJlMsFgsGI/H0Gq18Hg8cLlc4gBZLBYUi8UvLgp8FxgMBni9XsRiMdy+fRtfffWVnJupVAqFQmHmHKBj3u/3kUqlcHR0hHA4LOcuv6+eu/w9s9kMjUaDZrOJfD6PXC6Her1+rXrwOxueRV4wDYnJZILdbpeNZDabMZlM0Ol0kM/nUSwWMRwOhbnG6CWRSGA4HCKdTmN7exvhcBihUAh+vx86nW6GgaEaHqPRiF6vh3q9jnK5jG63+9ltVBpcq9UKh8MBm80mRUSj0Sj/DwCtVgv1eh39fh8AJF02b3zovQQCAQSDQUynU1QqFTQaDQyHQyF6OBwOSV9OJhO0Wi1Uq1W0220MBoOV8bkENOw+nw/7+/v49ttvodPpZH/6/X6YTCakUik0m03xGD8nh4l71u12w+v1wmq1Sg3S6/VibW0No9EIrVZL0uVOpxP9fh/pdBoHBwc4OTn57J7nm4bVasXu7i6+//57fP/999je3kan04HT6YTRaFx4Xg+HQzQaDZyfn8NqtWJtbQ02m02yTazz8LydTqfiiHa7XdTrdZRKpZkU22VO7jyubXjoadMQ6HQ6TCYTSYNZLBa43W6EQiFEIhH4fD4poNZqNQBArVbDaDSC0WiUA208HqPZbOL58+doNpvodDoAALfbLcaMXpLqGZIZxJSRz+dDJBJBs9lEoVD4rIwQPRCmxmw2G4xGoxAtxuMxarUaGo2GbAaj0QiTySTFaxoJGjLWz1wuFy4uLlCpVDAajeBwOODxeBCNRhEOh2cciEKhgNPTU6TTaZTL5S+OPfg2UAu3a2trsFgs6Pf7GAwGqFQqQiQIhUIwmUxwu92wWCwS+X8O66nuMb/fj/X1dQQCAVgsFgCA3+9HNBqdSa3Rier3+zAajSgUCjg7W0iMWuH/g3stEAggFoshHA5jMpng9PQUZ2dnqNVqrz2jzFywjpbNZnF+fo719XVxiADMnLdqCWP+vtZqNTlv3wbXMjxarVYMBcM4o9EoHotGo5HDan19HRsbG/B6vTAajeJNN5tNXFxcCAstGAzCYrGg1+uhWCyiWq0ik8nAZDIhGAxibW1N8o6MdpjyoRUnq21tbQ0PHjxAr9cTYzgejz+rPPFkMpEUl16vl2hEr9ejWq2i2WzKoabVauF0OuF2u9FoNGaiE3ozDocDbrcbJpMJnU4H9XodZrMZfr8fu7u72Nvbw8bGhkSmOp0OuVwOFosFg8EAzWbzi2IPvi20Wi3MZrM4YK1WCwcHB2i1WsjlcqjVatjf34fBYIDb7cb6+jpcLpcQQz6H9WRaxuPxIBwOIxqNIhqNwu12w2AwSDsEjS2L3NPpFK1WS9JtTCmv8Dr4HDudTng8HtjtdvR6PZydneHPf/4z/vKXvyCTyaDb7b62p9QsSL1eRzabRTabFUfJbDbPsGSBV8bHbDZjfX0dg8EAnU4HvV5PenluPOIxGo24desWHA6HGAGz2YzhcIhyuSzMtGAwCL/fD7/fL4aKhyCNT7vdhsPhQDAYhNFoRLVaRa/Xk++n02kYjUY0Gg1kMhncvn1b2BoM5+YXgxFPLBZDNptFJpORFMbnEPWwRqbRaDAYDKRhazqdwmg0olaryToaDAYx3oFAAFqtFpVKZYagwUPBarViMpmg3W5Dp9NhZ2cHsVgMOzs7cDqdGA6HKJVKmEwm8Hg80Ol0CAQCcLlckq9fYRZMmZEYo9Pp0Gq1kEgkkE6nMRqNEI1GpVeq2+1+NvsUeOWFB4NB7O/vIxaLIRAIwO/3w+l0wmw2w263w263i2PK2iENL2n9nzNJ6H1hNBqxvr6O3/zmN/juu+8QDAZxdnaGo6MjpNNp1Ot1cUQXgeter9dxdHQErVaLbrcrzGKHwzHj5Kt/t1qtCIVCiMfjSCaTODo6kjT9m86Eaxkek8mEnZ0dCZeNRiMsFosYHqPRiHA4LBuLX4yQLBYL9vb2JEdosVjg8Xgk1CN9t9/vo1Qqod1uI51OI5lMYjAYwOFwSDpCtdYqg81iscDn80ltqNVqAQDa7bYcussIGtperyeRRrvdljWwWq2SXmMKjn1RDocDzWZTSB7AyzXj+pvNZvE2XS4X7ty5g+3tbdhsNhSLRRwdHSGZTAIANjc34ff7Jdz+3GoSNwnmw202m0TtNCxerxc+nw9GoxGVSgXpdFpSIstWM1NJQvw7o/FYLIb79+8jGo2KF202m2GxWGCz2YT+z5rDYDCQvQWsGJRvgtFoxMbGBr799lt888030Ov1ePLkCZ4/fy4F/zet4XQ6RbvdRjKZRLPZRLVaxWQygd1uh1arlb5I3l/19UwmE3w+H/x+v9zLt3nP60rmzJAGrFYrbDYbtFot1tbWJBVHo8QaEBsbLRYL4vG48MS5wTqdDvr9PqxWqxTCh8OhhN96vR65XA6VSgVer1cYVvN0anpZPHDX19eh0WgkfVcsFq9zuZ8ceMgz3UYvRq/XS/TCYq7H44HX64XZbEa73X4tXcEQ3W63w2w2Q6/Xw+l0wmQyIR6Pw2Qy4fT0FAcHBzg6OkKpVILNZpspDr+Jqv0lgwZnY2MD+/v7CAQCqFar6HQ6Uut48OABfD6fEG46nc7SrScjZzIfyURlkXp7exvr6+vCmFJZbhaLZeaZJ9N1Mpmg2+2iVquh3W5/NlHgTUNtpbDb7VLLrdVqyGQySKVSUrN9E5hNqVQqsFgskg5mGh7AzBnCyIf1Sb/fD4/Hg0Kh8FbO07UMD+m2er0eo9EIg8EAAOB0OhEOhyUnqDIh+IFpLFhXoFEZDocwGAyoVqswmUwzIR3fczgciiHinyqpQP0dGp6NjQ10Oh14PB6k02m0Wi2USqXrXO6vCjWK41qoXsT8pnO73cJsczqdCIVCCAQCGI1GC4uzNPq8VzRYJIe0220kEgk8f/4c5+fnGA6HcnBQE+8yPbwvHfT6LRYLQqEQYrGYkDPG4zEGgwG8Xi/W19flHqpU42VaU71ej7W1NVitVrkGq9UKl8uFcDiMvb09rK2twe/3z5wJ1Fa8uLjAYDBAv9+XXpNut4uLiwskk0lUq9WV4VkA9uxQ6cXpdCKfz+Pi4gJnZ2e4uLjAxcUF2u32tfYT9+dgMJjpiWR9F5jtxeR5G4vFsL+/j9FohFQqJSn/y977Woan1+vh6dOnSKVSUtT2er3Y3t7GgwcPhLrHnDbBD65GQLwwMn3Y7zOfz1UvmuEeL1x9XUYCBoMBdrsdm5ubsNvtSCaTmE6nSCQSS5MrVgVRtVqtSFOoncf8vslkgs1mg8PhEEPACFP1MBdBXVu1CZcHAY08HQa/349gMAiz2YxCoXDtTf0lQo1QdTod7Ha7aFlVKhXo9Xp4vV7s7u6Kh/q2siOfAsxmM+7duwePxyM9IHa7HU6nE8FgEJFIRGq9TJ8BL2VYkskkHj9+LM3KvV5P9hwdxVKptNILnAPbIKLRKH7729/i4cOHWFtbwy+//IIffvgBT58+xcXFxTtF0DwTVDIXz3PeP0an/Hmn04k7d+7AZDLB6/Xi0aNHODw8RKFQkJaOeVzL8IxGI6TTafHQzGYzXC4XWq0W7Ha70J1Vy8h8HzngPp8PbrdbPPrRaDRTu1AXig9tv99HrVZDqVRCKBSSorZK61YXgr09ZNzl83nY7falMDwqZdput0On06Hf76Pdbgv1kd40KdXUZarVarLZPB7PDP3xMmYQv6fVatHr9VCr1ZBOpwFAuP1ms1mYV263W/j7zWZz5Y0uAHWsms0mjo6OYLfbpUGX6Ta9Xo9AIACv13vpw7kM0Ov1CIfDCIfDkra12+1wuVxwOp3CtqzValITBIB+v4/T01O8ePFCIpt+vy9Gdzweo9/vr7QCFfBs8Pl82N7exubmJnZ3dyXiKZVKODw8RCaTeWenkOdtpVJBqVRCLBaTjAizS2pfIAk0wWAQBoMBnU4HpVIJhUJhppdwHtcyPNRQGgwGcuhXq1VJw+TzeWGd0cubj2ru378vNNJer4d2u41yuSy6QvOLRdntTCaDZDKJeDwu1pfpu/l0G//kgUkq56feGa42iXq9XgQCAZjNZjQaDRSLRUlvmkymmRoOv18qlcTYhMPhGU9xkdEdjUbysPP7rVYL6XRa8rZmsxm9Xg8+nw82mw3tdhsXFxfIZDKiBbfC6xiNRqhWq/jxxx+RSqUQDoeh1+tRKpXQ7XbhcDjw1VdfQavVol6vI51OvzUj6FOC6lyy4ZgpW17v6ekput3ujBI3xSgrlQrq9TqKxaJQcXn9/LmV4XkJ9irG43H86U9/QiQSkWiE2nZkRr7rmo1GI2EVJ5NJ3LlzR1LAAGaCCjUyJ6uY5xbVq1UFFRXXbiCdD7P6/T4uLi6g1WqRSqVmulwXkQRqtRouLi5Ee43W9fz8HOVy+bWwmq+jagaxLqGmieYp1vQO3G43tre30e128ec///m6l/tRwaIsOfk8+A0GA/r9PjqdjnD2eXNbrRYajQby+Tzq9br046iS6ItABYJMJoNoNAqr1YpwOCzhtcvlgsvlkkIvqe6np6c4Pj5GuVy+Mof7pYOK0+VyGa1WC+VyGXq9XliW4XBYen2y2ezSkgt6vR4ODg6Qy+VEYJIRj9VqRaFQwOHhoVw3e/B4NvR6PZRKJdTrdTG63Lfzh9uXDqbYPB4P1tbWsLu7C61Wi3a7jadPn+L4+Bi1Wu2d2btqWni+tqPW69WIh4QQnrdOp1OIBjab7dL3ei+tNm6SVqslBS1GICr4IbVaLarVKo6OjmY2YK/XExrfonwupTSYRwZmU0SqnAMNFBeKh+idO3dgt9vf53I/KDQajagI8MvhcMBsNgMAut2uMAFdLhfcbjcmkwnK5TIKhYI0iDKdQWM9Ho+FCKA+wOPxGPV6HWdnZ/B6vZK6o0gja0c8IBjGHx0dIZ/Pix7eCleD+1tdKx7O7HXL5/OvRe7Lgn6/j6dPn86w1CwWC+x2O2w2m/ThdTqdmWeWIJuq1+stjJ5XRucVaAjG47GoX7jdbqTTafz3f//3DIX6XV9fNSA8F9QasTowcp5VzPqez+cTlt1luBGRUEovqOH0Im0g9ug0Go2ZuRvk719Gz6VnaDKZxHtk6kLViFOVDICXabpisSiNVGThfYrQ6XQS6VCll0whkicozse6T7VaRbFYlGtTjQ0ZgJVKRYyF6glNp1N0u13k83k8efIE7XYbkUhEuudpxFutFgqFAlKpFNLpNCqVykqu/ppgjwrwarIuayAejwdbW1soFArI5/NoNptLpbRBx1Or1aLVas1Qqo1Go/ScqVpe84xNEmdWuBp0YsgyzWaziEajSCQSODg4EA3M99k7pLqTOEa6OzFvbIBZlhv7swwGw5XR+40YHm6eN0Ht0ZkfJkSSwSKMx2N0Oh0pUnq9Xnk9tRhZq9XQarVkEbrdLg4ODvDs2TPpCfhUQT06SsHzZhuNxpmZGNx8lUpFjI5qsJmjzefz0Ov1yOfzSCQSC9OY/NmzszOZq8G6EQDppahUKrJ+w+Fw6dJBnxpUwky9XhdqLAcbvq3Q4qcCPn+qZuN8+nvR9Szbdf7a4DmYzWaRy+WQyWSwtrYm5YubaG8gvV8VdWXDvkosIAaDgYgF8/NxMBz1Nhfhow6CU/XTVFr0Vd3v0+lUpLu9Xi8ikQgsFosQBVjLaLVaODo6wsXFhbx+q9XC48eP8fjxYxHO/FTBelm9Xpdwl18EoxnebBYT5zdCLpdDv99HMpkUWupl+l/cLFSqTafTElqTzMCvVb79/TGZTNBoNHB2dga32y3PA43/Mq/xdVUslvU6fy3QYeGzTMHZ4XB4Y2PqVVYtIxe+t1r/YbBQq9VwcnIibQCj0UgMY71ev/R9PvoE0reNjlSQmlqtViXlpNVqRYG60+mg2WwikUggkUhI/rPT6SCRSCCZTF6aQ/5UQIVe0kfVfLha3GP0owoqqhiNRjIEjo2+b1IYYNTZ7/cXpkhXB8TNgczQcrmMRCKB6XQKg8GAUqk0QwpZYYVFUM8wRhfAzRlxGrd8Po9kMolAIIDpdCpnClmMPDOy2Sx++uknZDIZiWzJ0vxkIp53Ba0wDQ4Lkvl8HoeHh5JuKpfLyGazePr0qTSedbvdpeiwp8GYZ6OpTD6z2TxDZ+SfjEzUpk81qnzba/8SjQxllhhFq2wqrqNq6G8KrL+x4ZnkkU99n74tjEajCIACrxqT2Y7xpe2zD4WbXEfWIuv1OpLJJA4ODhCJRDCdTpHJZGS8NckyquFhpgmAKJtcFYEtheEhq83lcmE0GiGXy6FaraJcLqNSqUjBnZY2n8+jUqks1QhhtQDNfwMvmSIejwc7Ozu4e/euqNuPC6wAACAASURBVEMTk8kE1WpVpghmMpkVzfktwFqKwWDA+vo67t+/L3RyEmCo9l2pVFAul9+LMTQPzqufTCYitLsMDtJVUNlN6+vr+P777xGLxcRjzmQyODg4QCaTWen8fYJQ7184HMatW7cQCARE3Pbi4gLPnz+X1D2ZsezpWyTtdRlu1PDMM1YAiDgoAPF23sZzpDGhFlA8HsfW1pY0MVJYlNpkAJDL5eT9TCbTUrGDFoEifKFQCA8fPsS//Mu/IBaLzdAUh8MhUqkUHj16JDTLz0le/0OBqUwqrv/TP/0Tbt++DYvFgmaziWKxiGw2i2QyidPT05nokb1N76N2ztfgePF3SUF/StDr9XC73XC73TCbzXjw4AH+9V//Fffu3ROP+eeff0a32xX9tasIRSu8HxYxfa9aa46U4XC3P/7xj/jTn/6EcDiMdrstYtD1el16LgeDgWRbrnsfb8TwqPo+qny2RqOB2+3G1tYWptMp0un0W0utcFCc3+/H1tYWvv/+ezx8+BChUEgmmjLPqNfrMR6PYTAYZLDZkydPZADSMoN9SMFgUAbskWINQA6/i4sLeDyehWNuV3gdrKPp9Xq4XC5EIhGEQiExRiR2DAYDGfNtt9tlAN7Z2ZnIx7/pob7qM3wuB6/NZsPDhw/xxz/+EYFAAJFIBPfu3UMoFJKitNfrhcPhgNVqlZriTUY+l1F91T+/FKgCwMBLp/+ytdbr9djY2MA//MM/4Pbt2/B4PLh16xYikQhsNhvG4zGsVitisZjQtRuNxnsNgbwRw0NJfnUODwXlQqEQ9vb2RD/s4OBA+n6u2nDqobCxsSHTMCn9Tw+U/Src2C6XS0JA6j8t68OtMkzYTKoqNhAWi0UeaNUorXA5VKkPAOKBk8hBHcJAICB6dTqdDh6PB8PhEIFAQNQbisUiCoXCUs97eh8YjUZ4PB7cv38f//zP/yxKGOzlmUwmMvRxe3sb5XIZ5+fnqFarN9Jbp45aUM8fth4wsvxSiBvcv3REmR5WlSEIyvBsbGzgj3/8I77//ns5ayj6ytdixqlQKODo6Ai1Wu2dP+N7Gx6tVguXy4Xbt2+L/Ds/uFarhd1uRzAYxGQykW77Vqs1M2lwEViMHI/HMBqNorTMdAeHR1EQlPpwlJsPh8M4Pj5eqP+2TGA3MOfgqCkfNSfLIVuqmN+X8JC9D5iOYDNuq9USAVrKkwSDQWg0GtHIc7lc0Ol0+Oabb6TP6T//8z/xww8/XEkf/ZxhsVjg9XpnJotardYZgVqv14vbt2+LziML0CQLvSvYkGu32+H1emXiLutmpVIJmUwGuVzuixG1pQo6a5YkBCyabcQ5XJS4MZvNQqOmmgbltPR6PRqNxsIJBNfFexkeMoJ4kRsbG2JlVQn0drst4nbsgqfxedPhyE1ts9lmen9UdVR2S49GoxkBTco9LLOnw/EHNDpML1IaCHiVNlpW2ZVfA/QKKVtjtVolNUGHhtIh3MsmkwlWq1Win8lkgkKhgNPT009egPZDgpFFNpvFycmJGCBGHUyHezweRCIRbG9vC31cHd9+XfBQXFtbw9bWlgyd4/1kT8mzZ8/w6NEjYdUtsyP6JqhCww6HQ5RhLltjZo1qtRpOT09lrhrZiDyj1f/zeDzY3NxEu92WSPK6eC/DQ2+cKS6TySRNkJRV4YVTwnttbQ2RSEQYQt1ud6EXQo/T7/eLHD/VqHkI8zPQAgMvw36r1SqhvqottIygkSWll4U8eo4cNctC3zIx+X5NaDQvR3+vra0hHo9jfX0dHo9HOrP5MKmHFCnt/GLvFWnYi3qgLoOqM/g+asKfAjqdDnK5HJ49e4ZAIIBwOAy/3y/RJJU4mOFwuVy4e/cuWq2W6P5dd89SRisQCOCrr77C7373O9y9exd+v1/SQzqdDuVyGQ6HA4VCQZotl3mtVczrVQKvzk1mnNrttsgw8br5O6riSyKRwE8//YRAIIBoNAqn0ykqKqPRCBqNRhRnotEo7t27h1KpJKr518U7Gx56jJFIBHfu3MHe3h5GoxEODg6QzWZlM6mDoXq9HsxmM9bW1jAajWCxWJBKpdBoNF7beKrgHAdJ0aNXDQ+jLmq/0QipKadlBouwLA6yV6fT6UgqU53eqM4/WuFqqOrpqhov07qTyUT2EQCpIbDnR63pqA6RarzUZmDglWYhRVipWM2a5zLeu/F4LKkcsk1ZL2MDMwc/mkwm+Hw+UVJ/12dUHaD33Xff4dtvv4XH45Gmcs6tcTgciMViWF9fRyqVkv6SZVvjefDcYwSu7j3O8uJ1qqQvzvKyWq1yzwaDgTTck63Jvc3vsZbOtBxT/++a1n9nw8MLiEajuH37NjY3N5FIJHB6eoqjoyORp7HZbIhGozIATqvVity/0WhErVZDs9mc+eCqQqrb7YbD4ZAwjz/HArs6AlsdoOZ0OuF0OmGxWJY6vGazLNMSNECNRkOMMNW9uYmW9Vo/FvgAMl3GeuN4PJZBhiyK8+GiU0Mdu3q9jnq9jouLC/R6PRk0yKicQ8zU39NqtaIwTsamyWRCoVBAqVSSz6HS4ZdlFDZTNu12Wzxhj8cD4NVMrcFgICroNLA2mw0Wi+VaIyEY7ZA5xx6si4sL/PLLL3j27Bl0Oh22trbw1VdfwWQyIR6PI5VKoVwuywyqZQdru5x9w3o3/07jQYYm97Lb7YbP55Px1GwG7fV6qNfrKBQKMJlMsFgs8v/NZlMo85VKBaPRSMoszLhc67O/60Wzx4QzODiIiKkDWkk+RGT/7O3tIRwOw2AwoFgsXlqgMhgMsNlsrxXVOdGO3inBTWs2mxEKhXDr1i3s7++LcOgySvgzFCYzh4dSt9tFq9USynm32xW5HTL8Vrgc9AoDgYA4TXRuFhE4WONh1FIsFvH06VO8ePECFxcXyOfzWF9fF09So9GgXC6jWCxKYT0YDMJoNMqwN6aJrFYrMpkMMpkMWq2WNKsyPUVK96eO4XCIi4sL/PzzzxINfvXVV3C73TN72GQyyfo6nU7s7OxgMpkglUq99SRW7vtYLIa7d+8iEAig2Wzi8ePH+OGHH3BycoLJZCJzwnZ2dhCLxbC7u4vz83M0m82lT0mz4bzb7cqYajrdDocDGo0GzWZTpG7YqEyiAKVwqGjNablUwH/48CG2t7fFgHEWGGugOp0O0WgU/X4fx8fHqFQq11rP96rxqFaSDZvzaQVK2bRaLbRaLfh8PsTj8ZkBQpe9Nj37crkMj8cjuXSmMPgaPCSYcuJcGlKMlzndpsr+MNXGh5ieNQ0PZ9Yv8wP1McDeqI2NDdy6dQuxWEzGjKspXEbUZE1SjDafz+Onn37Cjz/+iFKpBJfLhVgshmg0Cq/XC61Wi3K5jIuLCxiNRoRCIaytrcFoNKJaraLb7cJoNMLn84kSsMvlEkPF2Uo8YJfBkRiNRiiXy5hOp0II2tjYgMPhADAbEQ0GAzkAObokm82+1fuo2ZD19XWsr68DAM7Pz/H48WM8ffoUuVxOCA9+vx8ulwuhUEhqFxzXsOzPCdcTeFULZv8O65Amk0m+rFYr3G63TDauVCoSzXc6HWQyGTk3w+Ew1tbWpPmf68kz2+v1SqounU5fO932zoZnPB6j1WohkUjA7XbD6XSKmKdKGKDXruYN1WFxlzEtOp2OjGAOBAJiyZkKUTu9KbyYz+fFAz08PJxRTV1WqN4iU21Mt+n1ejFGzKMvuyf3oUHGTygUwvr6OkKhkDDXuE8BzNQT+XCSPs0x7KwZcH4R0x78MxQKyThgu90uhfXRaCTNzqwp8aBotVrI5XKS9vN4PEvTBE1nsdPpiBq8GrlpNBppug0Gg5hOp6jVauJJv01aUc20BAIBOBwOlMtlvHjxAqenpzLJlWKV1BgLhUJCF1ZrxMuMeWOuRjjBYFCmgdpsNknt+nw+6PV6kRpTX4sRVLPZRLPZRKfTEdq0wWCQ4MFgMCAej4tqCtN413F639nw8OAnRZFjrFut1mvTLpmuYKhnNpvFq7vMKHBBW63WTHqNcuDj8VgOBzIzDg8P8fz5cySTSSSTSVGqXsY0G4DXIjlGO5QeYn2ChudzKJp+SBiNRlHCePDgAb766ivxytUU2zxtnw3JqVQKBwcH+Nvf/obT01OZmNtqtVAsFmE2m9Hv92G32+HxeBCLxaQVgA+21WoFgBmKPAVwGd0wtcFUc6lU+pVX7u0xHA5RLpdxenqKn376CaPRCNFoVNaRw+KYemy326hUKmg2mzg/P39j3x1JTXR2ASCTyeDw8BC5XG6mV2UwGAhxg+lVm8322ah7cI/SqWct2263Q6vVSu2n1+uJIxQIBGQO1zwmkwna7Tay2SyePHkCs9mMvb09cZp4Dnk8HgSDQRE2pnNfKBTeOl363nRqp9MpXhyNzvwBqNFoYLVa4ff7ZwaNqVMJ50EviQ8xZTaYqqDnYzKZZDjSs2fP8OTJE5ycnCCbzaJWq11K114WqMVvMktogIxGo2w61nZWhmcxWJDe3NzEH/7wB/zud7/Dzs4OHA6HeHVqmhh4uT9JCT4/P8df//pX/PDDD3jy5AlKpZLsQ+pX1et16Z/Y39+X1I7JZJJGaDpLqu6bOvGRDZjAyx62fr+PH3/88ddatmuj3+8jm81iMBig0Wig0Wjg+++/lzOCe3Q0GsHlcuH+/fsAXhqJTqcjY0EuA597ptGHw6FEoGzRUGVy+Fo07qoK+ecCXi9LD1RbN5vN6PV68jOqWofKdCMmkwnq9TqOjo4kuu/3+9jd3QXwSt1jOBxCr9cjHo9LpMNI6YMbHkYxnK3dbrclipk//HQ6Hfx+P3Z3d+Hz+aDRaKT/h9LpiwQ91fnsvV4PuVwOp6enkouMRCKIRCJoNBriLTHNwUjnc5AxUR/E4XCIVqsl10rjvYw03I8JVYlge3sb29vbiEQiM1GOKqioPsxqVEP1c+5XjUYjPWuNRgMOh2NmKuR4PIbD4ZBag6qywUwBjRFTb+wRolFbpuZUip+Wy+WZPrP9/X2Ew2FRkW+1WgiFQgiFQggGgyIu+ia5J547jFy47jwkv+RngHVJMl2z2Szq9Tp6vR5CoZAQO5h5mh9rDUAGQl5cXAhDs9ls4tatW6JSTad+c3NTGrDnswZvwntFPPRcer0e9Hr9DD1SlXjX6/UIBALY29sTCRLm2lmzWfSBuanS6bTQW3/66SecnJzAYDBgb28P9+7dk0Y+yvWQ6MB877KDm4l9Ea1WS9I8k8lkpVjwBmi1Wvh8PsRiMcTjcYRCIbhcLulupwqEqv83Ho9n+nFarRZqtZoMFFS9akaj9C49Hg9KpRKePHkiufWvvvpK0myqJ67q700mE2mAVOdIvY80ya8BtfZwcnKCXq8nBpnkiWKxiMFgMFOLcLvdkm68KkuhtlGoKiYqGL2yRkdK8KKMzDJBJb+wJWB+vSaTibB5eU6wCbTb7SIcDsNqtcJisSw09HSKCoWCBBKRSAR+vx+j0QiZTAYA4Ha7EQwG4fV64fP54HQ6pTTypvV9rxpPt9tFKpWCzWbD/v4+NjY2pBDKmRsM/ciU8Hq9aDQaMuGuWq0uLEqRkn1+fo7hcIjnz59Dq9Uil8uhXC6L5lu328Xt27extbUFh8OBdruNbrd76aIuK/igjcdjiS5psEmnVFlZK7yCwWDAN998g++//x53797FxsYG7Ha7OEsqNZ9GiDVMrncmk8Hp6elCsgp/lnsym83KRFzWlTqdjrDW2FzJUR+McMhkY62IPT/LeE9prKfTKRwOhxhspmXYaMrZPd988w0KhQJqtdrM786DtYZOpyO9P0xPqoP8+FwwlVqpVJDNZkWqZxl6o1RwH6hGl6y1Xq8n60Wjwb3Ybrfl34PBAPF4HD6fb4bgNQ+ucaPRgEajkTYNNgOzbUZVoajVaigUCuLwvynT9F4Rz2AwQLFYlPRZIBAQsgEAmTVCZVqDwYBut4tcLocXL17g6OjoUsPDiy8UCqhWq+IZqsU0qh/cunUL8XgcTqcThUIB5+fnn5XRAV5tPJVQQAkLKlIv4wH1MaDT6bCzs4Pf/e532NrakvEGfJDVqa7qGvLvg8EA9XodpVIJ7Xb70kOL7MpisYhqtSoPdy6Xw3A4FNopO/gpKmoymWSCLAVxu90uSqUSSqXSW+fNPyWQ4cY+pGKxiFQqBYvFArvdjna7LSKeGxsbQo12u924uLi49HXp8BYKBeRyOSEZhMPhGXKB0WiUcSIWi0XOBfZILWPEw2edDEga23q9Lgc9z0nglUNExqtOpxPBZr7eIvCMZeuKSpkmlZ01oGazCa/Xi3g8jmAwuDB9twjvZXjUwjcPQbvdju3tbfj9fmGeUaGatMcXL17g+fPnyOfzV05dVNMeXDiVccQajioTQdbM5wJeq1oYVFV/1RTRqs6zGPQObTabNDyTks+8tCpxA2Am0lDTGm96qNQ9y98fjUY4PDxEJpOR9J3JZEK1WkUikRACAwDEYjHcvn0bjUYDp6enePLkydKqXtMD73Q6ODg4wGQywe9//3vEYjFotVpUq1UZDudyuaQx8SoZFnrymUwGx8fH4kTE43FkMhkxaOzziUQisFgsSCQSSKVS7zVD5tcC9xyjYY/HIzRpphmHwyGMRqOkvVTlDPVnaISYHr4MKqMzl8vh//7v/6Qp2GQyIZ1OI5FIYDAYYH9/Xz7n22Zd3vuEZghHCrXf78fa2tqMNhA7uQ8PD3F6eoqzszMUi8U3zuVWF2FeSJFpJ0Y+agiv5nCXWSCUmM9hq4ciN9SqefRyMM9fLpfRbrfhdrvF2KiGHHh9Tg8L5Wp9522gOgH9fh+lUgm1Wk3eS6/XSwoPgLDb6vW67OOjoyO8ePFCMgjLCFJ3M5kMrFYrHj58CJfLJdIsxWIR4/FYZh5tbGwgl8tJrWDeKWUmpFQq4fDwEGazGbFYDD6fD7u7uxI9BgIBbG9vIxAIoN/vI51OI5vNLl2ajSLIlHEi+zEYDMJmswnzkpEOSwzMPLFGw2icVPdyuYx8Pn9lSoxpOzYGx2IxkTrjvdNoNIjH43A4HNjc3BRZItaVLr2u91kUGoROp4NqtYpoNIpQKASPxyMLQA/l/Pwcv/zyC05PT2VTXZfmrB6qPHA7nQ7K5TIymQzMZjPK5bLkIDnI6HNgti1KRdIjoVzRl87quQw6nQ6dTgfPnj2TZjoyotgcx7oOnSVGLo1GA6lUCplMRgYLXneN2Ys1GAzEoNFxajabksKw2WxIJBLI5XIYDAYol8vXliL5lEDiBT8/2Zh8PsmSOj8/R6fTQSwWw4MHD5BOp6WwvQisu52fn0uaNBQKyViWyWQCn88naaVMJoNUKiXq1MsE6qHxwHe5XPB4PHC5XNIfxh5J4GVauFKpoFgsCtOPhrbb7SKdTqNWq4l801VpXEarlNvpdDpoNpsixGwwGGYUIr7++mvk83mcnZ2h1Wpd2d5xIxEPPbdQKASfz4d2u41arSZRCr23QqEg3czv63VwU5fLZfz8888YDofwer3Q6XRot9twOByIRCKYTCYolUpL3USq9nwQKgOr3+/LbIxlPaQ+JHq9Hh4/fozT01PRDzSbzTIu3e/3IxKJiBAtDQ/w8kGm6vL7CLDOp0H5XKgEHDoRzCJQsWKZwWvh6IQff/wRo9EIGxsbolJfLpdFc4w6Y1cxz+h0VqtVMeTValX09lhXymQyKJVKODs7kzTcsvX0kf1rt9ths9ngcDjgcDhgsVgkuhsMBhKR93o9VKtVNBqN1yJ0UqXZUMuU8GXg+cIzhr1su7u78Hg8IueVTqdnBnby3L/qLHpvwzOdTqXb1eVyQaPRoFQqCY2SURGZFzcFLkqtVsOTJ0+Qy+UQjUbh9/tFpJBNZqQhLyO4fqylMYeq1g+63e5KmfoKdLtd/Pzzz7DZbBgMBqhWqyLJ73K5sLOzIyQYpi0ooUMtQlWn6iaMOw9P0lzV4X582NWa0zJDJQr97W9/g1arRSQSQSAQQLfbRaPRQLPZxMXFBdrttqgmX3Xt9LRJviiVSggEAtIrReFLEjQozrqsjtk83b/T6UiKPZ/P4/z8XM4A1twXpSnnjc3brAfXmjUdpkwB4OzsDAcHB8hkMvB6vcjn88KCuwo3YnjIWmFYS5Vd9c15gN4kh54PLxtXWXRk+M2hdMtqdIDZlGKz2RTVBxZh1e8tK1vnQ4Pp3n6/j2fPniGfz0Ov18Pn82Fvb08YbmazGVarVdhVvV4PFxcXODw8RD6fBwDpt7kpMBLiQcHa0+emucfIp9lsolarSVZiMpmgXC7j+PgYz549EyWCt639knHIZ50D4NQUNKX/l9GIM4LpdDoSUZhMJlFSH41GqFarqFT+X3tf9tRGlqX/ScrULmVqFyBABhsou+yO7rFrxjH90g/zMDH9585Ev/XExExERXRXeyuXDZgd7fuWqdT+e/DvHF8obIQNWHLdL4KoKkNZ5M1779m+850ap9Uu2ztX3VdUThmPx/B6vaxQMJm8F8198+YN2u02PB4POw831scjQpTLyeVybGQ+xsO/blC39GAwQK1W48+5KEU1bxDTaaRIDXzwgMRBcfPs0d0GBoMBSqUSqtUqy7qT8gXlyjVN4xlQlmWhVqvh9PQUjUbjV3Tr6wS9528dYgaEWgIajQb29/eZhXWVfSxGjlQ7Er837wac2GeGYTBrjJrJKftBKdmbSiPSHUPRuDgcju78o6MjrjlPs49tV3kpNputDOD4C57ha2N1MpnEvvYvcRHk2t4c5NreHOTa3hy+gbUFPrK+VzI8EhISEhISX4pvq71fQkJCQmLmIQ2PhISEhMStQhoeCQkJCYlbhTQ8EhISEhK3Cml4JCQkJCRuFdLwSEhISEjcKqThkZCQkJC4VUjDIyEhISFxq5CGR0JCQkLiViENj4SEhITErUIaHgkJCQmJW4U0PBISEhIStwppeCQkJCQkbhXS8EhISEhI3Cqk4ZGQkJCQuFVIwyMhISEhcauQhkdCQkJC4lahXOWHbTbbheNKHQ4H3G43NE2DpmlwuVyw2z/YtMlkgl6vB8Mw0Gw20el0vtp8+clkYrv8p24fH1vbWYDNZuNZ9pPJ5KMz7OXa3hzk2t4c5NpOB6fTCa/XC03T4Pf74XA40O/30Wq1YFkWxuMxLMtCr9cT74jKRaOvr2R4LoLNZkMkEsGjR4/w7//+7/iP//gPLC8vQ1VV2O122Gw2DAYDZLNZ/PTTT/jP//xP/PWvf0WpVMJgMPjSj//NgC5/m832q8v/psaX22w2uFwueDweeDweAEC320W320W/38d4PL6Rz5WQ+BhEB0jiYthsNr57J5MJxuPxF62Xw+GAx+NBOp3GkydP8G//9m94+vQpgsEgLMtCpVJBp9NBpVLBX/7yF/zXf/0XCoUC3e/HF/2d12J4FEWB3++Hz+eD0+mEw+HghwcAu93O1tLj8ZwxSnIDTQdVVeFyueBwODAejzEcDjEajQAA4/GYv657PW02G1RVha7r8Hg8MAwDjUYD9XodvV7vWj9LQuJTsNvtUBQFNpsNo9HoV/td3iUfzqvH44HL5UK/34dhGBgOh5+1PjabDX6/H8vLy/jDH/6AH374Aevr6/B6vXA6nXA6nQgEAhgOh6hUKshmszg+PsZoNEKxWOQ76jy+2PAAHyyi2+2G3W7HZDLhjWGz2TAej2G32+FyueB0OqEoyplUnMSn4XA4oGkaEokEvF4vhsMhOp0OLMvCZDLBcDiEZVnodrvnw9wvAjkVXq8XoVAIPp+PN3Or1fpNOg60b2W0d7uw2Wzwer2IRCJQFAW9Xo8zJqqqYjwewzRNdLtdDAaD38T7oegP+GB0qewRjUYRCoXQaDTYSb3qWbXb7fB4PIhGo1hfX8fvfvc7PHnyBKlUiu960REej8cIBAJIJpM4ODhAuVy+WcMjLsB4PMZgMGDjY7PZ4HA4MBqN+OtTdQKJs7Db7XC73VhZWcGTJ08Qi8XQ7/dRLBbRbDYBvE9/VSoV5HI5lMvlL0phiu9SVVU2eMFgEABgmiZM02Sn4qLU37eAi1I6FLlT+vhr1Sl/i1AUBYlEAk+ePIGmaWg0Guj1elBVFcFgEP1+HwcHBzg5OUGlUmEH7Fvbl8CHVBplluheBd6vUyAQwMLCAlKpFDKZDBqNBtdgrgKXy4VYLIaFhQUEAgG43W54vV4oioLRaAS73c6BhmEYyOfz2N/fx8HBARqNxic/71oMDxmbfr/PX+SFk9Xs9/uwLAuDweCzrO9vDRRt+Hw+RKNRLC0tIR6PIxwOYzgcQlVVJJNJBAIBdDod7O3twTRN1Gq1zzI8FKKLqVKXy4VEIoFkMglFUVCr1dBoNNDtdvlnKQJot9vXvQRfBXa7HaqqQlEUOBwODIdDvsScTic0TYPD4YBhGOh2u+zpSdwc6CzEYjH8/ve/x+LiIqrVKnq9HpxOJ4LBIHq9HqLRKHw+H3Z3d1Gr1WBZ1jf5fhwOB7xeL7xeL1RVhWmaaLVaGI1GXPaIxWJIJBJot9ucnrwqFEWBrutIJpPsfI5GIxiGAQBwu91wu92YTCawLAuNRgPZbBanp6dotVo3b3iGwyG63S5M00Sv1+Pis2EY7BV3u110Oh0Ohb9Vb+S6oCgKNE3D0tIS1tbWoGka9vf3sbu7C5vNBrfbjeXlZaTTafR6PZimiZOTE6iq+lnpNvKUAoEAvF4ve/fJZBK6rqPVavH7pc1NeWSbzYadnZ0bWonbAXmRqqrC7/fD7/fD5XKh2+2iVqthNBrxgXa73ajX66jX6+h0OryfJW4GlKbXNA0LCwtYWVmB3+9Hv9+Hw+Fgr1vXdQSDQTgcDhwfH6PRaMAwDFiWxc4upf7n1RjR2SfD4vV6kc/nYVkWLMuCqqrw+XycCvvc2q8Y8fv9fmxtbSGdTgMAGo0GRqMRfD4fs9sooBDrzZ/CFxseSqmJNYZ+vw/TNNFuxt/kkwAAIABJREFUtznnaFkWOp0ODMP4zeRgPxdkBNLpNLa2trC8vIx6vY7t7W3UajXY7Xbouo7RaIRYLIZIJIK1tTUUi0Vmk1zF+NCl63Q64ff7EQwGmQBit9vZWaDwOhAIYGVlBbFYDB6PB4PBAEdHRze7KDcMm80Gp9MJj8cDr9fLZBli8wFAIBBALBaD0+kEAF7nzy3cSkwHRVEQDAYRDofh8/nQ6/WQyWRgGAZH3tFoFAsLC1zvSafT6Ha7KJfLKBQKqFarGAwGsNvtMAwD7XZ7Llm1ouFJpVJwuVzodDoc1ZDj5PV6ufZy1aiPPsPj8bCxjkQi0DQNk8kE3W4Xo9GIoy3TNHF8fIw3b94gk8mg2+1eauyuJeIZjUZMsyXDQ307DocDPp8P3W4XrVYLhmFIKu4nQDUxMjybm5uIRCLodDqo1WrIZrMAgFKpBMuyEAwG8fjxY6ytraFSqWBvbw+tVgv9fv9KlyF5guImHY/HqNfraLVanN/1+/0IBAJYW1vD6uoqvF4vWq0W/vu///tG1uO2QIdW9BSJ0UN/5nK54Ha7AYCJMsTOlLg5qKqKUCgEXdcxHo+RyWTw008/oVgswu12I5FI4P79+0ilUlhdXeX3ZbPZkM1msbu7i4ODA5imCbvdjkwmg6OjI3Q6nY8Wv2cVVLogh3M4HDJZi0hexEDt9/tnMkxX/YxgMMjsNZvNxkZsMBhAURQoioJ+v8/v43//93/x7t07JjR8CtdmeMSIh/7dsiz+5cTvU0gm8WsQ716MWrxeL9xuN8bjMfr9PkajERe3A4EApxdoU3xODW00GsE0TQDg90bEhmAwyNRJYq5omgZd1+Hz+TCZTOaepTiZTDAYDDhNPBqN2JP2+/0YDocwDAPVahWmacKyLLTbbRnt3AIoA6AoCvL5PLrdLl+ElUoFDocDnU6H6566rqPdbqPT6SCVSnGETmfq5cuX3MRO9Yp5ADmlPp8PkUgEPp8P1WoVk8kEbrcbiqIgFAohFovB6/VyneuqqTbKgPj9fty9exd3796FoihotVro9XrshKqqCsMw0Ol0UC6Xkc/nucZ82eddq+GhGgB9UWRDEZBlWTLamQJUM6NoYzKZsIdBhX+iOa6urkLXdU6zEdvnKhuNKNl0oZK3qKoqEokEIpEI1zxog9tsNvT7/bn1+s9TUcfjMVN0ibVGaTa/3492u41Wq4VisYhWq8XenyTK3DxofzabTRweHsLtdiMUCmEwGKDZbCIYDMLtdp9hdpVKJRweHsLr9SIcDuP+/ftwOp3o9XpwOBwwTROKomBvb+8rP930oHSw3+9HNBqFpmkwDAO6rmN5eRl2ux0rKytIp9NwOp1oNBpX/gxKuWuahuXlZTx69Aibm5twOp1oNpuwLAuxWAyKopxJZdZqNZimObUjdi01HtG40D/pizxJ+vObaHL8FkHhrqqqzBQE3jNJnE4nwuEwlpaWoGka06vJG/9cmu/5QiQdeIfDwWE3XcqmaaJUKqHb7cLv90NRrsWHuTGQoSFvTuzHOf8lUlT7/T46nQ43zVKqmA6Y3Ms3D8uykMlkAADpdBq6rsNmszHj8+7du1haWjrDunz+/Dlev34Nr9eL9fV1fPfdd7hz5w4WFhbwL//yLwgGg4hEIpy6ngeQUaBemUQiAZ/PB13X0e124XK5sLi4iLW1NXZcyVGk9PGnQIYtGo3i0aNHePr0KTY3NxEIBDhjRXdRp9NBoVDA69ev8ezZM7x9+5ZTbLdieACc6aSnNBAVnUajEX9vPB7PXU71a4HytVTgG4/HcLlcSCaTiEajSCQSWFlZ4WKqw+GAy+WCy+Vinv1VcNFmoShANGbUPNpsNtFqtRCLxRAMBmfa8Njtdu4/oKiR0pkUhZ8HpTUAnOnZoe55ivoo8rvOxl2JsyDHilJK1Efm9Xrh8/ng8/kAgHt4KpUKXr9+jd3dXaiqilwuh06nA7vdjt/97ndYXV2FoiioVqtQVfUrP93VQE4T3adE3rLb7fD5fAiHw4jFYuh0OggEAvB4PFeiUxORg4y11+tFp9NhMoaqqsxQPjw8xOvXr/Hy5Utks1kOLKb6nC9ZBILY0GS32zEYDNDpdFCv1+Hz+ZgNJTE9yJsmL1xVVYTDYQSDQdy9e5cFWam4uLa2hlwuh4ODA6b4fmlKczQacXopEomg1+uhUCigWCxiMBggkUggEAjM/IXrcDiQTCZZsolSLY1GA5VKhQ0PHWo6zHSxEb2c9rHIpnI4HGg0GqhWq7Kh9IZAkTd52ZVKBZZlwe12o9lsYjgcIhqNwjRNZLNZbiRtNBrcvD4YDODz+djo0OU5T47wZDJBv99HtVrFu3fvUCgUUK/XuacpGAyi2+0y+4wMgSjy+ylQREWMNofDgVqthkKhAMMwmODRbDZxfHzMqX0qsVzlvrk2N1U0PrRJqLN+MpmcacqTuBy9Xg+5XA4+n4+7gFdWVuByuQCAe6K63S7sdjsSicSFyuCXQRQfBXBhKrTf76NcLrMRqlQqGI1G8Hg8c3HZKoqCaDSKYDAIn88Hu92OdruNyWSCTqeDXq/HaQbqTYjH41hYWEAwGORmWq/Xi1gsxoeMLjUAUzF5JD4f1KRYqVT4UnU6nej3+7zfqRZJ6SBy1qgeQj9/enqKV69eYXd398Jod1ZBz10ul/Hq1Ss4nU60Wi2uAycSCei6jkKhgF6vh5OTE6aRX+YcUlNqKpXC999/j3A4jFarxc5Zp9OB2+3mPsFcLoeTkxPkcjmYpnn1vsEvWYjzII9xPB6zR0EXIbGi5rEQfdsgCYp3796hXC4jHo9jbW0NS0tLaLfbePPmDbrdLpxOJxKJBPefUHpzWvokOQrkuVP6iQrm1BXu8/lY+rxer8M0TZZENwyDu6ZnFVQvI+NB+5CYaZRKCwaDiMVi/BUKhTCZTNBsNrmupmkabDYbms0mP3u5XJYR/S2AyAQAuK7sdDqZKED1n3q9zgy2zc1N3L17F9FoFOl0GpPJBNvb2/i///s/vH79eq56eeiZyRCIFGePx8OEi2q1ikqlgrdv3yKbzXJfzadALRwbGxt4+vQpAoEAarUams0mE8Ko5knsTxIE7XQ6X8/wXCTVT94zXWoS02M0GqHdbsOyLFaXpfRQLpdDr9dDKBTi8LpWq6FWq3GabRpQlzMRB6j3yjRN7mGJxWLw+Xwol8uo1Wr8u1COfHd3l6mrs4rhcIh8Ps9d1vTMtF+pbqDrOkKhEOx2O5rNJjMxW60WGx1d15lWXqvVkMvlUKvVZLRzC7ioI57Sn4qiYDKZIBwO49GjR9B1HaZpIpVKsdKBx+NBo9HA3t4ednZ2kM/nZ9phughUd6VIjRo8yXkE3ms3UvqXaOOXGQZS7aAm6clkwlI8drsd6XQawWAQlUqF08qkVvM5e/9aDA8ZGYLIHBJTOGSdZ70mMCsgRiDlsYvF4hkpImK80eVoWRYATOV902YlLaZgMAjTNNmTJw8oGo3C4XCgXC6zNhnl3KvVKtrtNmszzSooveLz+aAoCjweDx9UqkG63W74/X6oqsoGHAAzeYbDIZLJJDtSo9EI9Xodh4eHKJfL0vB8JVCUXqvVsL+/z0PK7ty5w1kBqkGQxNHBwQGKxSL3rc0jLhoHQew1Op/T0v2pOZr68qg+TCQxTdOwtraGyWSCX375BblcDq1W64vqyNeaajv/gGKBnNhRUtfqahALqxQyj8djKIqCdruN4+Nj7mImijXVLT7lzdFmCwaDHDl5PB6ugVDdx+12c6h9EdWaxGBnOaIl7200GiGbzaLf7zPzjyRvyFh3Oh1Uq1U0m02WgyItKlJ1oDWgZ5d7+uuARiEcHR2h0WjA5/Ox3mAwGITdbme1FOC9s2VZFk5PT7n591sBpc1VVb3SuBJyYGOxGB4+fMjpfHG/k0hrv99Hs9nE0dERDg8PUavVvq7hEQ3M+QemSIdSRldlP0i8hyh9Thur2+2iUCjg6OgI4XCYJUEuI3BQWE2zkRRFYQmYYDAIl8uFXq+HRqPB+no0++cizMP7JEYQ5cepFkmKvm63myOZdruNbrcLAGycI5EIp+dm3dD+VkDvIp/Po1QqcS2ZOvvdbjcqlQo7EfTOaTzztwSq/7TbbSYdTCvGbLfboWka0uk0IpEITNPklLqmafB6vdwneHR0hOPjY2QyGaa1fw6utcZDl+N5phQZHuqMl97hl4HWmqjrxKHv9/solUqXS5L//+FuqqpyOE3UYVVVme2VyWS4W5kUaecZ59UJ6FDa7XY22mJK2OFw8PTF1dVVRKNRloUnoURATr78mhDvHQCceqZ08UV9Wt9iE/tgMEClUmHVhmq1CsMwLj2ztH5EKqL7gFLnq6urUFUV29vb3BtVKBS+yOgA1xjxkHEhJhsATtnQw7Tb7SuLV0pcDFHTjbq1e70e2u32pc2MYnOqzWbj90JKCePxGOVyGblc7kzY/S28t48VqD9GyCBPsl6vs2SQaZrcz/Stec7zDnpf4vv8FvbtZSBJIRLwpFaLaZ59OByiXq/jl19+YXV7m82GeDzOqfdyuYzDw0Pk83kYhvHFEf+1KRdQ8S6Xy8Hj8XCxmwrTxWIR9Xp9KmqfxAdQ3lZUQwY+eG1iam04HLLA5adAXfmUams0GuzhUJczqYmTo/BbOLznMR6PuY5Wq9UQDoexvLyMyWSC3d1dloC/DOJ6Uy+EOL0VAMuQzFNfydcAkZYoRSzKwYjq6lSbEBvb56EeOQ3oTiDdRvpvUmEgubJpn5MczR9//BFOpxOqqiKVSsHv97N2I/Ce/enz+TiTRWxlqnPeegMpKRsfHBxws51hGDAMA06nk4cyTcspl3gPYpYlk0ncuXMH6XSaWVgUDhOVmYql0/RIEVmBCuemaaLT6Zw5rBQF/BYNDoHqQlTrajabzHYrFAq8ZpfB4XBA0zSsrq7i/v37SCaTZ6RaxuMxTk5O8OOPP+L09HTuU5o3BVKU8Pv9CIVCSCQSiEajnDJutVrMXCOnl8Z4eL1eVCoV7O/vc/PwPO5tanNYWFiApmk8MoJqWr1eD9VqFScnJ8w2nSZbIda9vF4vIpEIBoMBSqUS97B9//33WFpa4qZ1auXY3t5m1fBblcyhIh/JKIiFW/JOaGbPdaZsxFoSWWXKWc7jpjoP0l+6c+cO/vjHP+KHH35AOBzm9Bqlw05OTrC/v8+1B/L4aBbHeRBFu91u8xyeiy67b2ENvxQUWVI0SQypq3h4qqoiEongwYMH+POf/8waWOQkDAYDvHjxAq1Wiw3cPDU23hZoTEcsFsPm5ibu37+P1dVVnhRbLBZRLBZRLpdhGAbsdjvC4TDi8Tg0TUMmk4HdbkehUOBU1DxRqomJGgqFsLq6isXFRei6jtXVVSwvLyMYDMIwDOzt7eH58+eccpy2wZMyKKqqIhgMIhAIsELH+vo67t27x9GUw+FAJBJBtVplR5jEoKfBtZIL6Je6SYjGhkJMCg9JsJEUhOcdFIG4XC74/X7oug5N0ziPS+G1ZVnI5XLQdZ1nsff7fU4RXUSFFguyEp8GrdvnrhnJkUSjUSwuLiKVSrEWFgBuDn78+DGazSbevHnDMkkSH0AOkq7r2NjYwIMHD7CwsMCpYV3XkUgkWD9sMpnA5/Nxu0AymUQsFkM2m0U2m8Xe3h4ODg6+9mN9EuRUU99dLBZDNBplJuby8jJSqRTi8ThHdrVajYWEW63W1JGI2Nu3vr6Ox48fcyRFxCMxE+JyuZDJZLC6uorT01NWOZgGsyspfA7EJaeiuMvl4r4TmhI5GAxQr9eRzWaZhTXPh5eMOekl1Wo1ngZIPTaapvGFRpuDwuRIJIJ8Po92u416vY5ms3ktxkYcK/AxGv08Q5w3fx2glA/1SpFuodgvFQ6HsbGxgcPDQ5ycnKDdbs/13r0JEJkGALMw/X4/d+8TaSYUCnEtR1VVHmdO56RcLmN3dxej0QgnJydf+ak+DRrsRkZhcXERoVCIZ+bQtFHg/frQALdAIACXy8V1oGlAGZZkMom1tTWefux0Oln6jIZN0pk3DANLS0tIJBJXSrfNjeGhw7u0tMRTBmnhScak0Wjg6OiIF4fkHOb1AFPjYy6Xw9HREe7cuQOv18vClXTQkskkU6vJGDkcDjx8+BD5fB7Hx8d4/vw53r59+8WpBXIAaCOSBzTP7C6R+k8NdQ6H44xRpWjnc4wRvSefz8cEkfM1BvI0dV0/M7r5WzLoXwpKebZaLW4biMfjZwgHtC/JO1cUhTMi5KwSff74+Himx3k4HA5sbGzghx9+YIHTSCTCivA0ip6Yw71ej9OLfr+fJZ+mvf8cDgen7uLxOI9YcTgcHJ2LKjU2mw2BQAALCwuIx+Pwer2XtnIQZnfVBdBlp2kaNjc3sbGxAb/fD+CDlSedMZI/1zSNO5vn1fAAH5o9adYO1c/Iy1NVlYuMAFgI0+12I51OI51OIxqNcmH1KobnIk9JHIetKAp7oeVy+dqe+TZBqUyae6QoCqv80sXf6/VQKpVQKpWuPHeHjBqlRqmmJtZwKJ1Cs2VE3S2Js6CLT5z/JdZ1ad3onBCbUFVVJtXk83kcHBygXC7P9N3g8/nw9OlT/PDDDzxmmgZBUv2cjIO4z1wuF+sRUkr3MpUG+rtITDUWi0FVVXaA6M6hOhDw4eyEQiGEw+Ezn3XZGZkLw0PqwtFoFPfu3cPW1hb6/T4X1mu1GjNa+v0+XyTUoT6vniONuE4mk1hZWcHi4iIX/CjioU1BoF4T8qxDoRAzYKa9zOjvFtNB9BmqqkLXdcRiMe7XGo1GrBo8DxCjGlqjRCLBB+nevXtYWVmB2+1mdepnz559Vh8aDeijdwKAjQ6lLFwuF6uBixMj53Xf3iTogqRUEvAhJSrOU6JLUtzHpOf2/PlzvHr1CrlcbqZrwbqu41//9V+RSqW4jkX1WjI8ZByoDEHRHDU+Z7NZnjk1TW9fLBbDysoKwuEwnxFxkBz9NwCu89B9EI1GWTD3MkM384ZHLKotLy/D5/Px7PWDgwOu59BIABqJSzpc815/IO+YXuT5aYKk20ZeCF1e4/GYRzWT0vK01F9xLg1dhPT5lD/WdR3BYJDn2O/u7t7A018/RGMeCAQAAIuLi3jw4AECgQA7OTTJEQAymQwODw/ZmF8ldREMBpFKpbC1tYU7d+6w9h1NPx2Px/D7/XyZzPNevQ2QcC6Jt5JMjpgyozWkSJJqEdVqFYeHh3j37h2Ojo7QbDZnOuKheq3b7eY/oz49MgCUliWDTDUeYqXR9y+Doijw+/0Ih8M814hSbOKdcz4LYrfbsby8jIcPH6Jer2M0GmFnZwftdvuT9eSZNzzECIrH41haWoLL5UKtVsPBwQGOjo7QbrfZ6JAwpmEYHH7P8sb6FMTmNzI+g8GAL0Y6UKLhIa+ZtJoymQxyuRxKpRJ3JH8M4uRNIiyQajWFz5QDJnKHpmlwu92sDzXLUFUVbrcbLpcL4XAYm5ubSCQSAIDl5WU8evQIgUAApmmiVqtxZEcTRoPBIDRN4/6naQwEXQILCwu4e/culpeX4XQ60e12uW+C6hbdbheZTAbFYpENkjRCv8Z4PIZhGCiXy0wTpndL60VnniJa6nnL5XI4PT1luvWsi4SOx2NmN5JhpWwEGRox1SY21FL6i2ZLiUK5F0E00qJSCa0pGSGxHkppd3I+aWTLycnJpXI9M214KCUiqs6Gw2GEQiEYhsHzwMvlMsrlMnv41zH2+WvDbrfzxMt79+4hnU5zeoGYbWR4aDMCHzwSwzBwfHyMV69e4fj4GCcnJx8lAIhRTiQSweLi4pnUHoln0kRH8sBIXoeYWrMMTdOQSqWQSCSwuLiItbU1xGIxNqbBYBDVahXZbBaDwQC6rvPURZrMuL6+zp72tH02ZEDoUuj1eiy86nK5oKoqcrkcdnd38ezZM/z8888ol8tSWuojIMND/TqUVhLTP1T/pEhgOByiXC6zuOW8TIu1LAs///wzvF4vAoEAN4oGAgGuA1KqTWSa2mw2hEIhbG1tMQHg2bNnOD4+/qjxod7A4+NjvHnzBsFg8My6iql9MfoReyipj2ea+3emDQ/wgVhAFwBdurFYDIqiwLIsRKNRRKNRFAoFlEolVCqVuWoMuwjk0UQiEaytrfEwK5EeKXo/5NmRPlOtVsPJyQl2dnZweHj4URl4YgtSnWNhYQGpVIp7A3w+Hxs1yudScZM8dSJ4zCrEbm9KecXjcaiqyrXBUqmEg4MDHB8fw+VyIZlMwjRNLC0twe12Q9d1PvDT0lPJe6Tohgw/KU5EIhGoqopqtYrt7W384x//wOHh4Vx4418LpMpOUzabzSbvyYvSQedbEkidfB4cU9M08fe//x1utxsLCwtYW1tj51NkPhJEkoXX6+U6pcPh4PX62LiUwWCAdruNYrGIXC7HU1zpnIsyRPRZFLnTTK7j42P+jLk2PKL4KPD+kiQaZa1W4wFeyWQSiUQCqVQKh4eH+Pnnny+dRzPLECmgHo8Huq6z9yEKWpLRIXZZqVRCNptlLzqTyaBUKv3qIhMPKI3P3tjYwJ07d5iqThEmDU0j40+GiFJ/NpuNu8RnGbRO5D06HA4UCgU8f/6cZ/RUKhXU63XY7XbE43Fsbm4ypZW85mkUfwm0f4nWSsrsJOhKc4FIUZkch3ndt7cBkjEihQcaXChqlwEfRofQZUzvwTRNni816zBNEy9evODR806nEwsLC0yEoee9qKRARBld15nuHA6HeQ9+jHl2kQ4ckTXEsSy9Xg/FYhH5fB7FYhE7Ozt4+/YtD6uce1bbcDiEYRgolUrY3d09M0mQtMxEb5TSJs1mcy514ZxOJyKRCBcVacBbIBDgXC6lEsQ5SNTfQEOxstksDg8PuWlUTD2IBisQCGBtbQ0PHz5EOp1GKBTivC2pQtAXRZ3A2Smn0wiTfm10u13k83l4vV6YpglVVVEsFvHixQvk83m+mHq9Hl9e6+vr3KtEhvuq6S/x4qMLj0YzUO8F/fv5QXMSF+N807I4jkXsNxHTbuRAfUweahYxGAxQKBTg8XhYh5FGPaRSKYTDYZZeEgkV9E+qwSQSCTx8+BCTyQQHBwc4OTlBqVQ6o15NDDm/38+zp2hEikhooPPQaDSws7OD7e1tHB0dnRkON020PvOGh3px9vb2UCwWzzSGksdOshjJZBKKoiAYDCIcDqNWq8GyrLnZaLRRVldXsbW1BUVRsLi4yHIY1AciXvo0PZPyq9VqFc+ePcPe3h6q1SpvLirAklGhCCYSiWBrawv3799HLBZj4gDldslA0f9DlyQROhqNxqXEhVkAKW0XCgVm6lmWhVqtxg6KSNbwer1YXl5GJBJhYgX1RdDhuwpEdiJFPxTxkMGTrLbLIabexb4n0bEi0FqSQyqeoXkB1UtyuRwMw0ClUkGhUMDjx49x7949PrNkFETVc1I3icfjePLkCZLJJLa3t/Hy5Uv8/PPPKBaLZ1LlNAiSWG3EjO12uxxBuVwumKaJbDaLV69e4eXLl9jd3UWxWORBn9OcjZk3PADYwlKdQgwT7XY7Go0GKwdT8TgQCODo6OjK6ZGvCdFD+e6775BIJBCPx7G4uMi5Wjo0osdH+VeaRVKtVlEqlTisVhSFazjU6EURTDgcxtbWFpMXKMqh8FqsI5GXk8/nOWXR6XRYdHFWQSK2vV4PzWbzTI/Mp5QtyLHx+/3w+/1n9NWmgXgZUi9Ut9tFu92GZVnMCiSlDWl4LodId49Go6z0QLRfej/i3CVKOem6zjIy83AfEESB5fF4DE3TuJfRNE1mo1I24zzxSBwhQb1Nfr8fL1++xP7+Pp9d0XD3ej0UCgVuWSEneHFxEc1mE/v7+zg5OWFiF0U60+7fmTc8Yo6RUhXiRUH5TUqVOJ1OrK2tIZFInBFhnPU5HHTJUU1nZWUFDx48QCwWO8NaES9MaiIjb5pooxTliVRIksJIpVJcjPV4PNA0Dclk8oxEzHnjDrxf58PDQ/zP//wPdnZ2eD1pbWddMuf8RXOZHhsp7k4mE2iaxkZiWsNDHqSu64jH43A6nTAMg5mXlEuny0AanctBF6vP50M8HkcsFoOmaRyFioZHnA5LKSRd11nPcN5GftCZN02Ta1udTod7FoEP60N3AtGjRQcqmUzC7XYjGo3CNM0zM6XG4zE6nQ5yuRzcbjdGoxF++ukn7O3tweVyYWNjA99//z1GoxHq9TrG4zHXTGkMzrQGfaYND0UAXq+XGVQfs6rEIKLx2pFIBBsbG8zWmHWKNaVyiDKeSCQQi8UQDAb5dz+vlyRqiFH4e3BwwMQLgiiwqmkaYrEYN4h6PB6eqXFeDYEMUL/fR7/fx+npKZ4/f45MJnPm7xdrIPOCj10659dAvLimZbVRG0AkEsHvf/97/PM//zO8Xi93kQcCAayvr2NpaYkLsbK2cznoAg0Gg9wpTywviuxFHTyxn4fuEb/fD5fLNVcpeAJF7oVCAS9evOC5OVT7FQ0PRS50LkWDTcPcqEcPAEfj2WwWP/30E/b39zEcDnF8fIxisQhFUWAYBizLwsbGBtLpNMLhMHw+H6fvj46OeF7VZZhZwyPO3ojH40wwEHnoFF6LAoDJZBJer5e/J8qPzDIcDgdSqRTu3r2Lra0t9kxEZgmB9JJEGnO/30ez2USlUvnVaFrqfahWq1yopCFOxBTM5XJcQBRn0FABnEgd2Wz2jBCgmPr7VkDP1O/3eWYLzTqaNp1Atcb19XVsbm6i1Wqx4QkGg0gkElBVFfl8Hvl8HtVq9co6cL81iHWIaDTKUSixOs/L5ZARohqlqF92WWf9LIIcTWoOz2Qy2NraQjQa/VWNR1SPJ+IFOVCUJRKjbErTU2MuEQvI2NDf5/F4cO/ePdy5cweWZXGNrd1uI5fLTf0sM2t4aBgRXca1Wg02mw2tVuuMQB7l3wOBACKRCHfq1ut17mHERmTJAAAF5UlEQVShKXyzDLvdjnv37uFPf/oTHj9+jEQiwWkYkc1GG4m8mvPyILQBRQyHQ87BZjIZeDweXjdFUVCr1VCpVM5EVWIqijYojcMmeruIb8nwiGoCNM59b28Pe3t7nGK4DHQBEKmD1DToQrTZbCgUCvjHP/6BZ8+eYX9/f+qBXb9FkNGhc07RDjVQAjiTZhI78el7FPFTam5eQWlFilBisRgikciZsyum2QCcuTuazSYymQyq1eqZe3E0GjHphdZHpGqL6imUuhyNRqhWq7+S8roMM2t4aJEsy0K324Wmabh//z5T+4j+RzIoNBTN5XJhOByiUqlwmDjr9R2CKJ9PxUI6RKLhEY0Nhd/VahWNRuPCFAJFPDSyljxAKrR2u12+GGnDiTUlAunfzZuneBVQpK3rOqLRKCaTCY6OjpDJZNDpdKaOeM7TecUv4P0hrlQqODw8xOnpKarV6kwLVn5NkGzWwsICHjx4gD/84Q+4d+8eQqHQmX0qptmAD84TFefb7Taazebc3AcXQbwXO50Os9LI6NIZFu+O4XDI49sty8Le3h6ePXuGk5OTX+258zV08XNFZiYRFKhefFXMrOGhvpTDw0OYponvvvsOd+/eZU0m0YITu4ro1m63G6VSienU87DJRqMRMpkMfvnlF8RiMa4nUPqLPDsxAiHD0263kc/nueP4opTNec+FxkhQA9rHiu/nO6PnYS2/BHTJxWIxHqu8vb3NXt20dFyx/ia+C/EAm6bJl8G3vq6fC6pPBgIBpNNpPH36FP/0T/+E5eVleDweNjgik0ssttP90Gq1kM/nkc1mWcxyXmFZFsrlMpLJ5BmShLgWRLYiKZzd3V1ks1kYhoF3797hb3/7GzdOTwOR3EDKG6SkQt+/CmbW8FDOkXSViMpKc1Po8FLXd7vd5p91Op3odDoflYmZRQyHQxwcHDBfvt/vM7NEURTEYjEsLi7C6XSeCaHP14A+JYxKm2MaA/JbTfnQviOPkg6uGFFOszYXzYwR62FiL9S8MaxuG+J5JyNNmoXi2p6vW4hilmIt9Cq031kDrYVlWSiVStjb28Py8jIWFhY43TYcDll3sNPpoFqt4sWLFzyPq1wusxGadh3I8BiGgdPTU7x9+5ZJR5+jBjGzhgf44DUahoHDw0P2OoEPB5vyjvRFNRFKC83LBhuPx9wPY7fbUa/XWeZc0zTuohel+0naguaT+Hy+M/UgiauDGILlchmnp6cIBALodrtoNBosPzSN0RYns4qHUlQAphTnrDfffk3QRUvKE7/88gurk2iadiZyEYeVibUNUdRVbISe5zMymUxQrVaxs7ODtbU1fPfddwiFQhiPx2g2mzg5OcHbt29RKBSQz+fx6tUrTq0RU/Uqz0+Gp9Pp4ODgAF6vF91ulwkyV3XyZ9rwEEajEdrtNl/KwNmmSfoS8+n0M/MEivDevHnDFEaihVNagWihy8vLiEajXLOhGhc1f87bs88KxANWKBSY0Uf9StMeLjEPT3R+scGv3+/DMAx0Oh2pRH0JqEZTLBbx8uVLZquGw2GOKOn8n58fA7xPLTebTSYoXbUQPmsQoz+KNgidTgdHR0cs2Lm/v4/t7W1ks9kvnj9ETtnp6Sn/mdvtxs7ODjKZzJV6+ebC8JCHeNGhP39gxeLiPGIwGKBUKqFarcLhcGBpaQmLi4totVoczWmaBl3XEQ6Hz1AoKeU2z4ydWQH1f1E6R3RupoHYXEuSIyLNVRQPnefUz22Aohby1HVdRzKZRKPR4BoO3Q0k63Le8NBARABzP1b8ovoh7Ssy0ETPp5TbdQy9o/fQaDQ4/e9yuZghd5XI3XaVDW+z2coAjj/jd54VrE4mk9jX/iUuglzbm4Nc25uDXNubwzewtsBH1vdKhkdCQkJCQuJLIXMyEhISEhK3Cml4JCQkJCRuFdLwSEhISEjcKqThkZCQkJC4VUjDIyEhISFxq5CGR0JCQkLiViENj4SEhITErUIaHgkJCQmJW4U0PBISEhISt4r/B6pUbrk3C9ViAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIDJM1PnADIt"
      },
      "source": [
        "## Deeperdoesn't work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vcqne0XZLcYf"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdDmYca6CmXP"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MWRg9T5AFNR"
      },
      "source": [
        "class MyModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    \n",
        "    self.model = nn.Sequential(\n",
        "      nn.Conv2d(1, 16, kernel_size=3), nn.ReLU(), # 26*26\n",
        "      nn.Conv2d(16, 16, kernel_size=3), nn.ReLU(), # 24*24\n",
        "      # nn.MaxPool2d(2,2), # 13*13\n",
        "\n",
        "      nn.Conv2d(16, 32, kernel_size=3), nn.ReLU(),  # 22*22\n",
        "      nn.Conv2d(32, 32, kernel_size=3), nn.ReLU(),  # 20*20\n",
        "      # nn.MaxPool2d(2,2),  # 6*6\n",
        "\n",
        "      nn.Conv2d(32, 64, kernel_size=3), nn.ReLU(),  # 18*18\n",
        "      nn.Conv2d(64, 64, kernel_size=3), nn.ReLU(),  # 16*16\n",
        "      # nn.MaxPool2d(2,2),  # 2*2\n",
        "\n",
        "      nn.Conv2d(64, 128, kernel_size=3), nn.ReLU(), # 14*14\n",
        "      nn.Conv2d(128, 128, kernel_size=3), nn.ReLU(),  # 12*12\n",
        "      # nn.MaxPool2d(2,2),\n",
        "\n",
        "      nn.Conv2d(128, 256, kernel_size=3), nn.ReLU(),  # 10*10\n",
        "      nn.Conv2d(256, 256, kernel_size=3), nn.ReLU(),  # 8*8\n",
        "      nn.MaxPool2d(2,2),  # 4*4\n",
        "\n",
        "    )\n",
        "    \n",
        "    self.classifier = nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      nn.Dropout(0.25),\n",
        "      # nn.Linear(4096, 256),\n",
        "      nn.Linear(4096, 256),\n",
        "      nn.ReLU(),\n",
        "\n",
        "      nn.Dropout(0.5),\n",
        "      nn.Linear(256, 10),\n",
        "      nn.Softmax(dim=1)\n",
        "    )\n",
        "    \n",
        "  def forward(self, x):\n",
        "    f = self.model(x)\n",
        "    y_pred = self.classifier(f)\n",
        "    return y_pred\n",
        "\n",
        "network = MyModel()\n",
        "# summary(model, (3,256,256))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7daPmiD_EJ53"
      },
      "source": [
        "optimizer = optim.Adam(network.parameters())\n",
        "\n",
        "train_losses = []\n",
        "train_counter = []\n",
        "test_losses = []\n",
        "test_counter = [i*len(train_loader.dataset) for i in range(3)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tv66liBRDHL9"
      },
      "source": [
        "def train(epoch):\n",
        "  network.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    output = network(data)\n",
        "\n",
        "    loss = F.nll_loss(output, target) #negative log likelihood loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % 20 == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "        100. * batch_idx / len(train_loader), loss.item()))\n",
        "      train_losses.append(loss.item())\n",
        "      train_counter.append(\n",
        "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
        "\n",
        "def test():\n",
        "  network.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in val_loader:\n",
        "      output = network(data)\n",
        "      # this line is added to convert labels to LongTensor\n",
        "      # target = target.type(torch.LongTensor)\n",
        "      # target = torch.argmax(target, dim=1) # convert from 1-hot to 1D\n",
        "\n",
        "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "  test_loss /= len(val_loader.dataset)\n",
        "  test_losses.append(test_loss)\n",
        "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss, correct, len(val_loader.dataset),\n",
        "    100. * correct / len(val_loader.dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30Rz0VM-FKN5"
      },
      "source": [
        "for epoch in range(1, 4):\n",
        "  train(epoch)\n",
        "  test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJ1U0lhpHojX"
      },
      "source": [
        "print(torch.cuda.is_available())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3I5ztq6EYnV"
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "        self.fc1 = nn.Linear(3136, 1000)\n",
        "        self.fc2 = nn.Linear(1000, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        # print(out.shape)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3714Y8uHiYbV"
      },
      "source": [
        "class AlexNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AlexNet, self).__init__()\n",
        "        # input 1*28*28\n",
        "        self.conv1 = nn.Conv2d(1, 96, kernel_size=11, stride=4)\n",
        "        self.conv2 = nn.Conv2d(96, 256, kernel_size=5, padding=2)\n",
        "        self.conv3 = nn.Conv2d(256, 384, kernel_size=3,padding=1)\n",
        "        self.conv4 = nn.Conv2d(384, 384, kernel_size=3,padding=1)\n",
        "        self.conv5 = nn.Conv2d(384, 256, kernel_size=3,padding=1)\n",
        "        self.fc1 = nn.Linear(256*6*6, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "        self.fc3 = nn.Linear(4096, 10)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.max_pool2d(torch.relu(self.conv1(x)), kernel_size=3, stride=2) # 96 filters + maxpooling => 96*27*27\n",
        "        x = F.max_pool2d(torch.relu(self.conv2(x)), kernel_size=3, stride=2) # 256 filters + maxpooling => 256*13*13\n",
        "        x = torch.relu(self.conv3(x)) # 384 filters => 384*13*13\n",
        "        x = torch.relu(self.conv4(x)) # 384 filters => 384*13*13\n",
        "        x = F.max_pool2d(torch.relu(self.conv5(x)), kernel_size=3, stride=2) # 256 filters => 256*6*6\n",
        "        x = self.dropout(x) \n",
        "        x = x.view(-1, 256*6*6) \n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return F.softmax(x)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}