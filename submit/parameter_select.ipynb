{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ConvNet.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "CFEyrp_mYmIO",
        "ZX0XLJDjkqX_",
        "3yEtGFpwdYx1",
        "K7VpSLb22POJ"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jwang44/upgraded-octo-chainsaw/blob/main/submit/parameter_select.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWyG0fkjH2zZ"
      },
      "source": [
        "# ECSE 551 3rd Mini-Project\n",
        "Group 10: Junhao Wang, Yinan Zhou, Ruilin Ji\n",
        "\n",
        "This notebook contains all code related to \n",
        "\n",
        "**hyperparameter tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dOxAvsuIul3",
        "outputId": "29b4ce16-f205-4216-bf9d-0b015dfd035d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4WoprD5s5kd",
        "outputId": "bbe8fb57-2c07-4284-9fe5-c9147263ccc1"
      },
      "source": [
        "%cd '/content/drive/MyDrive/imageunderstanding'\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/imageunderstanding\n",
            "ExampleSubmissionRandom.csv  Test.pkl\t       VAL_ACCU_RES34_ROTATE.csv\n",
            "Load_data.ipynb\t\t     Train_labels.csv\n",
            "PRED_RESULT.csv\t\t     Train.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1-Hfwf3lrw4"
      },
      "source": [
        "TRAIN_DATA_PATH = \"Train.pkl\"\n",
        "TRAIN_LABEL_PATH = \"Train_labels.csv\"\n",
        "TEST_DATA_PATH = \"Test.pkl\"\n",
        "CSV_OUTPUT_PATH = \"PRED_RESULT.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XS2MS7nHzIN"
      },
      "source": [
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from PIL import Image\n",
        "import torch\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBXdYCpunCp2",
        "outputId": "f8d7ba9b-cf1c-4274-8e0d-8ae286448dd7"
      },
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "DEVICE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zppkOjQwVjy2"
      },
      "source": [
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2Vm3wY_K3DA"
      },
      "source": [
        "## Dataset Class / Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bydOoypc5SAM"
      },
      "source": [
        "IMG_SIZE = (224, 224)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMnXBwXMCqd5"
      },
      "source": [
        "# Transforms are common image transformations. They can be chained together using Compose.\n",
        "img_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize([0.5], [0.5]),\n",
        "    transforms.Resize(IMG_SIZE),\n",
        "    # transforms.RandomRotation(10)\n",
        "    # transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)\n",
        "    # transforms.RandomAffine(degrees, translate=None, scale=None, shear=None, interpolation=<InterpolationMode.NEAREST: 'nearest'>, fill=0, fillcolor=None, resample=None)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY0SndE2qcmb"
      },
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, img_file, label_file, transform=None, idx = None):\n",
        "        self.data = pickle.load( open( img_file, 'rb' ), encoding='bytes')\n",
        "        self.targets = np.genfromtxt(label_file, delimiter=',', skip_header=1, usecols=1) #[:,1:]\n",
        "        if idx is not None:\n",
        "          self.targets = self.targets[idx]\n",
        "          self.data = self.data[idx]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index].squeeze(), int(self.targets[index])\n",
        "        img = Image.fromarray((img*255).astype('uint8'), mode='L')\n",
        "        if self.transform is not None:\n",
        "           img = self.transform(img)\n",
        "        return img, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3b-QvEvqNE3"
      },
      "source": [
        "Get loader for all train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74kV_DwkqizL"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "dataset = MyDataset(TRAIN_DATA_PATH, TRAIN_LABEL_PATH,transform=img_transform, idx=None)\n",
        "# dataloader for all data\n",
        "data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZNiErb-pbPt"
      },
      "source": [
        "Get loaders for train/val data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05SXSXBRW4DX"
      },
      "source": [
        "VAL_SPLIT = 0.15\n",
        "shuffle = True\n",
        "\n",
        "# Creating indices for train and val split:\n",
        "dataset_size = len(dataset)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(VAL_SPLIT * dataset_size))\n",
        "if shuffle:\n",
        "  # set random seed so that we get the same split everytime\n",
        "  np.random.seed(0)\n",
        "  np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "train_dataset = MyDataset(TRAIN_DATA_PATH, TRAIN_LABEL_PATH,transform=img_transform, idx=train_indices)\n",
        "val_dataset = MyDataset(TRAIN_DATA_PATH, TRAIN_LABEL_PATH,transform=img_transform, idx=val_indices)\n",
        "\n",
        "# separate loaders for train and val data\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFEyrp_mYmIO"
      },
      "source": [
        "## Test Dataset / Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1d3LIGlYrDK"
      },
      "source": [
        "class MyTestSet(Dataset):\n",
        "  def __init__(self, img_file, transform=None):\n",
        "    self.data = pickle.load( open(img_file, 'rb' ), encoding='bytes')\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    # return self.data.shape[0]\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img = self.data[index].squeeze()\n",
        "    img = Image.fromarray((img*255).astype('uint8'), mode='L')\n",
        "    if self.transform is not None:\n",
        "      img = self.transform(img)\n",
        "    return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk5z7LRha2Uc"
      },
      "source": [
        "test_dataset = MyTestSet(TEST_DATA_PATH,transform=img_transform)\n",
        "# dataloader for test data\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdvC0-OfRnQV"
      },
      "source": [
        "## CNN models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFVSRS6gs8t7"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import torchvision.models as models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Av4Q9vIJizM"
      },
      "source": [
        "### Simple CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olN0jAO9JliZ"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    # This part defines the layers\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # At first there is only 1 channel (greyscale). The next channel size will be 10. \n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        # Then, going from channel size (or feature size) 10 to 20. \n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        # Now let us create some feed foreward layers in the end. Remember the sizes (from 320 to 50)\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        # The last layer should have an output with the same dimension as the number of classes\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    # And this part defines the way they are connected to each other\n",
        "    # (In reality, it is our foreward pass)\n",
        "    def forward(self, x):\n",
        "        \n",
        "\n",
        "        # F.relu is ReLU activation. F.max_pool2d is a max pooling layer with n=2\n",
        "        # Max pooling simply selects the maximum value of each square of size n. Effectively dividing the image size by n\n",
        "        # At first, x is out input, so it is 1x28x28\n",
        "        # After the first convolution, it is 10x24x24 (24=28-5+1, 10 comes from feature size)\n",
        "        # After max pooling, it is 10x12x12\n",
        "        # ReLU doesn't change the size\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "\n",
        "        # Again, after convolution layer, size is 20x8x8 (8=12-5+1, 20 comes from feature size)\n",
        "        # After max pooling it becomes 20x4x4\n",
        "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "\n",
        "        # This layer is an imaginary one. It simply states that we should see each member of x\n",
        "        # as a vector of 320 elements, instead of a tensor of 20x4x4 (Notice that 20*4*4=320)\n",
        "        x = x.view(-1, 320)\n",
        "\n",
        "        # Feedforeward layers. Remember that fc1 is a layer that goes from 320 to 50 neurons\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        # Output layer\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        # We should put an appropriate activation for the output layer.\n",
        "        return F.log_softmax(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-drv_m8JMfz"
      },
      "source": [
        "### LeNet-5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdUjLkwfJP0_"
      },
      "source": [
        "class LeNet5(nn.Module):\n",
        "    def __init__(self, kernel_size=5,ActFunc=['Tanh']):\n",
        "        super(LeNet5, self).__init__()\n",
        "        # input 1*28*28\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size)\n",
        "        self.conv3 = nn.Conv2d(16, 120, kernel_size)\n",
        "        self.fc1 = nn.Linear(120, 84)\n",
        "        self.fc2 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = F.avg_pool2d(F.tanh(self.conv1(x)), kernel_size = (2,2),stride = 2) # 6 filters + avgpooling => 6*14*14\n",
        "        # x = F.avg_pool2d(F.tanh(self.conv2(x)), kernel_size = (2,2),stride = 2) # 16 filters + avgpooling => 16*5*5\n",
        "        x = F.avg_pool2d(torch.tanh(self.conv1(x)), kernel_size = (2,2),stride = 2) # 6 filters + avgpooling => 6*14*14\n",
        "        x = F.avg_pool2d(torch.tanh(self.conv2(x)), kernel_size = (2,2),stride = 2) # 16 filters + avgpooling => 16*5*5\n",
        "        x = torch.tanh(self.conv3(x)) # 120 filters => 120*1*1\n",
        "        x = x.view(-1, 120) \n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        # x = (self.fc3(x) # output layer\n",
        "        return F.softmax(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEAeYXqjKiMi"
      },
      "source": [
        "### VGG-11"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcxqeS8vKhe7"
      },
      "source": [
        "model = models.vgg11(pretrained=False)\n",
        "model.features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "model.classifier[6] = nn.Linear(in_features=4096, out_features=10, bias=True)\n",
        "model = model.to(DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGj4HgAEMsxA"
      },
      "source": [
        "### AlexNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx68ZfoSGgZy"
      },
      "source": [
        "model = models.alexnet(pretrained=False)\n",
        "model.features[0] = nn.Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
        "model.classifier[6] = nn.Linear(4096, 10)\n",
        "model = model.to(DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci11eU_mKnkX"
      },
      "source": [
        "### Resnet-18"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfN0avQP398k"
      },
      "source": [
        "model = models.resnet18(pretrained=False)\n",
        "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "model.fc = nn.Linear(in_features=512, out_features=10, bias=True)\n",
        "model = model.to(DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Kcb2ohYKrRk"
      },
      "source": [
        "### Resnet-34"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-Lb5h6MJmXv"
      },
      "source": [
        "model = models.resnet34(pretrained=False)\n",
        "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "model.fc = nn.Linear(in_features=512, out_features=10, bias=True)\n",
        "model = model.to(DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFDcK-NeKvm4"
      },
      "source": [
        "### Optimizer & initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJqfMWSFIztX"
      },
      "source": [
        "# optimizer = optim.SGD(tutor_model.parameters(), lr=0.01, momentum=0.5)\n",
        "# optimizer = optim.SGD(tutor_model.parameters(), lr=1, momentum=0.5)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "# optimizer = optim.RMSprop(model.parameters())\n",
        "\n",
        "train_losses = []\n",
        "train_counter = []\n",
        "val_accus = []\n",
        "epochs = []\n",
        "val_losses = []\n",
        "val_counter = [i*len(train_loader.dataset) for i in range(3)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck4yNKs46Q1Y"
      },
      "source": [
        "## Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGIGk53cvz4Y"
      },
      "source": [
        "! pip install optuna\n",
        "! pip install mlflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6gSNmeqv2OV"
      },
      "source": [
        "import optuna\n",
        "import mlflow\n",
        "from pprint import pformat\n",
        "from torch.optim.lr_scheduler import StepLR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyUlGXEZvu0k"
      },
      "source": [
        "# for hyperparameter\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    train_set_size = len(train_loader.dataset)\n",
        "    num_batches = len(train_loader)\n",
        "    train_loss = 0.0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            batch_size = len(data)\n",
        "            print(f\"Train Epoch: {epoch} [{batch_idx * batch_size}/{train_set_size} \"\n",
        "                  f\"({100. * batch_idx / num_batches:.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
        "    avg_train_loss = train_loss / num_batches\n",
        "    return avg_train_loss\n",
        "\n",
        "# Testing loop\n",
        "def val(model, device, val_loader):\n",
        "    model.eval()\n",
        "    val_set_size = len(val_loader.dataset)\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            val_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    val_loss /= val_set_size\n",
        "\n",
        "    print(f\"Test set: Average loss: {val_loss:.4f}, Accuracy: {correct}/{val_set_size} \"\n",
        "          f\"({100. * correct / val_set_size:.0f}%)\\n\")\n",
        "    return val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ypgyetjvr-5"
      },
      "source": [
        "def suggest_hyperparameters(trial):\n",
        "    # Obtain the learning rate on a logarithmic scale\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
        "    # Obtain the dropout ratio in a range from 0.0 to 0.9 with step size 0.1\n",
        "    #dropout = trial.suggest_float(\"dropout\", 0.0, 0.9, step=0.1)\n",
        "    # Obtain the optimizer to use by name\n",
        "    optimizer_name = trial.suggest_categorical(\"optimizer_name\", [\"Adam\", \"RMSprop\"])\n",
        "    #momentum= trial.suggest_uniform('momentum', 0.4, 0.99)\n",
        "\n",
        "    print(f\"Suggested hyperparameters: \\n{pformat(trial.params)}\")\n",
        "    return lr,  optimizer_name#,momentum#dropout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk8bkl8OvpuC"
      },
      "source": [
        "# for simple CNN\n",
        "def objective(trial):\n",
        "    print(\"\\n********************************\\n\")\n",
        "    best_val_loss = float('Inf')\n",
        "    \n",
        "    # Start a new mlflow run\n",
        "    with mlflow.start_run():\n",
        "        # Get hyperparameter suggestions created by optuna and log them as params using mlflow\n",
        "        #lr,  optimizer_name, momentum= suggest_hyperparameters(trial)\n",
        "        \n",
        "        lr,optimizer_name= suggest_hyperparameters(trial)\n",
        "        mlflow.log_params(trial.params)\n",
        "\n",
        "        # Use CUDA if GPU is available and log device as param using mlflow\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        mlflow.log_param(\"device\", device)\n",
        "\n",
        "        # define model\n",
        "        #model = Tutor_model(dropout=dropout).to(device)\n",
        "        model = Net().to(device)\n",
        "\n",
        "        # Pick an optimizer based on optuna's parameter suggestion\n",
        "        if optimizer_name == \"Adam\":\n",
        "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "        if optimizer_name == \"RMSprop\":\n",
        "            optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
        "        scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
        "        \n",
        "        # Get DataLoaders for MNIST train and validation set\n",
        "        #train_loader, val_loader = get_mnist_dataloaders()\n",
        "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "        \n",
        "        # Network training & validation loop\n",
        "        for epoch in range(0, 10):\n",
        "            avg_train_loss = train(model, device, train_loader, optimizer, epoch)\n",
        "            #avg_train_loss = train( epoch,model,train_loader)\n",
        "            avg_val_loss = val(model, device, val_loader)\n",
        "            \n",
        "            if avg_val_loss <= best_val_loss:\n",
        "                best_val_loss = avg_val_loss\n",
        "\n",
        "            # Log average train and validation set loss metrics for the current epoch using mlflow\n",
        "            mlflow.log_metric(\"avg_train_losses\", avg_train_loss, step=epoch)\n",
        "            mlflow.log_metric(\"avg_val_loss\", avg_val_loss, step=epoch)\n",
        "            \n",
        "            scheduler.step()\n",
        "\n",
        "    # Return the best validation loss achieved by the network.\n",
        "    # This is needed as Optuna needs to know how the suggested hyperparameters are influencing the network loss.\n",
        "    return best_val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cb6DYQ9FvnSB"
      },
      "source": [
        "# for Resnet-18\n",
        "def objective(trial):\n",
        "    print(\"\\n********************************\\n\")\n",
        "    best_val_loss = float('Inf')\n",
        "    \n",
        "    # Start a new mlflow run\n",
        "    with mlflow.start_run():\n",
        "        # Get hyperparameter suggestions created by optuna and log them as params using mlflow\n",
        "        #lr,  optimizer_name, momentum= suggest_hyperparameters(trial)\n",
        "        \n",
        "        lr,optimizer_name= suggest_hyperparameters(trial)\n",
        "        mlflow.log_params(trial.params)\n",
        "\n",
        "        # Use CUDA if GPU is available and log device as param using mlflow\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        mlflow.log_param(\"device\", device)\n",
        "\n",
        "        # define model\n",
        "        #model = Tutor_model(dropout=dropout).to(device)\n",
        "        model = models.resnet18(pretrained=False)\n",
        "        model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "        model.fc = nn.Linear(in_features=512, out_features=10, bias=True)\n",
        "        model = model.to(DEVICE)\n",
        "\n",
        "        # Pick an optimizer based on optuna's parameter suggestion\n",
        "        if optimizer_name == \"Adam\":\n",
        "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "        if optimizer_name == \"RMSprop\":\n",
        "            optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
        "        scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
        "        \n",
        "        # Get DataLoaders for MNIST train and validation set\n",
        "        #train_loader, val_loader = get_mnist_dataloaders()\n",
        "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "        \n",
        "        # Network training & validation loop\n",
        "        for epoch in range(0, 10):\n",
        "            avg_train_loss = train(model, device, train_loader, optimizer, epoch)\n",
        "            #avg_train_loss = train( epoch,model,train_loader)\n",
        "            avg_val_loss = val(model, device, val_loader)\n",
        "            \n",
        "            if avg_val_loss <= best_val_loss:\n",
        "                best_val_loss = avg_val_loss\n",
        "\n",
        "            # Log average train and validation set loss metrics for the current epoch using mlflow\n",
        "            mlflow.log_metric(\"avg_train_losses\", avg_train_loss, step=epoch)\n",
        "            mlflow.log_metric(\"avg_val_loss\", avg_val_loss, step=epoch)\n",
        "            \n",
        "            scheduler.step()\n",
        "\n",
        "    # Return the best validation loss achieved by the network.\n",
        "    # This is needed as Optuna needs to know how the suggested hyperparameters are influencing the network loss.\n",
        "    return best_val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I82JX20lvkdB"
      },
      "source": [
        "# for Resnet-34\n",
        "def objective(trial):\n",
        "    print(\"\\n********************************\\n\")\n",
        "    best_val_loss = float('Inf')\n",
        "    \n",
        "    # Start a new mlflow run\n",
        "    with mlflow.start_run():\n",
        "        # Get hyperparameter suggestions created by optuna and log them as params using mlflow\n",
        "        #lr,  optimizer_name, momentum= suggest_hyperparameters(trial)\n",
        "        \n",
        "        lr,optimizer_name= suggest_hyperparameters(trial)\n",
        "        mlflow.log_params(trial.params)\n",
        "\n",
        "        # Use CUDA if GPU is available and log device as param using mlflow\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        mlflow.log_param(\"device\", device)\n",
        "\n",
        "        # define model\n",
        "        #model = Tutor_model(dropout=dropout).to(device)\n",
        "        model = models.resnet34(pretrained=False)\n",
        "        model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "        model.fc = nn.Linear(in_features=512, out_features=10, bias=True)\n",
        "        model = model.to(DEVICE)\n",
        "\n",
        "        # Pick an optimizer based on optuna's parameter suggestion\n",
        "        if optimizer_name == \"Adam\":\n",
        "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "        if optimizer_name == \"RMSprop\":\n",
        "            optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
        "        scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
        "        \n",
        "        # Get DataLoaders for MNIST train and validation set\n",
        "        #train_loader, val_loader = get_mnist_dataloaders()\n",
        "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "        \n",
        "        # Network training & validation loop\n",
        "        for epoch in range(0, 10):\n",
        "            avg_train_loss = train(model, device, train_loader, optimizer, epoch)\n",
        "            #avg_train_loss = train( epoch,model,train_loader)\n",
        "            avg_val_loss = val(model, device, val_loader)\n",
        "            \n",
        "            if avg_val_loss <= best_val_loss:\n",
        "                best_val_loss = avg_val_loss\n",
        "\n",
        "            # Log average train and validation set loss metrics for the current epoch using mlflow\n",
        "            mlflow.log_metric(\"avg_train_losses\", avg_train_loss, step=epoch)\n",
        "            mlflow.log_metric(\"avg_val_loss\", avg_val_loss, step=epoch)\n",
        "            \n",
        "            scheduler.step()\n",
        "\n",
        "    # Return the best validation loss achieved by the network.\n",
        "    # This is needed as Optuna needs to know how the suggested hyperparameters are influencing the network loss.\n",
        "    return best_val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFx2tM8IvhFB"
      },
      "source": [
        "    # Create the optuna study which shares the experiment name\n",
        "    study = optuna.create_study(study_name=\"pytorch-mlflow-optuna\", direction=\"minimize\")\n",
        "    study.optimize(objective, n_trials=10)\n",
        "\n",
        "    # Print optuna study statistics\n",
        "    print(\"\\n++++++++++++++++++++++++++++++++++\\n\")\n",
        "    print(\"Study statistics: \")\n",
        "    print(\"  Number of finished trials: \", len(study.trials))\n",
        "\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "\n",
        "    print(\"  Trial number: \", trial.number)\n",
        "    print(\"  Loss (trial value): \", trial.value)\n",
        "\n",
        "    print(\"  Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(\"    {}: {}\".format(key, value))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}